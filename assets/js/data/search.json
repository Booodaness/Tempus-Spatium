[ { "title": "Qualia in Temporal Naturalism (Part 2: Some Philosophical Puzzles)", "url": "/tempus-spatium/qualia-temporal-naturalism-puzzles/", "categories": "philosophy of mind", "tags": "qualia, temporal naturalism, Russellian monism", "date": "2024-08-12 00:00:00 -0400", "snippet": "IntroductionIn Qualia in Temporal Naturalism (Part 1: Towards Cosmoprotopsychism), I described a heuristic cosmoprotopsychic model where phenomenal consciousness internal to biological structures can potentially be explained via self-organization of protophenomenal consciousness intrinsic to the cosmic scale. The particular details of such a mechanism were motivated by Lee Smolin’s hypothesis of qualia being physically correlated to unprecedented events in the context of his and Roberto Mangabeira Unger’s philosophical framework of ‘temporal naturalism’.In this article, I will argue that unlike traditional physicalism, the temporal naturalization of qualia can reasonably avoid qualia and modal arguments against physicalism; as well as shed light on qualia absence arguments against functionalism. Some of the strategy for counter-argumentation will closely resemble David Lewis’ materialism.For brevity, I will roughly continue from the point where the previous post concluded. However, I will explicitly lay out summaries where possible, for autonomy. In terms of the University of Waterloo PHIL 255 (Philosophy of Mind) course content, I will implicitly, by virtue of continuation, refer to Dennett (1997), Chalmers (TED, 2014), Brentano (2015) and Millikan (1989). In the direct construction of the argument in this article, explicit references will be made to Kim (1992), Fodor (1974), Block (1978, 2002), Searle (2002), Nagel (1974), Jackson (1982), Lewis (1999) and Kripke (1980). On the other hand, external references will be identical to those in the previous post.Recap of temporal qualiaA temporal universe is a singular (i.e. non-plural) universe in flux, with mutable natural kinds and variable nomic character to inclusively causal processes. Smolin’s Principle of Precedents (PoP) is a non-nomic law (i.e. a stochastic law acting on system tokens and not types 1) which dictates that the updated state of a system be chosen randomly from the updated states of prior occurrences of such a system given its causal profile — i.e., the system’s precedents. Effectively, such a mechanism allows an unstable, non-nomic cosmic phase such as the Big Bang to self-organize into a stable, nomic (lawlike-evolving) phase where simple systems have so many precedents that they roughly evolve like in the past and across members of types. Therefore, evolutionary reasoning, only in stable situations, is compatible with structural reasoning based on fixed mathematical theories.The relevance of this framework to the philosophy of mind is that qualia may be explainable, at least partly, in terms of unprecedented events. In their highly sustained, non-nomic evolution, unprecedented events evolve as the coming-to-be of novel events, which could be correlated to the phenomenal experience of time. Even more significantly, unprecedented events escape the nomological net of lawlike evolution which only effectively applies to highly precedented events. Therefore, unprecedented events could conceivably be correlated to intrinsic qualities such as protoconsciousness, acting as grounds for the categorization of relational systems — thereby amounting to Russellian monism. This would not a priori contradict what we already know about purely relational quantities studied in traditional physics.Jackson’s knowledge argumentBy virtue of temporal qualia being ‘quiddities’ i.e. non-relational properties 2 in the spirit of Russellian monism, one can avoid Jackson’s knowledge argument (1982) as follows. In the famous setup, Mary is not sentient about colour studies even if she knows all there is to know about the dispositional properties of colour perception events, whether at a microcausal level of description or non-reductive, functional level of organization. This is because she is not yet aware of the Russellian monist’s non-dispositional, experiential qualities associated with colour perception events — which are part of an extended psychophysical reality.In a Leibnizian relationalism, distinct events have distinct configurations of causal profile + internal properties. In Russellian monism, these internal properties can categorize causal systems with the fundamental substrate being psychophysical; this is a form of dual-aspect monism where physical systems are categorized based on their mentality.In temporal naturalism, a weak form of this argument is that at the descriptive, i.e. extrinsic level 3, Mary does not know how colour perception events are correlated to unprecedented events in the physical substrate. However, knowledge of such intrinsic properties can be demanded by modifying the knowledge argument, shifting the problem of capture in the causal net of physicalist knowledge to internal properties. In response, a stronger but more speculative temporal naturalistic argument could be that either qualia are only effectively macro-internal but do have causal efficacy over long periods of time, i.e., only approximately epiphenomena (as argued in the previous post); or, even if they are fundamentally intrinsic, it is not a priori contradictory to suppose that Mary finds some way to access the intrinsic categorization of dispositional systems with colour perception, so that she can somehow induce in herself a phenomenal state that at the physical substrate corresponds to the novel character of the events experiencing said state internally.In other words, if qualia are psychophysically correlated to unprecedented events, that qualia lack causal efficacy does not prevent us from utilizing its physical correlate, which does have causal efficacy, to cause in Mary a physical correlation which in turn will be correlated to her internal qualia 4. However, since local hidden variables (which are non-relational properties only be correlated via local processes like signal transmission) have been falsified with statistical significance since the work of John Bell in quantum theory (1964), we must expect that at least some instances of the internal residue of qualia can be nonlocally correlated, which is an empirically falsifiable hypothesis 5! Therefore, a temporal naturalistic approach to qualia seems to have not only metaphysical but also empirical, scientific aspects!Nagel’s perspectical subjectivity argumentAlthough arguments of the above kind probably set a standard Russellian monistic direction for countering knowledge arguments (see Alter &amp;amp; Coleman, 2019), Nagel’s problem, of the perspectival subjectivity of qualia as tokens, remains. The phenomenally particular experiences of a bat, which are rigid descriptors by virtue of the bat being itself (in other words, the subjectivity of a first-person experience), cannot be identical to correlated states of Mary (even if she is a bat), whether physical or phenomenal — unless the rigid descriptor ‘Mary’ is necessarily identical to the rigid descriptor corresponding to the bat, i.e., Mary is the bat experiencing the first-person perspective of the psychophysical world.However, temporal naturalism can accept the Nagelian thesis without compromise and in fact, with added explanatory power — it is precisely because qualia are correlated to novelty that correlated mental states cannot have the same phenomenal character! Equivalently, maximally phenomenal systems are also maximally unprecedented and therefore, unital classes. By rejecting the thesis of principled omniscience via knowledge by description or ‘correlated acquaintance’ alone, a Russellian categorization of distinct tokens of internal experience is precisely what temporal naturalism seems to provide. This, again, leads to the empirical hypothesis that somehow, Mary’s correlated-to-bat phenomenal state should somehow reduce the novelty and hence, phenomenal character of the bat’s actual state in real time, perhaps in the manner of nonlocal hidden variables in quantum theory!Kripke’s modal argumentNow, consider the following short, related argument we can construct against Kripke’s modal argument. Even if the rigid descriptor ‘Mary’s phenomenal experience of pain’ is modally distinct from the rigid descriptor ‘Mary’s so-and-so psychophysical state’, such distinctness does not follow for the rigid descriptors ‘Mary’s internal experience of pain + the causal profile of the pain’s non-internal residue’ and ‘Mary’s corresponding unprecedented psychophysical state’. In other words, necessary correlation (at least effective in some stable cosmic phase) of a token of experience given a psychophysical state can ensure that the experience and psychophysical correlate are not conceivably separable.Qualia Absence Arguments Against FunctionalismIn this section, I will shortly describe how temporal qualia challenge Searle’s (2002) and support Block’s (2002) qualia absence arguments against functionalism.Against Searle’s semantical argumentA computer is an incredibly complicated system, making it unreasonable to a priori reject the possibility that every time a computer is operated, it results in an event without precedents. Therefore, it is possible that complicated artificial systems, in principle, have P-consciousness. However, contrary to Searle, this is perfectly compatible with such complex computers not possessing A-consciousness, an aspect of which is intentionality (Block, 1978).This is further supported by the evolutionary aspect of temporal naturalistic explanations: how reproducing, gene-transmitting organisms evolved to coevally develop intentionality and P-consciousness (possibly by virtue of how intentional systems can quickly self-organize into unprecedented systems due to their complexity) is not necessarily of the same nature as how systems with their complexity derivative from pre-existing novel systems (computers produced ‘abruptly’ in the biological and cosmic time scale) can gain P-consciousness without A-consciousness. Informally, instead of needing A-consciousness to accumulate novelty and ultimately P-consciousness, artificial computers possess an initial ‘head start’ in novelty by being in appropriate causal contact with other novel systems (factories and ultimately, P-conscious humans 6).Block’s conceivability argumentsRelated to the above, what makes functionalism liberal is that functional descriptions fail to capture the psychophysical details of systems necessary to judge novelty. In fact, the more complex the novel systems under consideration, the more relevance will be the fineness of differences of such systems have for difference in mentality, as there would be a corresponding fineness to the gradation of systems from novel to nomic. This suggests that the functional properties can describe novelty and hence, at least part of mentality for relatively simple systems, only if they seem to be P-conscious and continue to be so when evolving towards increasing novelty with some fixed functional character. The former situation would then act as a control and eliminate the correlation of P-consciousness to novelty as opposed to functional profiles. While this may indeed be the character of some systems in the current phase of the universe, it is yet another empirically falsifiable counterargument to temporal qualia!References Alter, T., &amp;amp; Coleman, S. (2019). Panpsychism and Russellian Monism. The Routledge Handbook of Panpsychism, 230–242. https://doi.org/10.4324/9781315717708-20 Alter, T., &amp;amp; Pereboom, D. (2023, July 4). Russellian monism. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/russellian-monism/ Bhattacharjee, S. (2024, August 5). Qualia in Temporal Naturalism (Part 1: Towards Cosmoprotopsychism). Tempus Spatium. https://booodaness.github.io/tempus-spatium/qualia-temporal-naturalism/ Block, N. (1970, January 1). Concepts of Consciousness. PhilPapers. https://philpapers.org/rec/BLOCOC Brentano, F. (2015). The Distinction between Mental and Physical Phenomena. In Psychology from an Empirical Standpoint (pp. 59–77). essay, Routledge. Chalmers, D. (2014, July 14). How do you explain consciousness?. YouTube. https://youtu.be/uhRhtFFhNzQ Davidson, D. (1970). Donald Davidson, mental events. PhilPapers. https://philpapers.org/rec/DAVME-2 Dennett, D. C. (1997). True Believers: The Intentional Strategy and Why It Works. In Mind Design II: Philosophy, Psychology, and Artificial Intelligence (pp. 57–79). essay, MIT Press. Fodor, J. A. (1974, January 1). Special Sciences. PhilPapers. https://philpapers.org/rec/FODSS Goff, P. (2017). Panpsychism. The Blackwell Companion to Consciousness, 106–124. https://doi.org/10.1002/9781119132363.ch8 Goff, P., Seager, W., &amp;amp; Allen-Hermanson, S. (2022, May 13). Panpsychism. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/panpsychism/ Fodor, J. A. (1974, January 1). Special Sciences. PhilPapers. https://philpapers.org/rec/FODSS Jackson, F. (1982). Epiphenomenal Qualia. The Philosophical Quarterly, 32(127), 127. https://doi.org/10.2307/2960077 Kim, J. (1992). Multiple Realization and the Metaphysics of Reduction. Philosophy and Phenomenological Research, 52(1), 1. https://doi.org/10.2307/2107741 Millikan, R. G. (1989). Biosemantics. The Journal of Philosophy, 86(6), 281. https://doi.org/10.2307/2027123 Nagel, T. (1974). What Is It Like to Be a Bat? The Philosophical Review, 83(4), 435. https://doi.org/10.2307/2183914 Skrbina, D. (n.d.). Panpsychism. Internet Encyclopedia of Philosophy. https://iep.utm.edu/panpsych/ Smolin, L. (2015). Temporal naturalism. Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics, 52, 86–102. https://doi.org/10.1016/j.shpsb.2015.03.005 Smolin, L. (2022). On the Place of Qualia in a Relational Universe. Consciousness and Quantum Mechanics, 482–514. https://doi.org/10.1093/oso/9780197501665.003.0018 Tye, M. (2007). Philosophical Problems of Consciousness. The Blackwell Companion to Consciousness, 23–36. https://doi.org/10.1002/9780470751466.ch3 Unger, R. M., &amp;amp; Smolin, L. (2021). The Singular Universe and the Reality of Time: A Proposal in Natural Philosophy. Cambridge University Press. Van Gulick, R. (2014, January 14). Consciousness (The functional question: Why does consciousness exist?). Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/consciousness/#FunQueWhyDoeConExi Wilkins, J. S., &amp;amp; Bourrat, P. (2022, October 27). Replication and Reproduction. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/replication/ In a Leibnizian, relational network of events, there cannot be simultaneous events with identical causal profile, as this would violate the identity of indiscernibles. Therefore, how PoP acts on a system, while depending on its precedent type, is itself a token statement, as it does not apply to any simultaneous system that is distinct at the finest causal level of description. &amp;#8617; Here, quiddities and non-relational properties will refer to those not deducible, for any event, from the properties of causally related events. Such non-relational properties may nonetheless have causal efficacy. Non-causal and non-relational properties are termed ‘internal’ properties in Smolin’s papers (2015, 2022). &amp;#8617; I.e., a level of description at which we can access relational and intrinsic properties i.e. those with causal efficacy, without knowing what it is like to be the event/events with the property, in the sense of Nagel, 1974. &amp;#8617; Notice the similarity of this argument to Lewis: “Having an experience is surely one good way, and surely the only practical way, of coming to know what that experience is like. Can we say, flatly, that it is the only possible way? Probably not. There is a change that takes place in you when you have the experience and thereby come to know what it’s like. Perhaps the exact same change could in principle be produced in you by precise neurosurgery, very far beyond the limits of present-day technique. Or it could possibly be produced in you by magic. If we ignore the laws of nature, which are after all contingent, then there is no necessary connection between cause and effect: anything could cause anything.” &amp;#8617; This is an instance of temporal naturalism’s power of empirical falsifiability, broadly resulting from choosing evolutionary explanations over structural explanations with supra-empirical, timeless metaphysics. &amp;#8617; Such naturalization of P-consciousness has the advantage that artificial computers now do not seem so ‘artificial’ after all — they merely have a very different natural history from biologically P-conscious organisms, in terms of self-organizability. Since self-organization has to do with how novelty and consciousness show up in evolving systems, these two modes of organization of presumably P-conscious systems — biological and ‘artificial’ — can result in different degrees of A-consciousness. &amp;#8617; " }, { "title": "Qualia in Temporal Naturalism (Part 1: Towards Cosmoprotopsychism)", "url": "/tempus-spatium/qualia-temporal-naturalism/", "categories": "philosophy of mind", "tags": "qualia, temporal naturalism, Russellian monism", "date": "2024-08-05 00:00:00 -0400", "snippet": "IntroductionIn the article Temporal naturalism by Lee Smolin (2013) and his related joint work with Roberto Mangabeira Unger (The Singular Universe and the Reality of Time, 2014), Smolin conjectures that the physical correlates of qualia are the so-called unprecedented events, whereas their experiential aspects at least partially involve intrinsic, i.e. non-relational properties. In this essay, I will use Smolin’s philosophy of mind to argue for a form of Russellian constitutive cosmoprotopsychism which can potentially solve multiple puzzles on the nature of consciousness, such as the hard problems of consciousness concerning mechanism, duplicates and qualia (Tye, 2007) as well as the mind-evolution problem (Chalmers, 1995).In the argument, I will utilize ideas from Davidson (1970), Fodor (1974), Kim (1992), Block (2002), Nagel (1974), Jackson (1982), Dennett (1997), Chalmers (TED, 2014), Brentano (2015) and Millikan (1989) in the course readings of PHIL 255 (Philosophy of Mind) offered at the University of Waterloo. Additionally, I will refer to Smolin’s peer-reviewed article and his co-authored book with Unger mentioned earlier; the peer-reviewed publications On the place of qualia in a relational universe (Smolin, 2020), Panpsychism and Russellian Monism (Torin Alter &amp;amp; Sam Coleman, 2018), Panpsychism (Goff, 2007) and Philosophical Problems of Consciousness (Tye, 2007); and finally, related articles from the Stanford Encyclopedia of Philosophy and the Internet Encyclopedia of Philosophy.Temporal naturalism: evolution over structureTemporal naturalism commits to three theses 1: The inclusive reality of time The singular existence of the universe The selective realism of mathematics Inclusive reality of timeThe inclusive reality of time entails that causal relationships ordered in time are an irreducible aspect of the universe. This is supported by the ‘phenomenal particularity of time’, which cannot be rejected via the premise that the laws and causal types of the universe seem to be time-independent, as such features of cosmic evolution conceivably fail in the hot, dense, unstable state of the universe (the Big Bang) even in the traditional standard model of particle physics. Furthermore, mathematics only selectively describes non-temporal and non-phenomenal aspects of the world, and therefore leaves out qualia; hence, if qualia are structurally absent in mathematics, instead of rejecting the reality of qualia (which is arguably epistemically more certain than supra-experiential commitments of physics), natural philosophy should positively tackle qualia and the phenomenal particularity of time in a manner that is revisionary for traditional physics and physicalism.Singular existence of the universeSince according to the above thesis, time is a global ‘stuff’ of the cosmos (which does not contradict general relativity in careful reformulations such as shape dynamics) and timeless laws are merely a stable mode in the cooled-down state of the universe, explanatory power for relational events 2 is to be found in a historical, evolutionary description of structure formation and not symmetry-involving, fixed-structure laws which are only effective in the stable states of the universe (this can be captured in the maxim, “cosmology should study the evolution of structure, not the structure of evolution”). This means that historically causally connected regions constitute a single universe and any regions forever causally disconnected from such a universe form other universes, but with no explanatory power in the former universe. Therefore, genuine science with explanatory closure and empirically falsifiable implications cannot involve a multiverse of eternally causally disconnected regions, contrary to traditional anthropic approaches to questions such as ‘why these cosmic initial conditions and laws?’. Instead, the effective laws of the current stable state of the universe must emerge via self-organization of the singular 3 universe as it evolves in cosmic time. The untapped explanatory power of this scheme is analogous to that of evolution theory in the context of how complex lifeforms can self-organize while satisfying explanatory closure of the biosphere (such as Kauffman’s far-from-equilibrium dynamics).Selective realism of mathematicsThe selective realism of mathematics is the unequivocal consolidation of the above insights that the temporal character of the universe is fundamental, while its seemingly eternal character, captured in mathematical models, is only a partial feature emergent in stable phases of the universe. Seriously considered in conjunction with the inclusive reality of time, this gives rise to the thesis that novel structures can temporarily self-organize in various phase-transitions of the universe from one mode of evolution to another, but nevertheless cannot remain indefinitely stable, due to flux. The relational aspects of the world captured in mathematical models are selectively real; a full-blown realism wherein mathematical structuralism precedes historical explanation is not justified a priori.Smolin’s Principle of Precedents (PoP)It will now be seen that Smolin’s principle of precedents (PoP) is a simple cosmological principle that is compatible with temporal naturalism, while also explaining the self-organization of stable, lawlike states of the universe.An event-ontological interpretation of PoP is the following. Let a system refer to any collection of simultaneous events with distinct descriptions of relational properties (by Leibniz’s law, these must be distinct events). The input of a system is the set of all events in the causal past of (the events in the) system; similarly, the output of a system is the set of all events in the causal future of the system. Finally, the precedents of a system refer to all systems in its causal past such that they are its copies. A system is a copy of another if and only if: their events are one-to-one correspondent (i.e., bijective, and having the same number of events, if finite). every pair of corresponding events across the similar pair exhibits identical descriptions of relational properties. Putting the above together, a rough analogy from functionalist terminology would be that a precedent of a system is any past instantiation of its causal profile, which would encode number of simultaneous events and their input/output events and their relational properties.Now, the principle of precedents can be stated as follows:The singular universe evolves in cosmic time, such that for any given system \\(S\\) and input state \\(\\psi\\), its output state \\(\\varphi\\) is randomly chosen from the corresponding output states \\(\\varphi&#39;, \\varphi&#39;&#39;, \\dots\\) of its precedents characterized by corresponding input states \\(\\psi&#39;, \\psi&#39;&#39;, \\dots\\) and so on.Clearly, PoP, by minimally committing to a singular, temporally evolving universe with stochastic (i.e., non-deterministic/random) evolution agrees with the basic premises of temporal naturalism.It follows from PoP that the more time passes since the occurrence of a system, the more choices does its most recent instantiation have for choosing its output from precedent outputs. Therefore, statistically speaking, the typicality of precedent outputs in being chosen as new outputs evens out with time. In modified Humean terms, this has the far-reaching consequence that the distant future (with respect to an unstable cosmic state) begins to look like its past! Moreover, if there are multiple simultaneous instantiations of a system, the statistical differences in their evolution begin to disappear, making causal types emerge. Hence, the regularity and universal character of causation in the nomic conception of physical systems can be explained as an emergent property in a temporal universe.The reader may be wondering how temporal naturalism and PoP relate to the philosophy of mind. The next section will consider speculations on qualia being potentially correlated to unprecedented physical systems and how a suggestion of the intrinsic character of such qualia as being grounds for categorizing relational properties amounts to Russellian monism (Alter &amp;amp; Coleman, 2019) and in particular, a form of cosmoprotopsychism.Interlude: Russellian constitutive cosmoprotopsychismRussellian monismPhysics, whether in the traditional or temporal naturalist dressing, seems to describe a causal chain of relational properties pertaining to events. In the philosophy of mind, these relational properties have been frequently called ‘dispositional properties’ with causal profiles dictated by nomic laws. In the context of the selective realism of mathematics in temporal naturalism, a restricted view of traditionally physical quantities is that they are the relational residue of extrinsically-accessible properties. Even if the relational nature of such extrinsic properties is non-nomic, as is possible in unstable states of the universe, the inconceivability of the perspectival, intrinsic character of qualia in the currently relational framework of physics is unaltered.Russellian monism, at least in one form, is a revisionist program which hopes to broaden physicalism to include intrinsic properties of events and systems i.e. properties which cannot be merely explained in relation to events external to the system concerned. Consciousness, then, might be explainable, at least partially, in terms of intrinsic properties. Since intrinsic properties need not be internal (i.e. lack causal efficacy), Russellian monism avoids the pitfalls of property dualism. However, a caveat is that such a monism can, in the manner of substance dualism, run into causal overdetermination issues, such as Howell’s 2014 modified exclusion principle, modelled based on Kim’s famous 2005 causal exclusion principle. Although I believe one can argue against this using the powerful tools of temporal naturalism, addressing this issue directly is beyond the scope of this essay.Panprotopsychism and cosmoprotopsychismA popular form of Russellian monism is panprotopsychism, which in the ‘smallist’ scheme (the commitment that properties of any collection of entities are entailed, whether constitutionally or causally, by the fundamental entities) is the view that all fundamental entities in physicalism 4 have protophenomenal 5 intrinsic properties, which causally accumulate over complex systems to give rise to phenomenal, internal properties. One could also suppose it is internal protophenomenal properties that give rise to internal macrophenomenal states, however, the former notion seems difficult to defend given the successes of physics when it comes to microcausal explanatory power. Furthermore, due to the transitive nature of causal connection, it is difficult to see how micro-internal properties can explain macro-intrinsic properties, without involving some form of emergentism, which would prima facie run into causal overdetermination issues assuming causal closure.An alternative to smallism is priority monism, which may be interpreted as saying that ultimately, the most stable natural kind is that of physical membership in the singular universe. In other words, properties of subsystems of the universe are entailed by those of the universe. A psychic implication of this is cosmoprotopsychism: parts of the universe being phenomenally conscious is entailed by the universe being protoconscious. Once again, taking causal closure to be a reasonable hypothesis, constitutive cosmoprotopsychism is an attractive view, wherein said cosmic entailment is non-causal.It is worth noting that Russellian constitutive cosmoprotopsychism, which we will call cosmic middle-range theories or CMR theories in short (as the minimal commitment is that fully phenomenal consciousness emerge at a level smaller than that of the protoconscious cosmos, without there being protophenomenal or phenomenal aspects to maximally fundamental i.e. small entities) are a form of Russellian monism that possess its virtues such as avoiding the duplicate objection, i.e. conceivability of zombie worlds, by furnishing missing phenomenal information with intrinsic physical information. Furthermore, CMR theories inherit the pros of panprotopsychism, such as being ‘gentle’ with regard to the combination problem, without immediately running into the cons of emergentism, property dualism, traditional physicalism or smallist panprotopsychism.Nonetheless, two relatively model-dependent issues remain open. Firstly, how would a CMR theory deal with the cosmic version of the combination problem, called the decombination problem — i.e., how does a protoconscious universe entail phenomenally conscious mid-range structures? Secondly, does such an entailment imply the same anti-physicalist connotation as epiphenomena on a physical substrate? (in the spirit of Kim, 1992).Temporal qualia in a speculative CMR hypothesisQualia as correlates of novel eventsWe return to empirical terra firma, or its closest cousin in temporal naturalism. In Smolin’s writings on the framework, he posits a philosophy of mind, which is elaborated with personal interpretations as follows.Firstly, consider the categorization of dispositional systems based on whether they internally contain precedents, in the sense of the principle of precedents. Associate to the two possible ensemble-level answers of yes/no the properties of proto-nomic and novel respectively. Such a categorization, although built using relational notions, is not relational at the level of organization of ensembles of precedents: Either such an ensemble is a unital class, hence contains a system without precedents, hence is novel, or, The ensemble is a non-unital class, hence contains a maximal system with multiple precedents, hence is proto-nomic. In both the cases above, the property of novelty of an ensemble is judged with respect to its own elements, and not those of any other. Since ensembles are equivalence classes, they are also partitions, therefore, the elements of an ensemble that enter novelty evaluation cannot belong to a distinct ensemble, by virtue of partitions being mutually exclusive. Therefore, ensemble novelty is a unary property.An implication of this is that temporal naturalism with PoP admits a non-relational categorization of dispositional systems (specifically, ensembles), which resembles Russellian monism! Furthermore, since universal membership is the only necessarily stable physical type given the singular existence of the universe and the inclusive reality of time, and since proto-nomic systems increasingly become deterministic, effectively relapsing to the non-intrinsic character of pure relationalism, a natural series of psychophysical hypotheses is motivated as follows: The universe has at least a single protophenomenal, intrinsic property in at least some relatively stable state of the universe. Increments in phenomenal character are associated with increments in novelty. In other words, phenomenal consciousness supervenes on novel events, in a monotonical manner. Hypothetically, events with high novelty would, at some stage of novelty and complexity, become temporal qualia.Self-organization of CMR structuresThe question of how mid-range phenomenal consciousness self-organizes from cosmic protophenomenal consciousness in some semi-stable cosmic state then reduces to: how do novel systems accumulate internal properties from a universal intrinsic property? If a mechanism satisfies the condition affirmatively, it could avoid the hard problem of mechanism while allowing further avenues to be opened for solving the hard problems of qualia and their evolutionary origin.The first problem has a straightforward solution given the power of PoP. If a property is intrinsic by virtue of universal membership, every system possesses the property. Now, novelty is naturally selected in the following manner: if a system is highly novel, it is highly likely that it has no precedents at a given time. Therefore, it will evolve stochastically, likely producing a new state which is even more novel, and so on. In fact, if a novel system contains a subdued proto-nomic part, this stochastic process will introduce noise, likely reducing the nomicity of all its parts. It follows that novel systems will likely keep accumulating novelty and hence, phenomenal particularity originating from an initial universal protoconsciousness. Exceptionally, the proto-nomic may become a source of counter-noise and eventually destroy the stable novel structure, in accordance with the inclusive reality of time.However, this still does not explain how such phenomenality will satisfy causal internalism. A possible explanation is that as a novel system self-organizes into an increasingly novel system, it loses clumps of proto-nomic parts via the above natural selection, thereby entering such causal relationships with proto-nomic parts of the universe that they differ significantly (such as in time scale) from the causal connection between proto-nomic parts among themselves. Without the availability of specific models, it is unfortunately possible to form logically valid counter-intuitions, such as, couldn’t novel systems causally interact to a considerable degree to form a more novel system and so on? This is a return of the problems of ownership and combination.Here, I will speculate that perhaps a missing element of the argument is that in certain circumstances, causation can effectively be approximated by local processes, for example, when the degree of organization in a causal network with locality is so high that such a property gets naturally selected for its increasing nomicity. However, such stability is temporary, as a central feature of a temporal, in-flux cosmos is that it is perhaps never a priori predictable when novel increments in the nomic or novel character of a region will amplify instead of descending into chaos. Such stable and chaotic conditions might constrain how novel systems causally interact. In one extreme case, they may effectively lose causal efficacy (but only for non-cosmic periods of time) with respect to other relatively stable systems, thereby giving rise to temporal qualia.Notice how, again at an effective i.e. approximate, era-level (as opposed to cosmic-time-scale-level) description, such qualia are similar to epiphenomena but can still possess causal efficacy over long periods of time. In general, the multifarious possibilities of temporally naturalized models of qualia evolution should allow one to impose constraints that give rise to effectively different kinds of mind-body relationships, at least prima facie. It should then be an important theoretical standard for such a framework to account for both the physical and mental phenomena which have coevally self-organized in this stable phase of the universe, in terms of ontologically prior entities (whether events or not) which are psychophysical.Conclusion: BiosemanticsTemporal qualia, as hypothesized above, significantly bridge the explanatory gap between the questions ‘why do qualia exist?’ and ‘how do qualia form in physical systems?’. This bridging was a direct result of temporal naturalism’s ability to express ‘why these properties?’ in the form of ‘why the self-organization of such properties?’, which illustrates the explanatory power gained by holding causation and protopsychic properties to be universal properties.It is historically interesting to remark that a common inspiration for Smolin’s temporal naturalism as well as the field of biosemiotics itself is Charles Sanders Peirce, whose concept of matter being “effete mind, inveterate habits becoming physical laws” is an anecdotal summary of the principle of precedence.That said, besides so many others, an important part of the mind-evolution problem still lies open: namely, the biological aspect, concerning questions such as, Why do gene-transmitting, reproducing structures form temporal qualia? Does this help in their survival, at the teleolonomical level of organization? Why does the mentality of such structures involve intentionality? For now, I will speculate on the second possibility as being a window to the first, as follows. Taking intentional, phenomenally conscious systems as data, temporal naturalism is a successful candidate for constructing philosophies of mind only if intentionality is somehow connected to novelty. It is clear that intentional systems, even at an abstract, symbolic level of organization, can involve correlations between intentional states and external structures. Due to the singular existence of the universe, we cannot ‘pre-encode’ intentionality at a cosmic level, but would rather want to demonstrate how it emerges effectively at the purely relational, subsystem level. Fortunately, intentionality, in its semantical-informational aspect, seems to be relational. Therefore, it is possible that details can be laid out about how intentional systems, being complex, specifically invoke novelty and hence temporal qualia, potentially bringing together access and phenomenal consciousness in a Russellian monistic expansion to traditional physicalism.References Alter, T., &amp;amp; Coleman, S. (2019). Panpsychism and Russellian Monism. The Routledge Handbook of Panpsychism, 230–242. https://doi.org/10.4324/9781315717708-20 Alter, T., &amp;amp; Pereboom, D. (2023, July 4). Russellian monism. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/russellian-monism/ Block, N. (1970, January 1). Concepts of Consciousness. PhilPapers. https://philpapers.org/rec/BLOCOC Brentano, F. (2015). The Distinction between Mental and Physical Phenomena. In Psychology from an Empirical Standpoint (pp. 59–77). essay, Routledge. Chalmers, D. (2014, July 14). How do you explain consciousness?. YouTube. https://youtu.be/uhRhtFFhNzQ Davidson, D. (1970). Donald Davidson, mental events. PhilPapers. https://philpapers.org/rec/DAVME-2 Dennett, D. C. (1997). True Believers: The Intentional Strategy and Why It Works. In Mind Design II: Philosophy, Psychology, and Artificial Intelligence (pp. 57–79). essay, MIT Press. Fodor, J. A. (1974, January 1). Special Sciences. PhilPapers. https://philpapers.org/rec/FODSS Goff, P. (2017). Panpsychism. The Blackwell Companion to Consciousness, 106–124. https://doi.org/10.1002/9781119132363.ch8 Goff, P., Seager, W., &amp;amp; Allen-Hermanson, S. (2022, May 13). Panpsychism. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/panpsychism/ Fodor, J. A. (1974, January 1). Special Sciences. PhilPapers. https://philpapers.org/rec/FODSS Jackson, F. (1982). Epiphenomenal Qualia. The Philosophical Quarterly, 32(127), 127. https://doi.org/10.2307/2960077 Kim, J. (1992). Multiple Realization and the Metaphysics of Reduction. Philosophy and Phenomenological Research, 52(1), 1. https://doi.org/10.2307/2107741 Millikan, R. G. (1989). Biosemantics. The Journal of Philosophy, 86(6), 281. https://doi.org/10.2307/2027123 Nagel, T. (1974). What Is It Like to Be a Bat? The Philosophical Review, 83(4), 435. https://doi.org/10.2307/2183914 Skrbina, D. (n.d.). Panpsychism. Internet Encyclopedia of Philosophy. https://iep.utm.edu/panpsych/ Smolin, L. (2015). Temporal naturalism. Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics, 52, 86–102. https://doi.org/10.1016/j.shpsb.2015.03.005 Smolin, L. (2022). On the Place of Qualia in a Relational Universe. Consciousness and Quantum Mechanics, 482–514. https://doi.org/10.1093/oso/9780197501665.003.0018 Tye, M. (2007). Philosophical Problems of Consciousness. The Blackwell Companion to Consciousness, 23–36. https://doi.org/10.1002/9780470751466.ch3 Unger, R. M., &amp;amp; Smolin, L. (2021). The Singular Universe and the Reality of Time: A Proposal in Natural Philosophy. Cambridge University Press. Van Gulick, R. (2014, January 14). Consciousness (The functional question: Why does consciousness exist?). Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/consciousness/#FunQueWhyDoeConExi Wilkins, J. S., &amp;amp; Bourrat, P. (2022, October 27). Replication and Reproduction. Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/entries/replication/ For brevity of explanation, the first and second theses have been exchanged with respect to their order in Smolin and Unger’s writings. &amp;#8617; As Smolin uses the terms, relational events are those whose properties can be explained by the properties of other events, in an event ontology. On the other hand, intrinsic events are non-relational but can enter causal relationships in their own right; lastly, internal events are intrinsic and do not enter causal relationships, resembling the quiddities of Russellian monism. &amp;#8617; I.e. non-plural; not to be confused with mathematical singularities, which by virtue of the selective realism of mathematics, are not committed to with regard to physical existence, in temporal naturalism. &amp;#8617; such as events in an event-ontological realization of temporal naturalism. &amp;#8617; Chalmers (2015) characterizes protophenomenal experience as lacking the quality of to be like something in the sense of Nagel (1974) while being positively intrinsic. &amp;#8617; " }, { "title": "A Functional Treatment of the Direct Product of Groups", "url": "/tempus-spatium/functional-treatment-direct-products-groups/", "categories": "abstract algebra", "tags": "direct products, function spaces", "date": "2024-05-30 00:00:00 -0400", "snippet": "Definition of direct productsLet \\(A\\) be an arbitrary well-ordered set and \\(\\{ G_{\\alpha} \\vert \\alpha \\in A \\}\\) a set of groups under the operations \\(\\{ *_{\\alpha} \\vert \\alpha \\in A \\}\\) respectively,\\[\\forall \\: \\alpha \\in A : *_{\\alpha} \\in \\left( G_{\\alpha} \\right)^{G_{\\alpha} \\times G_{\\alpha}}\\]One way to define the direct product of these groups is via the standard Cartesian product,The direct product of the entries of \\((G_{\\alpha} \\vert \\alpha \\in A)\\) for well-ordered set \\(A\\) is the set,\\[\\prod_{\\alpha \\in A} G_\\alpha = \\left\\{ \\left( a_{\\alpha} \\right)_{\\alpha \\in A} \\middle\\vert \\forall \\: \\alpha \\in A : a_{\\alpha} \\in G_{\\alpha} \\right\\}\\]equipped with the symmetric group operation \\(\\displaystyle{* : \\prod_{\\alpha \\in A} G_\\alpha \\times \\prod_{\\alpha \\in A} G_\\alpha \\to \\prod_{\\alpha \\in A} G_\\alpha}\\),\\[\\forall \\: a, b \\in \\prod_{\\alpha \\in A} G_\\alpha : a * b = (a_{\\alpha} *_{\\alpha} b_{\\alpha} \\vert \\alpha \\in A)\\]IsomorphismsLet \\(\\cong_{\\text{Set}}\\) denote set-theoretic isomorphism i.e. isomorphism in the category \\(\\text{Set}\\), which is equivalence of sets under bijective maps.1 In general, let \\(\\cong_C\\) denote isomorphism in a small category \\(C\\) i.e. one in the category \\(\\text{Cat}\\).Then, in the category \\(\\text{Grp}\\) of groups, by definition,\\[(G, *_{G}) \\cong_{\\text{Grp}} (H, *_{H}) \\iff \\left[ G \\overset{\\varphi : G \\to H}{\\cong_{\\text{Set}}} H \\right] \\land \\left[ \\forall \\: u, v \\in G : \\varphi(u *_{G} v) = \\varphi(u) *_{H} \\varphi(v) \\right]\\]Functional construction of direct productsStatementWe want to prove that the group-theoretic direct product is group-isomorphic to the following one,\\[G = \\left\\{ f \\in \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A} \\middle\\vert \\forall \\: \\alpha \\in A : f(\\alpha) \\in G_{\\alpha} \\right\\}\\]when equipped with the binary operation \\(\\displaystyle{*_{G}} : \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A} \\times \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A} \\to \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A}\\),\\[\\forall \\: f, g \\in G : f *_{G} g = f \\circ g\\]i.e.,\\[\\left( \\prod_{\\alpha \\in A} G_{\\alpha}, * \\right) \\cong_{\\text{Grp}} (G, *_{G})\\]ProofThe conditions for the group isomorphism of the above groups are satisfied as follows: As per the definition of direct products,\\[\\begin{align*}\\prod_{\\alpha \\in A} G_\\alpha &amp;amp; = \\left\\{ (a_{\\alpha})_{\\alpha \\in A} \\middle\\vert \\forall \\: \\alpha \\in A : a_{\\alpha} \\in G_{\\alpha} \\right\\} \\\\&amp;amp; = \\left\\{ (f(\\alpha))_{\\alpha \\in A} \\middle\\vert f \\in \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A}, \\forall \\: \\alpha \\in A : f(\\alpha) \\in G_{\\alpha} \\right\\} \\\\&amp;amp; \\overset{\\varphi}{\\cong_{\\text{Set}}} \\left\\{ \\text{im}_{f}(A) \\middle\\vert f \\in \\left(\\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A}, \\forall \\: \\alpha \\in A : f(\\alpha) \\in G_{\\alpha} \\right\\} \\\\&amp;amp; \\overset{\\eta}{\\cong_{\\text{Set}}} \\left\\{ f \\middle\\vert f \\in \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^{A}, \\forall \\: \\alpha \\in A : f(\\alpha) \\in G_{\\alpha} \\right\\} \\\\&amp;amp; = G\\end{align*}\\]where \\(\\displaystyle{\\varphi : \\prod_{\\alpha \\in A} G_{\\alpha} \\to \\bigcup_{\\alpha \\in A} G_{\\alpha}}\\) is the bijection,\\[\\forall \\: U \\in \\prod_{\\alpha \\in A} G_{\\alpha} : \\varphi(U) = \\set{U_{\\alpha}}{\\alpha \\in A}\\]with the inverse \\(\\displaystyle{\\varphi^{-1} : \\bigcup_{\\alpha \\in A} G_{\\alpha} \\to \\prod_{\\alpha \\in A} G_{\\alpha}}\\),\\[\\forall \\: V \\in \\bigcup_{\\alpha \\in A} G_{\\alpha} : \\varphi^{-1}(V) = \\left( \\alpha \\in V \\vert \\alpha \\in V \\right)\\]Similarly \\(\\displaystyle{\\eta : }\\) is the bijection \\(\\displaystyle{\\bigcup_{\\alpha \\in A} G_{\\alpha} \\to \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^A}\\),\\[\\forall \\: U \\in \\bigcup_{\\alpha \\in A} G_{\\alpha} : \\eta(U) = \\left\\{ (\\text{preim}_{f}(a), a) \\middle\\vert a \\in U \\right\\}\\]with the inverse \\(\\displaystyle{\\eta^{-1} : \\left(\\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^A \\to \\bigcup_{\\alpha \\in A} G_{\\alpha}}\\),\\[\\forall \\: V \\in \\left( \\bigcup_{\\alpha \\in A} G_{\\alpha} \\right)^A : \\eta^{-1}(V) = \\text{im}_{f}(V)\\]It can be verified that the composition of the above maps, i.e. is the bijection \\(\\displaystyle{\\eta \\circ \\varphi : \\prod_{\\alpha \\in A} G_{\\alpha} \\to G}\\),\\[\\forall \\: a \\in \\prod_{\\alpha \\in A} G_{\\alpha} : (\\eta \\circ \\varphi)(a) = \\eta(\\varphi(a)) = \\left\\{ (\\alpha, a_{\\alpha}) \\middle\\vert \\alpha \\in A \\right\\}\\]with the inverse \\((\\eta \\circ \\varphi)^{-1} = \\varphi^{-1} \\circ \\eta^{-1}\\). Using the definition of direct products and the condition for group isomorphism,\\[\\begin{align*}\\forall \\: a, b \\in \\prod_{\\alpha \\in A} G_{\\alpha} : (\\eta \\circ \\varphi) (a * b) &amp;amp; = (\\eta \\circ \\varphi) ((a_{\\alpha} *_{\\alpha} b_{\\alpha} \\vert \\alpha \\in A)) \\\\&amp;amp; = \\{ (\\alpha, a_{\\alpha} *_{\\alpha} b_{\\alpha}) \\vert \\alpha \\in A \\} \\\\&amp;amp; = \\{ (\\alpha, a_{\\alpha}) \\vert \\alpha \\in A \\} *_{G} \\{ (\\alpha, b_{\\alpha}) \\vert \\alpha \\in A \\} \\\\&amp;amp; = (\\eta \\circ \\varphi)(a) *_{G} (\\eta \\circ \\varphi) (b)\\end{align*}\\]as required. in the plural, as if \\(f \\in B^A\\) is a bijection, so are all the elements in \\(\\displaystyle{\\left\\{ \\underset{{f \\in U}}{\\bigcirc} f \\middle\\vert U \\in \\mathcal{P} \\left( B^A \\right) \\right\\}}\\) where \\(\\bigcirc\\) is recursive composition. For an infinite set of functions, one would use transfinite composition in the sense of category theory. &amp;#8617; " }, { "title": "The Question of &#39;Transitory Worlds&#39; in Plato&#39;s Metaphysics: Part 1", "url": "/tempus-spatium/transitory-worlds-plato-metaphysics/", "categories": "metaphysics", "tags": "concept innateness, idealism", "date": "2024-05-22 00:00:00 -0400", "snippet": "MotivationIn the Meno, Plato argues for concept innatism, based on the observation that concept empiricism at face value does not seem to explain how we seemingly acquire knowledge of universals even while being exposed only to particulars through our senses. For example, we can only observe swarms of acts exemplifying virtue, but to know that they instantiate virtue, we must already possess the deeper idea of what virtue is as a concept. Plato goes on to demonstrate how one of Meno’s slaves acquires knowledge of mathematical universals such as squares, even though the latter had never been trained in the subject.Suppose we think of principles as overarching constraints on how we think of concepts in some given field. Furthermore, let us say a hypothesis is a detailed proposition about a concerned field’s working, which incorporates its principles in a particular structure and seeks to either explain the principles in the context of the structure, or simply explore principle-consistent consequences of the proposition in the said context 1.For example, a principle of Newtonian mechanics is that of Galilean relativity, which is the statement that an observer moving with uniform velocity in some reference frame should not be able to measure their motion using any experiment in their frame alone (i.e., velocity is always relative to some other frame). A hypothesis consistent with the principle of Galilean relativity, in the particular framework of electrodynamics, is that since the electric field is a measurable quantity, a moving charge must have the same electric field as measured by an observer, whether the charge-observer supersystem is moving in its entirety or not, in some given reference frame.In this light, we can think of Plato as formulating concept innatism as a principle of epistemology, in his exposition. The nature of this formulation is dialectical, and he focuses more on the very logic of concepts such as virtue and its knowledge. Interestingly, the hypothesis Plato comes up with to flesh out and go over and beyond the epistemological principle of concept innatism, is the metaphysical notion of anamnesis. This is the conclusion of the argument: Knowledge is innate (this is motivated by the problem of knowledge of universals by acquaintance). A participant of the Socratic method is capable of forming inner knowledge of things (as seen with Meno’s slave), HENCE, Forming inner knowledge i.e. knowledge within one’s mind is recollection (as the knowledge must have been innate). One either acquires knowledge or has always possessed it (by definition). One does either of the above either in this life, or potentially, earlier. If one has always possessed knowledge, they would have no need for recollecting it. Socratic participants such as Meno’s slave cannot have acquired relevant knowledge in this life, THEREFORE, Such participants must have acquired said knowledge (such as that of universals) before this life and merely recollected it in this life.The corresponding dialogue from the Meno is from “(Socrates): What do you think, Meno? In giving his answers, has he expressed any opinion that was not his own?” to “(Meno): Here again, I think you are right, Socrates.”Interpretation of Plato’s AnamnesisIf we look at the above interpretation of Plato’s argument for anamnesis, we see that he draws metaphysical conclusions from semantical wordplay. It is no wonder that the philosopher Jaegwon Kim, in his article What is “Naturalized Epistemology”?, refers to the ‘true’ part of the traditional tripartite analysis of knowledge (justified-true-belief) as its semantical-metaphysical element. This also captures the dichotomous nature of truth as found in universals i.e. concepts and their meaning, as well as particulars we encounter around us. Therefore, one would expect that important aspects of Plato’s metaphysics of anamnesis are reflected in the semantical side of his abstract argument. With this in mind, let us see how the argument structurally functions.We see that Plato’s argument rests on the dialectics of ideas such as the acquiring, possession and recollection of knowledge; as well as general observations such as how Meno’s slave responds to questions in terms of previously unassessed true beliefs. So far, his argument seems valid if we try to think from his point of view. However, we must consider implicit principles (similar to principles in the sense discussed in the previous section) internal to his premises. In particular, premises (4) and (5) imply, at least at the stage of the argument in (5), that there are 4 things a person (or rather, a Platonic soul) could do: possess knowledge in this life. acquire knowledge in this life. possess knowledge from before this life. acquire knowledge before this life.Plato rules out the first two possibilities by arguing that Meno’s slave cannot have possessed or acquired knowledge of geometry in this life. The third possibility suggests possession of knowledge from an indefinitely distant point in the past, and before this life, which he rules out in premise (6). Therefore, we are only left with the last possibility.This is where a logician would say, “Hold up Plato! You’ve just used an extended form of the principle of excluded middle!”. In its simple form, the principle constrains how we think of possibilities and actualization: if we have potentially A or B and A cannot be actualized, then B must be actualized (as long as one of them must be). This is to avoid contradiction, as per the principle of contradiction, which together with the principle of identity (to the effect that every object is identical with itself) forms the three principles of thought. A fun fact is that principles for rational discourse itself can be traced back to Plato, and were extended into the framework of Aristotelian logic.The Question of Transitory WorldsIn my opinion, we should be careful before applying the principle of excluded middle to premises (4) and onward in Plato’s argument for anamnesis, because it seems to assume that the distinction between this life and his envisioned beforelife where we acquired knowledge of universals is not vague.In other words, what happens if we consider ‘transitory worlds’ between the beforelife with access to universals and this life? Or what if this life is a transitory world? There are many ways to interpret such a notion, and hence ask the question: Suppose the beforelife has access to universals, including The idea of particulars itself. If such a beforelife is a world of universals, how can it convey the idea of particulars to its full depth until it is actually experienced — via particulars — in this life? If the above process is unclear, would it be helpful to consider transitory worlds ‘between’ The beforelife with access to pure universals, and this one, will increasing amounts of perception of particulars? By the above reasoning, what if this life is a transitory world and the ‘next’ world is even more perception-based and further away from pure ideas? Another question that pops up is — is the first question as related to Russell’s paradox as it seems? There seems to be a hierarchy where universals precede particulars, and perhaps combining the two results in logical paradoxes. On a side note, it is interesting that Russell’s name came up, for, as he writes in this regard in his book A History of Western Philosophy (in the chapter ‘Plato’s Theory of Immortality’), Moreover, unless our existence before birth was not one of sense-perception, it would have been as incapable of generating the idea as this life is; and if our previous existence is supposed to have been partly super-sensible, why not make the same supposition concerning our present existence?In general, it feels like it would be interesting exploring the logical principles implicit in Plato’s hypotheses, and exploring how alternative logics would modify his metaphysics. For example, not being able to apply the principle of excluded middle to vague, transition-filled beforelife and present life might be a form of fuzzy logic. Similarly, there is some temporality implicit in his notion of beforelife, which must transcend the particular notion of time we experience in this life — is this a hint of temporal logic?These are the kind of questions which I hope to shed some light on (hopefully), in the next post in this series! Thanks for reading and stay tuned for more philosophy and new math/physics posts :) This is inspired by the phenomenal chapter ‘First, Principles’ in Lee Smolin’s very original book (as usual), Einstein’s Unfinished Revolution: The Search for What Lies Beyond the Quantum. &amp;#8617; " }, { "title": "A Closer Look at Quantum Measurements: Part 2.1 (Classical Stern-Gerlach Experiments)", "url": "/tempus-spatium/closer-look-quantum-measurements-part-2.1/", "categories": "quantum mechanics", "tags": "Stern-Gerlach experiments, measurement problem", "date": "2024-05-22 00:00:00 -0400", "snippet": "Why this postIn A Closer Look at Quantum Measurements (Slides), we looked at an abstract toy model for a quantum system whose associated quantum and classical probabilities (in the sense of ensemble theory) in experiments were interchangeable in a sense. This allowed one to equate, for a pure state, its seemingly impure post-objective-collapse density matrix and the original pure density matrix, automatically giving rise to the Born rule (as otherwise, the former would indeed be impure, leading to contradiction). This result was generalized for impure states prior to objective collapse, generalizing the form of the Born rule to ensembles.As discussed at the end of the mentioned post, as satisfying as the emergence of the Born rule from such considerations seems to be, such a machinery fails to operate in cases where classical and quantum probabilities simply differ — in fact, this happens when quantum probabilities exhibit interference, which has no classical analogue. 1One of the most important and famous types of experiments which highlights this above quantum weirdness is that of Stern-Gerlach. So, let us start with a clean slate (what one might call a slate’s vacuum state) and see why probability and the Born rule manifest themselves in quantum mechanics (as they have historically from Stern-Gerlach-type experiments, among others, in a series of crushing successes).To do so, this will be a two-subpart addendum to the series A Closer Look at Quantum Measurements. This first part will only look at part of what classical physics has to say with regard to Stern-Gerlach experiments.Basic setupConsider a polarized beam of light with a classical wavefunction \\(\\pmb{\\psi}\\), travelling along the \\(z\\)-axis, striking an analyzer \\(\\mathcal{A}_{\\theta}\\) that selects waves polarized along the vector:\\[\\widehat{\\pmb{e}}_{\\theta} = \\cos(\\theta) \\pmb{e}_x + \\sin(\\theta) \\pmb{e}_y\\]The polarization vector for \\(\\pmb{\\psi}\\) is given by the Jones 2-vector \\(\\vec{J}\\) when the wavefunction can be represented as,\\[\\pmb{\\psi}(\\pmb{x}, t) = \\Psi\\left(z \\: \\widehat{\\pmb{e}}_z, t, \\vec{\\phi}\\right) \\vec{A} = \\Phi\\left(z \\: \\widehat{\\pmb{e}}_z, t\\right) \\vec{J}\\]where $A^x, A^y$ represent the amplitudes of the \\(x\\) and \\(y\\) polarization states and \\(\\phi^x, \\phi^y\\) are analogous to their phase factors. Furthermore, \\(\\left\\lvert \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) \\right\\rvert = 1\\) for all \\(z, t \\in \\mathbb{R}\\). For plane waves, one can write,\\[\\begin{align*}\\psi^k(z \\: \\pmb{e}_z, t) &amp;amp; = \\Psi\\left(z \\: \\pmb{e}_z, t, \\phi^k \\: \\pmb{e}_k \\right) \\\\&amp;amp; = \\exp\\left(i(\\omega t - k z + \\phi^k)\\right) A^k \\\\&amp;amp; = \\exp\\left(i(\\omega t - k z)\\right) \\exp(i \\phi^k) A^k \\\\&amp;amp; = \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) J^k \\\\\\Phi(z \\: \\widehat{\\pmb{e}}_z, t) &amp;amp; = \\exp\\left(i(\\omega t - k z)\\right) \\\\J^k &amp;amp; = \\exp(i \\phi^k) A^k\\end{align*}\\]The results of such an experiment follow a trend general enough so that we can replace the polarized light ray with a stream of silver atoms possessing magnetic moments, selected by analyzers generating a magnetic field along some orientation, and so on.Now, let us see what classical physics predicts will happen in such a situation.FilteringAccording to classical physics, the output of \\(\\mathcal{A}_{\\theta}\\) is a beam travelling along \\(\\widehat{\\pmb{e}}_z\\) and polarized along \\(\\widehat{\\pmb{e}}_{\\theta}\\), with an amplitude \\(\\widehat{\\pmb{e}}_{\\theta} \\cdot \\widehat{J}\\). In other words, if the new beam has a wavefunction \\(\\pmb{\\psi}&#39;\\),\\[\\pmb{\\psi}&#39; = \\left( \\widehat{\\pmb{e}}_{\\theta} \\cdot \\widehat{J} \\right) \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) R_{\\theta - \\angle(\\vec{J})}\\left( \\vec{J} \\right)\\]where $R_{\\phi}$ is the standard 2-rotation by angle \\(\\phi \\in [0, 2 \\pi]\\) on the \\(xy\\)-plane and,\\[\\angle(\\vec{J}) = \\tan^{-1} \\left( \\frac{J^y}{J^x} \\right)\\]Since energy is proportional to amplitude squared in wave mechanics, the energy \\(E&#39;\\) of the \\(\\pmb{\\psi}&#39;\\)-wave is given in terms of that of the original $\\pmb{\\psi}$-wave as,\\[E&#39; = \\left\\lvert \\pmb{\\psi}&#39; \\right\\rvert^2 E = \\left( \\widehat{\\pmb{e}}_{\\theta} \\cdot \\widehat{J} \\right)^2 E\\]Due to the triangle inequality, $E_{\\pmb{\\psi}’} \\leq E_{\\pmb{\\psi}}$ and the two are equal if and only if $\\vec{J}$ is oriented along $\\pm \\widehat{\\pmb{e}}_{\\theta}$.Compound setupSuppose we add onto the basic setup as follows. We first pass a \\(\\vec{J}\\)-polarized \\(\\pmb{\\psi}\\)-wave through an analyzer \\(\\mathcal{A}_{\\theta}\\) to yield a \\(\\vec{J}&#39;\\)-polarized \\(\\pmb{\\psi}&#39;\\)-wave, and then we again pass it through a second analyzer \\(\\mathcal{A}_{\\theta&#39;}\\) to get a \\(\\vec{J}&#39;&#39;\\)-polarized \\(\\pmb{\\psi}&#39;&#39;\\)-wave as the output.Schematically,Here, \\(\\mathcal{S}\\) is a source, whose output is passed through \\(\\mathcal{A}_{\\angle(\\vec{J})}\\) to prepare the corresponding \\(\\pmb{\\psi}\\)-wave (assumed to have a non-zero amplitude). The final \\(\\pmb{\\psi}&#39;&#39;\\)-wave is directed to a measurement apparatus \\(\\mathcal{M}\\), such as a beam-splitter followed by perpendicular polarization detectors oriented along incrementing angles (as we shall see, the detector with no output measures the input wave’s polarization phase shifted by 90 degrees).By recursively applying the calculation in the basic setup,\\[\\begin{align*}\\pmb{\\psi}&#39; &amp;amp; = \\left( \\widehat{\\pmb{e}}_{\\theta} \\cdot \\widehat{J} \\right) \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) R_{\\theta - \\angle(\\vec{J})}\\left( \\vec{J} \\right) \\\\\\pmb{\\psi}&#39;&#39; &amp;amp; = \\left( \\widehat{\\pmb{e}}_{\\theta&#39;} \\cdot \\widehat{\\pmb{e}}_{\\theta} \\right) \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) R_{\\theta&#39; - \\theta}\\left( \\left( \\widehat{\\pmb{e}}_{\\theta} \\cdot \\widehat{J} \\right) R_{\\theta - \\angle(\\vec{J})}\\left( \\vec{J} \\right) \\right) \\\\&amp;amp; = \\cos(\\theta&#39; - \\theta) \\left( \\widehat{\\pmb{e}}_{\\theta} \\cdot \\widehat{J} \\right) \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) R_{\\theta&#39; - \\angle(\\vec{J})}\\left( \\vec{J} \\right)\\end{align*}\\]In fact, for a series of $n$ analyzers \\(\\mathcal{A}_{\\theta_1}, \\dots, \\mathcal{A}_{\\theta_n}\\), one has,\\[\\pmb{\\psi}_n = \\left( \\prod_{k=1}^{n-1} \\cos(\\theta_{k+1} - \\theta_k) \\right) \\left( \\widehat{\\pmb{e}}_{\\theta_1} \\cdot \\widehat{J} \\right) \\Phi(z \\: \\widehat{\\pmb{e}}_z, t) R_{\\theta_n - \\angle(\\vec{J})}\\left( \\vec{J} \\right)\\]Clearly, for any positive amount of light to exit from \\(\\mathcal{A}_{\\theta_n}\\), we must never have, for any step comprised of \\(\\mathcal{A}_{\\theta_k}, \\mathcal{A}_{\\theta_{k+1}}\\), that,\\[\\theta_{k+1} = \\theta_{k} + \\frac{(2l+1) \\pi}{2}, \\qquad l \\in \\mathbb{Z}\\]Therefore, up to signature and no flipping of the polarization of waves, information about a wave is conserved between intermediate interaction with filters, as long as none of them in the resultant chain of filters is oriented perpendicular to another.Huygens-Fresnel principle for wave impulsesBy the definition of the Dirac delta function \\(\\delta(\\pmb{x}&#39;)\\) over \\(\\mathbb{R}^3\\) centred at \\(\\pmb{x} \\in \\mathbb{R}\\),\\[\\pmb{\\psi}(\\pmb{x}, t) = \\int_{\\pmb{x}&#39; \\in \\mathbb{R}^3} d^3 x&#39; \\: \\delta(\\pmb{x}&#39; - \\pmb{x}) \\: \\pmb{\\psi}(\\pmb{x}&#39;, t)\\]Equivalently, and rather more rigorously, \\(d^3 x \\: \\delta(\\pmb{x}&#39; - \\pmb{x})\\) can be replaced in the above expression, with the pointwise indicator function \\(1_{\\pmb{x}}(\\pmb{x}&#39;)\\) which acts as a measure for integration over \\(\\mathbb{R}^3\\).What this means is that classically, a \\(\\pmb{\\psi}\\)-wave is a superposition of contributions from all points, which act like sources of \\(\\pmb{\\psi}\\)-impulse. This is reminiscent of the Huygens-Fresnel principle, where all points on wavefronts are sources of spherically dispersing waves. The bridge between this case and the one above lies in Fourier optics — where the natural wavefronts are planar, and planar wavefronts for the different sinusoidal contributions of a \\(\\pmb{\\psi}\\)-wave add up along different orientations to form more complicated wavefronts. Here, we are merely considering the contributions from individual points and how they add up, hence the need for impulses. One may also say that we are considering ‘pointlike wavefronts’, although the notion is somewhat paradoxical were it not for the machinery of measure theory.ConclusionThrough the above considerations, we have built up a little toolkit for analyzing Stern-Gerlach experiments classically, in a structure that I believe could be helpful to capture deep parallelisms with quantum theory. This will be the subject of the next subpart. As a remark, objective-collapse theories in general share the problem of not having been observed in any experiment as of May 2024. They also share the problem of not being easily reconcilable with special or general relativity. There are results to the effect that spontaneous collapse of ensembles with sufficient quantum randomness would defy relativity rather marginally, which may or may not be a problem depending on one’s position amidst the myriad of debates in quantum foundations. &amp;#8617; " }, { "title": "Analysis of the Three-body Problem (Report)", "url": "/tempus-spatium/analysis-three-body-problem/", "categories": "classical mechanics", "tags": "homotopy, non-holonomic mechanics, stability, N-body problem", "date": "2024-01-28 00:00:00 -0500", "snippet": "A little late to the party here, but Happy New Year friends! It’s been a while since the last post on here, so it’s time for shenanigans again! :DWhile the world has been turning a radian or two, I’ve been kept busy by life and, thankfully, the innumerable opportunities it has offered to travel, meet new people, explore music, art, hobbies, new university courses and so on. An upshot of this is that I’ve gotten invested in philosophy — especially epistemology, metaphysics, philosophy of physics and mathematics, logic and existentialism.So, this year, I’ll try my best to pull out some time to write philosophy posts including poetry that has been lurking on my tablet haha. I’m also hoping to continue working on algebraic topology and its significance in classical physics, culminating into an elaborate parallel to loop quantum gravity that we’ll call loop classical mechanics.Speaking of which, one of the purposes of this post, apart from saying hi (hi! :), is to upload a project that some friends and I had fun with last term. It’s a study of the classical three-body problem, which pedagogically starts from Appendix A (which is a homotopic approach to non-holonomic mechanics), and then goes on to introduce formal particles as canonical objects that ‘holonomize’ the N-body problem in a sense (these are the parts I worked on :). With this backdrop for the three-body problem, the report goes on to study its equations of motion, conserved quantities, orbits, Lyapunov stability and the associated eigenvalue problem (credit: Avery Cormier). The problem is then solved numerically using Python to find particular quasi-stable orbits and chaotic states, and plot the evolution of various parameters (credit: ZiLing Chen).Hope you have fun reading this!" }, { "title": "A Closer Look at Quantum Measurements (Slides)", "url": "/tempus-spatium/closer-look-quantum-measurements/", "categories": "quantum mechanics", "tags": "measurement problem, density operators, slides", "date": "2023-08-03 00:00:00 -0400", "snippet": " “What is particularly curious about quantum theory is that there can be actual physical effects arising from what philosophers refer to as counterfactuals - that is, things that might have happened, although they did not in fact happen.”— Roger PenroseThe measurement problemQuantum measurement has remained a disputed aspect of quantum mechanics for about a century. One of the first models for quantum measurement was projection-valued measures (PVM), later generalized to positive operator-valued measures (POVM). These further fall under the study of quantum operations, which describe the kind of transitions quantum systems can undergo. For example, POVM is generalized by introducing Kraus operators, which further describe how quantum systems change under measurement.In general, modelling quantum measurement warrants a study of how quantum systems interact with the measurement apparatus and an environment. This is the subject of quantum thermodynamics, which has revealed deep connections between the measurement problem and how open quantum systems (which are quantum systems interacting with an external quantum system which is typically a heat bath) work. In particular, open quantum systems with the Markovian property, together with their environment, form a completely unitary quantum system with no wavefunction collapse. The apparent wavefunction collapse of the open system, seemingly induced by its interaction with the environment, simply turns out to be an artefact of looking at a small part of a larger, unitary system.There are even more general models in quantum thermodynamics, such as that of open quantum systems without the Markovian property, or those with non-unitary evolution (for example, the GKSL formalism).It can be safely said that the research arena for the problem of measurement in quantum mechanics is vast.A toy modelTo begin playing around with the nature of quantum measurement, here’s a study of a toy model with lots of simplifications compared to the kind of models described above. In particular, we consider mixed states and interpret wavefunction collapse to do the following: collapse the states comprising the mixed states to new mixed states resembling ensembles of eigenstates of the measurable observed not change the density operator of the mixed state in the above transition! It turns out that such notions automatically give rise to the Born rule! As an alternative approach, we also model transition of mixed states to mixed states and construct a simplified picture of quantum operation. We can then apply this to transitions from mixed states to eigenstates of observables, to derive the standard Born rule.Here are some slides elaborating on this topic:CaveatsThe above model, being simplified, has some caveats: Upon closer inspection, we find that the probability that pure states transition to themselves is $1$, and yet, there is a non-zero probability of a pure state to transition to some other state. This is an artefact of not fully considering the space of states the transition pertains to. It turns out that the notion of transition we construct is fairly general in that now, we should consider all possible ways a transition can occur (in the Hilbert space of states) as opposed to via the evolution of some particular states arbitrarily chosen for mixed states. In fact, given the initial and final states between a transition, we should sum over all states contributing to such a transition. Consequently, a natural formalization of the toy model’s approach is the path integral formulation of quantum mechanics (interestingly, computing probabilities of transition between mixed states involves traces of products of their densities, which can be represented by tensor networks rudimentarily resembling Feynman diagrams; this will likely be explored in a future post). When dealing with notions such as the above, we should also be careful about how we interpret probabilities, typically using measure-theoretic notions. It turns out that the solution to the problem of transition probabilities adding up to more than $1$ (since a state can transition to other states and seemingly must transition to itself too) is to reconsider the meaning of the probability function used — it is, in fact, a probability density to be integrated with an appropriate measure. Therefore, in the slides, while we dealt with traces and such linear algebraic constructs accurately, the probabilities they were assigned to were not given much attention. The density operator for a mixed state involves classical uncertainity with respect to the ensemble of states. On the other hand, the density operator for a mixed state after measurement involves an ensemble of eigenstates, and the kind of uncertainty here is purely quantum and has no classical analogue. Therefore, equating the density operator before and after measurement, as we did, is generally not valid (for example, in Stern-Gerlach like experiments, where mixed states have discrete, but classical probability distributions of outcomes; while states in quantum superposition always end up in the same projected states determined by the orientation of the measurement apparatus — and the probabilities associated with these events do not look classical at all). ConclusionThere are likely many more caveats to the simplified model studied. What, then, does it describe? I’d say it provides insight on some important aspects of quantum mechanics that we would encounter in some more general situations, such as processes involving transition amplitudes, transition symmetry, models of wavefunction collapse for parts of unitary systems which yield the Born rule and so forth. After all, we do derive transition amplitudes, their symmetry, the Born rule from a simple form of wavefunction collapse, etc.This means that these things, usually involving more general and consistent approaches than in the slides, do have analogues in the simplified world of Copenhagen-type interpretations. And, we must remember that these analogues don’t all fit into a consistent system, which isn’t surprising as we are not using a constructive approach but instead, making some interesting observations that individually parallel complicated measurement-related phenomena.Above all, the purpose of this little project was to have fun playing around with measurement and hopefully, you will too! :)" }, { "title": "The Real Reason Nothing Travels Faster Than Light: Part 2 (Dynamical Interpretation)", "url": "/tempus-spatium/real-reason-nothing-travels-faster-than-light-part-2/", "categories": "relativity", "tags": "4-momentum", "date": "2023-07-19 00:00:00 -0400", "snippet": "In The Real Reason Nothing Travels Faster Than Light, we analytically proved that faster-than-light motion is logically impossible within the framework of special relativity. In doing so, we disproved the very kinematics of the situation \\(v&amp;gt;c\\). This, as we discussed, is logically stronger than disproving the dynamics of the situation, i.e. the question of how such a velocity can be achieved through, say, acceleration. In this post, we will specifically look at the dynamical aspect, and how it agrees with the stronger kinematical argument.When it comes to this topic, there are multiple layers of common misconception. The first is the supposition that disproving the dynamics also disproves the kinematics, i.e. if one can show that faster-than-light velocity cannot be achieved via any physical process, it must be theoretically impossible given the working of spacetime itself. While such an implication would be nice, it is not always true. After all, no object can be accelerated to the speed of light, and yet, light travels at, well, the speed of light! This makes sense in special relativity because things which are travelling at the speed of light cannot be accelerated in the first place, by virtue of having an invariant speed. In other words, the argument ‘impossible dynamics implies impossible kinematics’ implicitly assumes \\(v &amp;lt; c\\), which in turn assumes that we are dealing with objects carrying positive mass. Thus, the argument fails for \\(v \\geq c\\) and we are forced to base kinematical arguments solely on the kinematics as opposed to the dynamics.The second layer of popular misconception is the notion that theoretically, one would strictly need infinite energy to accelerate a body of positive mass from \\(v&amp;lt;c\\) to \\(v = c\\). It turns out that this is not true! In general, a body would need to convert all its mass to relativistic kinetic energy in order to approach the speed of light — but that means it would cease to exist as matter by the time it reaches the speed of light! 1PropulsionConservation lawsSuppose an accelerating body has its mass as a function of proper time, \\(m \\left( \\tau \\right)\\) (which is a Lorentz invariant). Assuming the body’s dynamics are invariant under linear translation, Noether’s theorem implies the conservation of total linear momentum of all the constituent parts of the body. 2In the case of a body accelerating by virtue of changing mass, there are two distinct parts: The ‘main’ part of the body whose accelerating motion is being tracked. The mass of this part evolves with proper time as \\(m \\left( \\tau \\right)\\). The part of the body thrust out from the main part. The mass of this evolves with proper time as \\(m \\left( 0 \\right) - m \\left( \\tau \\right)\\) assuming, without loss of generality, that the main body starts losing mass at \\(\\tau = 0\\). Let the 4-velocity of the first part in the frame of an observer tracking the body be labelled as \\(U^\\mu \\left( \\tau \\right)\\). Let the corresponding 3-speed be \\(u \\left( \\tau \\right) = u_i \\left( \\tau \\right) u^i \\left( \\tau \\right)\\) such that \\(U^i \\left( \\tau \\right) = \\gamma \\left( u_i \\left( \\tau \\right) u^i \\left( \\tau \\right) \\right) u^i \\left( \\tau \\right)\\). Similarly, let the 4-velocity and linked 3-speed of the ejection being thrust behind be denoted as \\(V^\\mu \\left( \\tau \\right)\\) and \\(v \\left( \\tau \\right)\\), respectively.Last but not the least, without loss of generality, let the initial 4-velocity of the body be zero.Then, by conservation of 4-momentum of the main and ejected parts of the body combined,\\[m \\left( \\tau \\right) U^\\mu \\left( \\tau \\right) + \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right] V^\\mu \\left( \\tau \\right) = 0\\]We will now focus only on the timelike part of the above equation,\\[\\begin{align}\\gamma \\left( u \\left( \\tau \\right) \\right) \\left[ m \\left( \\tau \\right) \\right]^2 c + \\gamma \\left( v \\left( \\tau \\right) \\right) \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^2 c &amp;amp; = 0 \\\\\\gamma \\left( u \\left( \\tau \\right) \\right) \\left[ m \\left( \\tau \\right) \\right]^2 + \\gamma \\left( v \\left( \\tau \\right) \\right) \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^2 &amp;amp; = 0\\end{align}\\]It is worth mentioning that multiplying both sides of the above equation by \\(c^2\\) turns it to a statement about the conservation of total relativistic kinetic energy. More on relativistic kinetic energy can be found on the previous post. Briefly, at the end of the linked post, we found that for a body of mass \\(m\\) moving with a 3-speed \\(v\\), its relativistic kinetic energy is given by \\(E^2 = \\left( mc^2 \\right) + \\left( \\gamma mvc \\right)^2\\). For bodies moving slower than the speed of light (or equivalently, those with positive mass), the equation can be transformed to yield,\\[\\begin{align}E^2 &amp;amp; = m^2 c^2 \\left( c^2 + \\gamma^2 v^2 \\right) \\\\&amp;amp; = m^2 c^4 \\left( 1 + \\frac{v^2}{c^2} \\frac{1}{1 - \\frac{v^2}{c^2}} \\right) \\\\&amp;amp; = m^2 c^4 \\left( 1 + \\frac{v^2}{c^2 - v^2} \\right) \\\\&amp;amp; = m^2 c^4 \\left( \\frac{c^2}{c^2 - v^2} \\right) \\\\&amp;amp; = m^2 c^4 \\left( \\frac{1}{1 - \\frac{v^2}{c^2}} \\right) \\\\&amp;amp; = \\gamma^2 m^2 c^4 \\\\\\therefore E &amp;amp; = \\gamma mc^2\\end{align}\\]Armed with these ideas, let us see how the conservation of total 4-momentum applies to an speeding particle on a diet.Solving for 3-speedUsing the timelike part of the equation for the conservation of the added 4-momenta of an accelerating particle and the mass ejected,\\[\\gamma \\left( u \\left( \\tau \\right) \\right) \\left[ m \\left( \\tau \\right) \\right]^2 + \\gamma \\left( v \\left( \\tau \\right) \\right) \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^2 = 0\\]\\[\\begin{align}c^{-1} \\gamma \\left( u \\left( \\tau \\right) \\right) &amp;amp; = - \\left[ m \\left( \\tau \\right) \\right]^{-2} c^{-1} \\gamma \\left( v \\left( \\tau \\right) \\right) \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^2 \\\\\\left( c^2 - \\left( u \\left( \\tau \\right) \\right)^2 \\right)^{-1/2} &amp;amp; = - \\left[ m \\left( \\tau \\right) \\right]^{-2} \\left[ c^2 - \\left( v \\left( \\tau \\right) \\right)^2 \\right]^{-1/2} \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^2 \\\\c^2 - \\left( u \\left( \\tau \\right) \\right)^2 &amp;amp; = \\left[ m \\left( \\tau \\right) \\right]^4 \\left[ c^2 - \\left( v \\left( \\tau \\right) \\right)^2 \\right] \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^{-4} \\\\\\left( u \\left( \\tau \\right) \\right)^2 &amp;amp; = c^2 - \\left[ m \\left( \\tau \\right) \\right]^4 \\left[ c^2 - \\left( v \\left( \\tau \\right) \\right)^2 \\right] \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^{-4} \\\\u \\left( \\tau \\right) &amp;amp; = c \\sqrt{1 - \\frac{\\left[ m \\left( \\tau \\right) \\right]^4}{\\left[ \\gamma \\left( v \\left( \\tau \\right) \\right) \\right]^2 \\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4}} \\\\&amp;amp; = c \\sqrt{1 - \\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right)} &amp;amp; [\\text{say}]\\end{align}\\]Clearly, the above 3-speed is: Lesser than \\(c\\) iff, \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) &amp;gt; 0\\). Equal to \\(c\\) iff, \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) = 0\\). Greater than \\(c\\) iff, \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) &amp;lt; 0\\). By construction, \\(m \\left( \\tau \\right) \\in \\left[ 0, \\infty \\right)\\) and \\(v \\left( \\tau \\right) \\in \\mathbb{R} \\implies \\gamma \\left( v \\left( \\tau \\right) \\right) \\in \\left( 1, \\infty \\right) \\cup i \\left( 0, \\infty \\right)\\). Therefore, \\(\\left[ \\gamma \\left( v \\left( \\tau \\right) \\right) \\right]^2 \\in \\left( - \\infty, 0 \\right) \\cup \\left( 1, \\infty \\right)\\). The below follows from these notions.Subluminal ejectionIf \\(v \\left( \\tau \\right) \\in \\left( -c, c \\right)\\), i.e. \\(\\gamma \\left( v \\left( \\tau \\right) \\right) \\in \\left( 1, \\infty \\right)\\), \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) &amp;gt; 0\\) iff, \\(m \\left( \\tau \\right) &amp;gt; 0\\) and \\(\\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4 \\in \\left( 0, \\infty \\right)\\). \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) = 0\\) iff, \\(m \\left( \\tau \\right) = 0\\) or \\(\\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4 = \\infty\\). \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) &amp;lt; 0\\) iff, \\(m \\left( \\tau \\right) &amp;gt; 0\\) and \\(\\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4 \\in \\left( - \\infty, 0 \\right)\\). The first situation describes how when the body’s mass is positive and decreasing, its velocity is always subluminal and vice-versa.On the other hand, the second case suggests that the body will reach the speed of light only when it entirely loses its mass (or has an infinite initial mass, which is unreasonable).Last but not the least, by construction, the third case is not possible as the masses involved are strictly real (and positive).Light-speed ejectionWhen \\(v \\left( \\tau \\right) = \\pm c\\), we have, \\(\\gamma \\left( v \\left( \\tau \\right) \\right) = \\infty\\), yielding \\(u \\left( \\tau \\right) = c\\).Hence, if any part of the accelerating body with non-zero mass is thrust out at the speed of light, the main part will also shoot off at the speed of light.Superluminal ejectionGiven \\(v \\left( \\tau \\right) \\in \\left( - \\infty, c \\right) \\cup \\left( c, \\infty \\right)\\) i.e. \\(\\left[ \\gamma \\left( v \\left( \\tau \\right) \\right) \\right]^2 \\in \\left( - \\infty, 0 \\right)\\), \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) &amp;gt; 0\\) iff, \\(m \\left( \\tau \\right) &amp;gt; 0\\) and \\(\\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4 \\in \\left( - \\infty, 0 \\right)\\). \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) = 0\\) iff, \\(m \\left( \\tau \\right) = 0\\) or \\(\\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4 = \\infty\\). \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right) &amp;lt; 0\\) iff, \\(m \\left( \\tau \\right) &amp;gt; 0\\) and \\(\\left[ m \\left( 0 \\right) - m \\left( \\tau \\right) \\right]^4 \\in \\left( 0, \\infty \\right)\\). These are identical to the situations in the case of subluminal ejection, except the first and third of those scenarios have been exchanged. Then, by similar reasoning as before, we find the following.The first situation is not possible due to the fact that mass is not allowed to be non-real, let alone negative.The second situation is reasonable only when the body loses its entire mass (as opposed to having infinite initial mass), thereby travelling at the speed of light.The third case entails superluminosity of the object when its rate of losing mass is positive.To summarize, decreasing mass is a general way to make an object approach the speed of light. If the ejection is subluminal, so is the body, while if the ejection occurs at the speed of light, then the body also travels at the speed of light and so on. In short, the nature of the speed of the object (subluminosity) is tied to that of the ejected part, via conservation of relativistic kinetic energy.Dynamics of propulsionDecreasing massFrom the above observations, as \\(m \\left( \\tau \\right) \\to 0^+\\) for some \\(\\tau \\in \\mathbb{R}\\), no matter the velocity of the ejected part, \\(u \\left( \\tau \\right)\\) approaches \\(c\\). It can be shown that in particular, when \\(\\left\\lvert v \\left( \\tau \\right) \\right\\rvert &amp;lt; c\\), \\(u \\left( \\tau \\right) \\to c^-\\) while when \\(v \\left( \\tau \\right) = c\\), \\(u \\left( \\tau \\right) \\to c\\) and lastly, when \\(\\left\\lvert v \\left( \\tau \\right) \\right\\rvert &amp;gt; c\\), \\(u \\left( \\tau \\right) \\to c^+\\), at least assuming that \\(\\displaystyle{\\exists \\: \\widetilde{\\tau} \\in \\mathbb{R} : \\lim_{\\tau \\to \\widetilde{\\tau}} m \\left( \\tau \\right) = 0^+}\\).Increasing ejection velocityAs \\(v \\left( \\tau \\right) \\to c\\) for some \\(\\tau \\in \\mathbb{R}\\), we see that \\(u \\left( \\tau \\right)\\) approaches \\(c\\). Specifically, for \\(v \\left( \\tau \\right) \\to c^-\\), \\(u \\left( \\tau \\right) \\to c^-\\). Similarly, when \\(v \\left( \\tau \\right) \\to c^+\\) around some \\(\\tau \\in \\mathbb{R}\\), \\(u \\left( \\tau \\right) \\to c^+\\). Last but not the least, at \\(v \\left( \\tau \\right) = c\\), \\(u \\left( \\tau \\right) = c\\).The above properties also demonstrate that \\(\\kappa \\left( m \\left( \\tau \\right), v \\left( \\tau \\right) \\right)\\) is continuous with respect to both its arguments.Summary + Desmos playgroundTo summarize the above ideas, a body with parts not moving at the speed of light, cannot reach the speed of light unless it eventually thrusts out all its parts!This neatly obeys the idea that a body with positive mass can only approach the speed of light but never reach it and that only massless objects travel at the speed of light. Such statements are kinematical in nature and as discussed, logically severe and it is nice to have verified that they hold even in extreme situations such as objects thrusting out their entire mass or ejecting parts with speeds approaching that of light (even though these facts are guarunteed from abstract arguments based on the nature of spacetime).As they say, a picture is worth a thousand words. So, here is a little Desmos graph showing the relationship between a self-propelling body’s mass \\(m \\left( \\tau \\right)\\), its speed \\(u \\left( \\tau \\right)\\) and the speed of the ejected material, \\(v \\left( \\tau \\right)\\),Feel free to play around with the quantities shown in the iframe by clicking the ‘edit graph on desmos’ button!And with that, folks, comes an end to our little two-post discussion on why nothing can travel faster than light :) That said, it is indeed true that if the body were to be accelerated to the speed of light without a change in its mass, it would require infinite energy. However, this case leaves out the possibility of the body propelling itself, which does not necessarily involve infinite energy. &amp;#8617; Though the linked post talks about linear momentum in classical mechanics, special relativity works in an analogous way, at least in the above context. &amp;#8617; " }, { "title": "Reconstructing Relativistic Kinetic Energy", "url": "/tempus-spatium/reconstructing-relativistic-kinetic-energy/", "categories": "relativity", "tags": "4-momentum, mass current density", "date": "2023-07-04 00:00:00 -0400", "snippet": "In classical mechanics, kinetic energy encodes the tendency of a system being tracked to move through space. Since relativity is based on the geometric setting of spacetime, one would expect that a relativistic analogue of kinetic energy talks about how a system moves through space and time.In this post, we will explore these notions of systems ‘moving through’ space and time, in terms of how the motion of mass in spacetime has an associated current which gives rise to momentum.Constructing 4-momentumWe begin by defining a 4-vector called 4-energy-momentum or 4-momentum in short. Analogous to 3-momentum, it is defined as,\\[P^\\mu = m U^\\mu = m \\frac{d X^\\mu}{d \\tau}\\]where \\(X^\\mu\\) represents the components of 4-position, which in ‘laboratory’ coordinates is of the form,\\[X^\\mu \\pmb{e}_\\mu = ct \\: \\pmb{e}_0 + x^i \\pmb{e}_i\\]In a holonomic basis, we have \\(\\pmb{e}_0 = \\frac{1}{c} \\partial_t\\) and \\(\\pmb{e}_i = \\partial_i\\).Another way to think of 4-momentum is that it is the conjugate momentum obtained from an appropriate action encoding dynamics in spacetime. For particles, this action would be of the form,\\[S = \\int_{\\tau_1}^{\\tau_2} mc^2 d \\tau\\]The intuition for the above action lies in the following facts: The Lagrangian must be a scalar related to the body’s dynamics. It must also be additive when combining non-interacting particles into larger systems. These severe restrictions leave only one kind of choice for the Lagrangian — the mass of the particle, up to linearity. The Lagrangian must be generalizable to general relativity. In general relativity, test particles travel along geodesics, which extremize the spacetime interval. This must therefore also apply to special relativity. These ideas suggest that the action along some path \\(C\\) with the endpoints having proper times \\(\\tau_1\\) and \\(\\tau_2\\) resembles the following:\\[\\begin{align}S &amp;amp; = \\int_C mc \\: ds \\\\&amp;amp; = \\int_{\\tau_1}^{\\tau_2} mc \\left( \\frac{dX_\\mu}{d \\tau} \\frac{dX^\\mu}{d \\tau} \\right)^\\frac{1}{2} d \\tau \\\\&amp;amp; = \\int_{\\tau_1}^{\\tau_2} mc^2 d \\tau\\end{align}\\]Thus, the appropriate conjugate momentum is of the form,\\[\\begin{align}P^\\mu &amp;amp; = \\eta^{\\mu \\nu} \\frac{\\partial L}{\\partial U^\\nu} \\\\&amp;amp; = \\eta^{\\mu \\nu} \\frac{\\partial}{\\partial U^\\nu} \\left( mc \\left( \\frac{dX_\\mu}{d \\tau} \\frac{dX^\\mu}{d \\tau} \\right)^\\frac{1}{2} \\right) \\\\&amp;amp; = mc \\eta^{\\mu \\nu} \\frac{\\partial}{\\partial U^\\nu} \\left( U_\\rho U^\\rho \\right)^\\frac{1}{2} \\\\&amp;amp; = mc \\eta^{\\mu \\nu} \\cdot \\frac{1}{2} \\left( U_\\rho U^\\rho \\right)^{- \\frac{1}{2}} \\cdot 2 U_\\rho \\delta^\\rho_{\\phantom{\\rho} \\nu} \\\\&amp;amp; = m c \\eta^{\\mu \\nu} \\cdot \\frac{1}{c} U_\\nu \\\\&amp;amp; = m U^\\mu\\end{align}\\]Interpreting 4-momentum4-momentum may be interpreted as a current vector field associated with mass, in spacetime. Such a current vector field, associated with some body of mass \\(m\\), may be defined as the vector field \\(J^\\mu \\left( x^\\nu \\right)\\) characterized by: A magnitude in spacetime, \\(J\\), which equals the amount of mass passing in unit proper time through the hypersurface of simultaneity \\(\\Sigma\\) (in a given frame) passing through \\(x^\\nu\\). Having the same direction as the motion of the mass at each point, i.e. having the same direction as the velocity vector field \\(U^\\mu\\). By construction, the mass current vector is its magnitude times the unit vector along 4-velocity, \\(\\frac{1}{c} U^\\mu\\).In a small proper time \\(d \\tau\\), the amount of mass \\(dm\\) which passes through \\(\\Sigma\\) is the mass of the traversing particle, \\(m\\), times the fraction of it that passes through \\(\\Sigma\\). This fraction is \\(\\frac{dX}{L}\\) where \\(dX\\) is the distance travelled in \\(d \\tau\\) by the particle perpendicular to \\(\\Sigma\\) and \\(L\\) is the proper length of the particle perpendicular to \\(\\Sigma\\). Furthermore, \\(dX = c d \\tau\\). We therefore have,\\[\\begin{align}dm &amp;amp; = \\frac{mc}{L} d \\tau \\\\J^\\mu &amp;amp; = \\frac{dm}{d \\tau} \\cdot \\frac{1}{c} U^\\mu \\\\&amp;amp; = \\frac{mc}{L} \\cdot \\frac{1}{c} U^\\mu \\\\&amp;amp; = \\frac{1}{L} m U^\\mu \\\\&amp;amp; = \\frac{1}{L} P^\\mu \\\\\\implies P^\\mu &amp;amp; = L J^\\mu\\end{align}\\]Thus, the 4-momentum of a particle travelling in some direction (with a non-zero proper length along it) is simply its current vector for mass scaled by the proper length of the particle in its direction of motion. For particles with zero proper length along the direction of motion, we need to be more careful.If a particle is completely localized, it has no length in any direction, i.e. \\(L = 0\\) in every frame. Its mass is also localized, resulting in its associated mass density field resembling a Dirac delta distribution:\\[\\rho \\left( x^\\mu; \\tau \\right) = m \\delta \\left( x^\\mu - X^\\mu \\left( \\tau \\right) \\right)\\]so that the total mass is as expected,\\[\\begin{align}\\int_{\\mathbb{R}^4} dx^4 \\rho \\left( x^\\mu; \\tau \\right) &amp;amp; = \\int_{\\mathbb{R}^4} dx^4 m \\delta \\left( x^\\mu - X^\\mu \\left( \\tau \\right) \\right) \\\\&amp;amp; = m\\end{align}\\]For a completely localized particle, its length along any direction, in the context of motion, may be taken as \\(d X\\). The intuition for this is that no matter how small a proper time \\(d \\tau\\) we pick, the length \\(c d \\tau\\) of the particle which passes through \\(\\Sigma\\), is always the whole of \\(L\\). This allows us to take \\(L = c d \\tau = dX\\). The particle’s 4-momentum is then,\\[\\begin{align}P^\\mu &amp;amp; = L J^\\mu \\\\&amp;amp; = dX \\cdot \\frac{dm}{d \\tau} \\cdot \\frac{1}{c} U^\\mu \\\\&amp;amp; = dX \\cdot \\frac{1}{d \\tau} \\left( \\frac{mc}{dX} d \\tau \\right) \\cdot \\frac{1}{U} U^\\mu \\\\&amp;amp; = m U^\\mu\\end{align}\\]Thus, even a completely localized particle has 4-momentum, simply equal to its current vector for mass.Relativistic kinetic energyAn important fact about 4-momentum is that since it is 4-velocity scaled by mass and 4-velocity is normalized to \\(c^2\\), we have,\\[P_\\mu P^\\mu = \\left( mc \\right)^2\\]This is known as the energy-momentum relation. To justify it, we first write the above equation in laboratory coordinates (essentially those in which \\(\\eta_{0 i}\\) disappear, \\(\\eta_{0 0}\\) can be normalized to \\(1\\) and \\(\\eta_{i j}\\) is some induced metric \\(- g_{i j}\\) for space):\\[P_0 P^0 - g_{i j} P^i P^j = \\left( mc \\right)^2\\]Since we have \\(P^i = m U^i = \\gamma m v^i\\),\\[\\left( P^0 \\right)^2 = \\left( mc \\right)^2 + \\left( \\gamma p \\right)^2\\]where \\(p = \\left( g_{i j} P^i P^j \\right)^\\frac{1}{2} = - \\left( P_i P^i \\right)^\\frac{1}{2}\\). It is useful to explicitly express both sides of the above equation in the units of energy, by multiplying by \\(c^2\\),\\[\\left( c P^0 \\right)^2 = \\left( mc^2 \\right)^2 + \\left( \\gamma p c \\right)^2\\]One way to understand the above equation is to replace \\(P^i\\)s with \\(J^i\\)s, since we saw that 4-momentum and the mass current vector of a point particle are one and the same despite being apparently different concepts:\\[\\left( c J^0 \\right)^2 = \\left( m c^2 \\right)^2 + \\left( jc \\right)^2\\]where \\(j = - \\left( P_i P^i \\right)^\\frac{1}{2} = \\left( g_{i j} P^i P^j \\right)^\\frac{1}{2} = \\gamma \\left( g_{i j} p^i p^j \\right)^\\frac{1}{2} = \\gamma p\\) for particles not moving at the speed of light. 1Einstein, by conceptually investigating the original form of the energy-momentum relation, interpreted the left hand side of the equation as the relativistic kinetic energy of a particle, by virtue of its motion in spacetime. Related to his interpretation is a power series approach to which we shall come across soon. Before treading there, let us see what the energy-momentum relation tells us in the ‘mass current form’:\\[E^2 = \\left( mc^2 \\right)^2 + \\left( j c \\right)^2\\]Clearly, in the rest frame of a particle, \\(j=0\\) and we have,\\[\\begin{align}E &amp;amp; = mc^2 \\\\&amp;amp; = mc \\cdot c \\\\&amp;amp; = c \\cdot m U^0 \\\\&amp;amp; = c J^0\\end{align}\\]In the above relationships, we have three objects of interest: the so-called relativistic kinetic energy \\(E\\), mass \\(m\\) and the timelike component of mass current.Firstly, we notice that \\(m = \\frac{1}{c} J^0\\), indicating that measuring the mass of a body corresponds to measuring its flow of matter through time, encoded in the timelike part of the mass current vector. 2 Secondly, \\(E = cJ^0\\), showing that \\(E\\) is imparted by the motion of mass along time. Last but not the least, the previous notion is expressed in terms of the mass flowing in time, by the celebrated equation \\(E = mc^2\\).In a frame moving with a velocity \\(v &amp;lt;&amp;lt; c\\) relative to the particle, we have \\(\\frac{v}{c} \\approx 0\\), yielding:\\[\\begin{align}E^2 &amp;amp; = \\left( mc^2 \\right)^2 + \\left( \\gamma m v c \\right)^2 \\\\&amp;amp; = \\left( mc^2 \\right)^2 \\left[ 1 + \\frac{\\gamma^2 v^2}{c^2} \\right] \\\\&amp;amp; = \\left[ 1 + \\frac{v^2}{c^2} \\left( 1 - \\frac{v^2}{c^2} \\right) \\right] \\left( mc^2 \\right)^2 \\\\&amp;amp; = \\left( 1 + \\frac{v^2}{c^2} - \\frac{v^4}{c^4} \\right) \\left( mc^2 \\right)^2 \\\\&amp;amp; \\approx \\left( 1 + \\frac{v^2}{c^2} + \\frac{1}{4} \\frac{v^4}{c^4} \\right) \\left( mc^2 \\right)^2 \\\\&amp;amp; = \\left[ \\left( 1 + \\frac{1}{2} \\frac{v^2}{c^2} \\right) mc^2 \\right]^2 \\\\\\implies E &amp;amp; \\approx \\left( 1 + \\frac{1}{2} \\frac{v^2}{c^2} \\right) mc^2 \\\\&amp;amp; = mc^2 + \\frac{1}{2} mv^2\\end{align}\\]Thus, in the non-relativistic limit, the relativistic kinetic energy still contains the non-relativistic kinetic energy.For all of the above reasons, it is appropriate to interpret \\(E\\) as relativistic kinetic energy by virtue of motion in spacetime. Here, the convention is that lowercase letters for 4-vectors denote non-relativistic vectors, whereas uppercase ones denote 4-vectors. Conversion from one to the another, is, therefore, done appropriately via the Lorentz factor. &amp;#8617; An interesting result of this idea is that an object moving at the speed of light cannot have mass. This is because of the following. Such a body would not appear to move along proper time. Therefore, there is no event where some non-zero \\(dm\\) passes through some hypersurface \\(\\Sigma\\) in a small time interval \\(dt\\), in inertial frames with \\(v&amp;lt;0\\) (a Lorentz-invariant statement). This suggests that the timelike part of mass current is null. &amp;#8617; " }, { "title": "Bundles in Classical Gauge Field Theory (Slides, In Progress)", "url": "/tempus-spatium/bundles-classical-gauge-field-theory/", "categories": "classical field theory", "tags": "bundles, gauge invariance, slides", "date": "2023-06-29 00:00:00 -0400", "snippet": "Field theory is fundamental to our modern understanding of the Cosmos. From classical fields to quantum fields, we see a startling complexity of incredibly wide-ranging physical phenomena emerge from relatively simpler structures and axioms.Often, the essence of these complicated mechanisms is captured by geometry, which among other (and somewhat bigger) things, makes it easier to see intricacy emerge from the deep structure underlying physical theories.In the context of relatively modern refinements of the construction of field theory, an important geometric structure is that of bundles (pedantically, they are topological in nature). The geometry and dynamics of a field theory can be encoded into the bundle structure of the theory, to the point that, for instance, classical fields are now defined after bundles (as sections of fibre bundles).My recent talk at the Canadian Undergraduate Mathematics Conference at the University of Toronto focused on trying to motivate the above developments in the context of classical field theory. Examples were drawn from Newtonian gravitation and classical electrodynamics, both being classical field theories.Here are the slides for the presentation (the source code can be found here):It is also my plan to add some more content to the slides in the next few weeks. For instance, there are classical field theories apart from Newtonian gravitation and classical electrodynamics — such as Klein-Gordon theory (which was the subject of my previous talk at the University of Waterloo). I will also add some more topological and geometric notions and proofs especially towards the end of the slides (in the original talk, the latter half was meant more for the purpose of intuition than rigour).It has been very fun and illuminating to learn new mathematics and physics from people at university, the conference, and on the internet. I hope to be able to spread some of this joy back through this post! :3" }, { "title": "Applying the Klein-Gordon Theory to Gravitation (Slides)", "url": "/tempus-spatium/applying-klein-gordon-theory-gravitation/", "categories": "classical field theory", "tags": "gravitation, Klein-Gordon theory, energy-momentum tensor, gauge invariance, slides", "date": "2023-02-23 00:00:00 -0500", "snippet": "Hi everyone, hope you’re all doing well :) In this short post, I’ll be sharing the slides from my recent presentation at SASMS (Short Attention Span Math Seminars), hosted by the Pure Math Club at the University of Waterloo.The presentation was on an informal construction of the ideas developed in some posts on this blog: Scalar Field Lagrangian From Symmetry Considerations Gauge Invariance in Classical Field Theory Scalar Field Lagrangian From Symmetry Considerations: Part 2 (Gauge Invariance) Deriving the Lagrangian Density for Newtonian GravitationIn the talk, I gave an overview of the upgradation of classical mechanics to classical field theory. The motivation for this was to examine the striking analogies between gravitation and electromagnetism, by putting them on a somewhat equal footing in terms of the language being used to describe them, i.e. field theory.It turns out that the more we delve into the structure of classical field theory, the more do the analogies between the said theories deepen. For instance, we later found in the exposition that Newtonian gravitation is a non-relativistic, 3-dimensional application of the Klein-Gordon theory, which in the relativistic, 4-dimensional setting, describes electrons (and couples interestingly with electromagnetism in scalar electrodynamics and quantum field theory).This, as was discussed in the talk, is not a coincidence; in classical field theory, the condition of the field-theoretic energy-momentum tensor being symmetric is a severe one, which ends up making any classical field theory obeying the condition, a Klein-Gordon theory!Armed with this understanding, we found a field-theoretic formulation for Newtonian gravitation by treating its equation of motion, the Poisson equation, to resemble the Klein-Gordon equation. Finally, we incorporated matter fields into the model by considering a Lagrangian density for matter and imposing that when varied with respect to the gravitational field, it must yield the mass density field characterizing the matter, as we would expect in a theory respecting both gravitation, matter and their interaction.Without further ado, here are the slides: 1Overall, it was super fun preparing the document and being able to present it to an involved audience. Working on this topic also meant getting a fresh perspective on variational principles, Klein-Gordon analysis, gauge theories and so on, which will be implemented in this blog in the future.That brings us to a little note I’d like to conclude this post with. It’s that we’ll be having plenty of related posts on this blog as soon as possible! They’ll be to do with special relativity, classical field theory, analysis and, surprise, topology. It might still take a couple of months to get them published given that they’re taking time to read up and write on. Until then, current progress and future ideas for this blog can be seen on its new Notion database!And with that, folks, have a good one! :) The source code for the document can be found here. &amp;#8617; " }, { "title": "A Brief Geometric Analysis of Harmonic Oscillators: Part 3 (Matrix Exponentials)", "url": "/tempus-spatium/geometric-analysis-harmonic-oscillators-part-3/", "categories": "classical mechanics", "tags": "harmonic oscillators, phase space, translation operator, Euler formula, tensors", "date": "2023-01-27 00:00:00 -0500", "snippet": "In A Brief Geometric Analysis of Harmonic Oscillators: Part 2 (Tensor Algebra), we generalized the methods used in A Brief Geometric Analysis of Harmonic Oscillators to analyze the behaviour of harmonic oscillators in phase space.We began with Hooke’s law, \\(\\omega^{-1} \\ddot{x} + \\omega x = 0\\) and explored the Hamiltonian flow described by it in the phase space \\(S\\) represented by \\(\\left( x, y \\right)\\) where \\(y = \\omega^{-1} \\dot{x}\\). We then showed using geometric notions that this flow resembles concentric circles in phase space, with the phase \\(\\theta \\left( t \\right)\\) being proportional to \\(t\\) by a factor of \\(\\omega\\), i.e. \\(\\theta \\left( t \\right) = \\omega t\\).In this post, we will explicitly find the solution for the position vector \\(\\left( x, y \\right)\\) in phase space and derive the phase as a consequence of the same.Hamilton’s equationsAs found in the previously mentioned posts, in phase space, the equation of motion for a harmonic oscillator is ingrained in Hamilton’s equations of motion,\\[\\dot{\\pmb{u}} = \\pmb{\\omega} \\left( \\pmb{u} \\right)\\]where \\(\\pmb{\\omega} = \\omega \\left( \\partial_x \\otimes \\text{d} y - \\partial_y \\otimes \\text{d} x \\right)\\) and \\(\\pmb{u} = x \\partial_x + y \\partial_y = x \\partial_x + \\dot{x} \\partial_{\\dot{x}}\\).In the matrix representation,\\[\\begin{pmatrix} \\dot{x} \\\\ \\dot{y} \\end{pmatrix} = \\omega \\begin{pmatrix} 0 &amp;amp; 1 \\\\ -1 &amp;amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\]Now, we will try to solve for \\(\\pmb{u} \\left( t \\right)\\).Time translation operatorSuppose we want to construct a time translation operator \\(\\mathcal{U}\\) such that,\\[\\mathcal{U}^t \\pmb{u} \\left( 0 \\right) = \\pmb{u} \\left( t \\right)\\]The exponentiation of the time translation operator has to do with the way its compositions (products) add up in terms of their arguments,\\[\\prod_{k} \\mathcal{U}^{t_k} = \\mathcal{U}^{\\sum_k t_k}\\]With this in mind, we have, using a series expansion of the time translation operator,\\[\\begin{align}\\mathcal{U}^t &amp;amp;= \\sum_{n=0}^\\infty \\frac{1}{n!} t^n \\frac{d^n}{dt^n} \\mathcal{U}^t \\Bigg|_{t=0} \\\\ &amp;amp; = \\exp \\left( t \\frac{d}{dt} \\right) \\mathcal{U}^0 \\\\ &amp;amp; = \\exp \\left( t \\frac{d}{dt} \\right) \\mathcal{I} \\\\ &amp;amp; = \\exp \\left( t \\frac{d}{dt} \\right)\\end{align}\\]where \\(\\mathcal{I}\\) is the identity map \\(\\text{id}_S\\).Now, we perform a little trick. Since we imagine \\(\\mathcal{U}\\) to act on \\(\\pmb{u} \\left( t \\right)\\), the linear operator \\(\\frac{d}{dt}\\) is the linear map \\(\\widehat{L} : \\pmb{u} \\to \\dot{\\pmb{u}}\\). From Hamilton’s equations of motion, we know that this map is nothing but \\(\\pmb{\\omega}\\). Let us write \\(\\pmb{\\omega} = \\omega \\pmb{J}\\) where \\(\\pmb{J} = \\partial_x \\otimes \\text{d} y - \\partial_y \\otimes \\text{d} x\\). Then, we have from the above equation,\\[\\mathcal{U}^t = \\exp \\left( \\omega t \\pmb{J} \\right)\\]Therefore, from the definition of the time translation operator, we have,\\[\\pmb{u} \\left( t \\right) = \\exp \\left( \\omega t \\pmb{J} \\right) \\: \\pmb{u} \\left( 0 \\right)\\]where,\\[\\exp \\left( \\omega t \\pmb{J} \\right) = \\sum_{n=0}^\\infty \\frac{1}{n!} \\omega^n t^n \\pmb{J}^n = \\mathcal{U}^t\\]In principle, this is indeed an explicit solution for \\(\\pmb{u} \\left( t \\right)\\) in terms of initial conditions \\(\\pmb{u} \\left( 0 \\right)\\). However, to extract the notion of periodicity from the exponential, we will need to derive Euler’s formula for matrix exponentials. 1Euler’s formula for the time translation operatorTo make sense of the solution we derived above for the position vector in phase space, we will do the following:\\[\\mathcal{U}^t = \\exp \\left( \\omega t \\pmb{J} \\right)\\]Some useful facts to simplify the above expression are,\\[\\begin{align}\\pmb{J}^0 &amp;amp; = \\pmb{\\delta} \\\\\\pmb{J}^1 &amp;amp; = \\pmb{J} \\\\\\pmb{J}^2 &amp;amp; = \\left( \\partial_x \\otimes \\text{d} y - \\partial_y \\otimes \\text{d} x \\right) \\left( \\partial_x \\otimes \\text{d} y - \\partial_y \\otimes \\text{d} x \\right) \\\\ &amp;amp; = - \\partial_x \\otimes \\text{d} x - \\partial_y \\otimes \\text{d} y \\\\ &amp;amp; = - \\delta^i_{\\phantom{i} j} \\: \\partial_i \\otimes \\text{d} x^j \\\\ &amp;amp; = - \\pmb{\\delta} \\\\\\pmb{J}^3 &amp;amp; = \\pmb{J}^2 \\pmb{J} \\\\ &amp;amp; = - \\pmb{\\delta} \\pmb{J} \\\\ &amp;amp; = - \\pmb{J} \\\\\\pmb{J}^4 &amp;amp; = \\pmb{J}^3 \\pmb{J} \\\\ &amp;amp; = - \\pmb{J} \\pmb{J} \\\\ &amp;amp; = - \\pmb{J}^2 \\\\ &amp;amp; = \\pmb{\\delta} \\\\ &amp;amp; \\vdots \\\\\\pmb{J}^n &amp;amp; = \\pmb{J}^{n+4k} \\: \\forall \\: n, k \\in \\mathbb{Z}\\end{align}\\]The above can be summarized in the following commutative diagram,Armed with this knowledge,let us collect even and odd terms in the power series for \\(\\mathcal{U}^t\\),\\[\\begin{align}\\mathcal{U}^t &amp;amp; = \\sum_{n=0}^\\infty \\frac{1}{n!} \\omega^n t^n \\pmb{J}^n \\\\ &amp;amp; = \\sum_{n=0}^\\infty \\frac{1}{\\left( 2n \\right)!} \\omega^{2n} t^{2n} \\pmb{J}^{2n} + \\sum_{n=0}^\\infty \\frac{1}{\\left( 2n+1 \\right)!} \\omega^{2n+1} t^{2n+1} \\pmb{J}^{2n+1} \\\\ &amp;amp; = \\sum_{n=0}^\\infty \\frac{\\left( -1 \\right)^n}{\\left( 2n \\right)!} \\omega^{2n} t^{2n} \\pmb{\\delta}^{2n} + \\sum_{n=0}^\\infty \\frac{\\left( -1 \\right)^n}{\\left( 2n+1 \\right)!} \\omega^{2n+1} t^{2n+1} \\pmb{J} \\\\ &amp;amp; = \\cos \\left( \\omega t \\right) \\pmb{\\delta} + \\sin \\left( \\omega t \\right) \\pmb{J}\\end{align}\\]Thus,\\[\\begin{align}\\pmb{u} \\left( t \\right) &amp;amp; = \\mathcal{U}^t \\pmb{u} \\left( 0 \\right) \\\\ &amp;amp; = \\left[ \\cos \\left( \\omega t \\right) \\pmb{\\delta} + \\sin \\left( \\omega t \\right) \\pmb{J} \\right] \\pmb{u} \\left( 0 \\right) \\\\ &amp;amp; = \\cos \\left( \\omega t \\right) \\pmb{u} \\left( 0 \\right) + \\sin \\left( \\omega t \\right) \\pmb{J} \\pmb{u} \\left( 0 \\right) \\\\ &amp;amp; = \\cos \\left( \\omega t \\right) \\left( x \\left( 0 \\right) \\partial_x + y \\left( 0 \\right) \\partial_y \\right) + \\sin \\left( \\omega t \\right) \\left( \\partial_x \\otimes \\text{d} y - \\partial_y \\otimes \\text{d} x \\right) \\left( x \\left( 0 \\right) \\partial_x + y \\left( 0 \\right) \\partial_y \\right) \\\\ &amp;amp; = \\cos \\left( \\omega t \\right) \\left( x \\left( 0 \\right) \\partial_x + y \\left( 0 \\right) \\partial_y \\right) + \\sin \\left( \\omega t \\right) \\left( y \\left( 0 \\right) \\partial_x - x \\left( 0 \\right) \\partial_y \\right) \\\\ &amp;amp; = \\left( \\cos \\left( \\omega t \\right) x \\left( 0 \\right) + \\sin \\left( \\omega t \\right) y \\left( 0 \\right) \\right) \\partial_x + \\left( \\cos \\left( \\omega t \\right) y \\left( 0 \\right) - \\sin \\left( \\omega t \\right) x \\left( 0 \\right) \\right) \\partial_y\\end{align}\\]Once again, in the matrix representation,\\[\\begin{align}\\mathcal{U}^t &amp;amp; = \\cos \\left( \\omega t \\right) \\begin{pmatrix} 1 &amp;amp; 0 \\\\ 0 &amp;amp; 1 \\end{pmatrix} + \\sin \\left( \\omega t \\right) \\begin{pmatrix} 0 &amp;amp; 1 \\\\ -1 &amp;amp; 0 \\end{pmatrix} \\\\ &amp;amp; = \\begin{pmatrix} \\cos \\left( \\omega t \\right) &amp;amp; \\sin \\left( \\omega t \\right) \\\\ - \\sin \\left( \\omega t \\right) &amp;amp; \\cos \\left( \\omega t \\right) \\end{pmatrix} \\\\ \\pmb{u} \\left( t \\right) &amp;amp; = \\mathcal{U}^t \\pmb{u} \\left( 0 \\right) \\\\ &amp;amp; = \\begin{pmatrix} \\cos \\left( \\omega t \\right) &amp;amp; \\sin \\left( \\omega t \\right) \\\\ - \\sin \\left( \\omega t \\right) &amp;amp; \\cos \\left( \\omega t \\right) \\end{pmatrix} \\begin{pmatrix} x \\left( 0 \\right) \\\\ y \\left( 0 \\right) \\end{pmatrix} \\\\ &amp;amp; = \\begin{pmatrix} \\cos \\left( \\omega t \\right) x \\left( 0 \\right) + \\sin \\left( \\omega t \\right) y \\left( 0 \\right) \\\\ - \\sin \\left( \\omega t \\right) x \\left( 0 \\right) + \\cos \\left( \\omega t \\right) y \\left( 0 \\right) \\end{pmatrix}\\end{align}\\]From this, we can concretely conclude that \\(\\pmb{u} \\left( t \\right)\\) is a periodic function with a period of \\(T = \\frac{2 \\pi}{\\omega}\\). Furthermore, the argument \\(\\omega t\\) assumes the role of an angle parameter \\(\\theta \\left( t \\right)\\) in characterizing \\(\\pmb{u} \\left( t \\right)\\).ConclusionThrough a series of posts, we have managed to demonstrate the periodic evolution of harmonic oscillators, following as a geometric consequence of the structure of their equation of motion. The basis for this argument was the Hamiltonian flow of a harmonic oscillator in phase space, which was then explored using a range of mathematical toolkits, namely: vector calculus, tensor algebra and finally, matrix exponentials.This analysis hopefully lays the groundwork for building theories of more complicated systems resembling harmonic oscillators, such as Klein-Gordon fields and quantum harmonic oscillators — topics we will want to explore on this blog soon.As always, thanks so much for reading and feel free to leave suggestions or any other comments below :) Pedantically, matrices are only representations of underlying tensors. What we will really be doing is deriving Euler’s formula for antisymmetric tensors. &amp;#8617; " }, { "title": "Homogeneity From Additivity for Linear Operators on a Real Vector Space", "url": "/tempus-spatium/homogeneity-from-additivity-linear-operators-real-vector-space/", "categories": "analysis", "tags": "linear operators, Dedekind cuts", "date": "2023-01-02 00:00:00 -0500", "snippet": "StatementConsider the 2 well-known properties of a linear operator \\(T : \\mathbb{R}^m \\to \\mathbb{R}^n : m, n \\in \\mathbb{N}\\),\\[\\begin{align}T \\left( \\sum_a \\pmb{u}_a \\right) &amp;amp; = \\sum_a T \\left( \\pmb{u}_a \\right) &amp;amp; \\forall \\: \\pmb{u}_a \\in \\mathbb{R}^m &amp;amp;&amp;amp; \\left( 1 \\right) \\\\T \\left( c \\pmb{u} \\right) &amp;amp; = c T \\left( \\pmb{u} \\right) &amp;amp; \\forall \\: c \\in \\mathbb{R}, \\pmb{u} \\in \\mathbb{R}^m &amp;amp;&amp;amp; \\left(2 \\right)\\end{align}\\]Proposition: Property \\(\\left( 2 \\right)\\) (homogeneity) can be derived from property \\(\\left( 1 \\right)\\) (additivity) using the underlying structure of \\(\\mathbb{R}\\).Thus, we propose that linearity can be unambiguously defined using axiom \\(\\left( 1 \\right)\\) only, at least up to \\(\\mathbb{R}^m\\).Overview of proof: Represent an arbitrary \\(c \\in \\mathbb{R}\\) as an infinite series of rationals, using Dedekind cuts. Plug the result into \\(\\left( 1 \\right)\\) and manipulate to obtain \\(\\left( 2 \\right)\\).Let us now elaborate on the above.Reals as infinite series of rationalsSuppose we are given any \\(c \\in \\mathbb{R}\\). The construction of \\(\\mathbb{R}\\) from \\(\\mathbb{Q}\\) using Dedekind cuts guarantees the following:\\[\\forall \\: c \\in \\mathbb{R} : \\exists \\: \\left\\{ a_n : n \\in \\mathbb{N} \\right\\} : c = \\lim_{n \\to \\infty} a_n, a_n \\in \\mathbb{Q} \\: \\forall \\: n \\in \\mathbb{N}\\]Now, we can define another sequence of rationals \\(\\displaystyle{ \\left\\{ b_k : k \\in \\mathbb{N} \\right\\} : \\sum_{k=1}^n b_k = a_n }\\). It follows,\\[\\begin{align}b_n &amp;amp; = \\sum_{k=1}^n b_k - \\sum_{k=1}^{n-1} b_k \\\\ &amp;amp; = a_n - a_{n-1} \\\\c &amp;amp; = \\lim_{n \\to \\infty} a_n \\\\ &amp;amp; = \\lim_{n \\to \\infty} \\sum_{k=1}^n b_k \\\\ &amp;amp; = \\sum_{k=1}^\\infty b_k\\end{align}\\]Thus, every real number can be represented as an infinite series of rationals,\\[\\implies \\forall \\: c \\in \\mathbb{R} : \\exists \\: \\left\\{ b_k : k \\in \\mathbb{N} \\right\\} : c = \\sum_{k=1}^\\infty b_k, b_k \\in \\mathbb{Q} \\: \\forall \\: k \\in \\mathbb{N}\\]Furthermore, by definition, every rational is some integer divided by some non-zero integer,\\[\\forall \\: b \\in \\mathbb{Q} : \\exists \\: p \\in \\mathbb{Z}, q \\in \\mathbb{Z} \\backslash \\left\\{ 0 \\right\\} : b = \\frac{p}{q}\\]Let us proceed to derive homogeneity from additivity for the special case \\(c \\in \\mathbb{Q}\\) (however, we will still have \\(\\pmb{u} \\in \\mathbb{R}^m\\)). Then, we will apply the result above to extend our observations to \\(c \\in \\mathbb{R}\\).Suppose \\(c \\in \\mathbb{Q} , \\pmb{u} \\in \\mathbb{R}^m\\). Consider a linear operator \\(T : \\mathbb{R}^m \\to \\mathbb{R}^n\\) which obeys additivity i.e. property \\(\\left( 1 \\right)\\). Let us express \\(c\\) as \\(c = \\frac{p}{q} : p \\in \\mathbb{Z}, q \\in \\mathbb{Z} \\backslash \\left\\{ 0 \\right\\}\\). The signature of \\(c\\) can be encoded in either the numerator \\(p\\), which, pedantically speaking, is preferred over \\(q\\) as the domain of the former is the entirety of \\(\\mathbb{Z}\\).For \\(c \\in \\mathbb{Z}\\)We will first derive homogeneity from additivity for the simple case \\(q=1, p \\in \\mathbb{Z} \\implies c \\in \\mathbb{Z}\\). To do so, we will investigate the individual scenarios \\(p \\in \\mathbb{Z}^+, p=0, p \\in \\mathbb{Z}^-\\).For \\(c \\in \\mathbb{Z}^+ = \\mathbb{N}\\)We have,\\[\\begin{align}T \\left( c \\pmb{u} \\right) &amp;amp; = T \\left( p \\pmb{u} \\right) : p \\in \\mathbb{N} \\\\ &amp;amp; = T \\left( \\sum_{a = 1}^p u \\right)\\end{align}\\]By additivity,\\[\\begin{align}T \\left( c \\pmb{u} \\right) &amp;amp; = \\sum_{a=1}^p T \\left( \\pmb{u} \\right) \\\\ &amp;amp; = p T \\left( \\pmb{u} \\right) \\\\ &amp;amp; = c T \\left( \\pmb{u} \\right) &amp;amp; \\square\\end{align}\\]For \\(c=0\\)For \\(c=0\\), we have,\\[\\begin{align}T \\left( 0 \\cdot \\pmb{u} \\right) &amp;amp; = T \\left( \\pmb{0}_m \\right) \\\\ &amp;amp; = T \\left( \\pmb{0}_m + \\pmb{0}_m \\right) \\\\ &amp;amp; = T \\left( \\pmb{0}_m \\right) + T \\left( \\pmb{0}_m \\right) \\\\T \\left( \\pmb{0}_m \\right) &amp;amp; = T \\left( \\pmb{0}_m \\right) + T \\left( \\pmb{0}_m \\right) \\\\ \\implies T \\left( \\pmb{0}_m \\right) &amp;amp; = \\pmb{0}_n\\end{align}\\]where \\(\\pmb{0}_k\\) is the null vector in \\(\\mathbb{R}^k\\).For \\(c \\in \\mathbb{Z}^-\\)Let \\(p = -n : n \\in \\mathbb{Z}^+\\),\\[\\begin{align}\\pmb{0}_n &amp;amp; = T \\left( \\pmb{0}_m \\right) \\\\ &amp;amp; = T \\left( n \\pmb{u} - n \\pmb{u} \\right)\\end{align}\\]By additivity,\\[\\begin{align}\\pmb{0}_n &amp;amp; = T \\left( n \\pmb{u} \\right) + T \\left( -n \\pmb{u} \\right) \\\\ &amp;amp; = n T \\left( \\pmb{u} \\right) + T \\left( -n \\pmb{u} \\right) &amp;amp; \\left[ \\because n \\in \\mathbb{Z}^+ \\right] \\\\\\implies T \\left( p \\pmb{u} \\right) &amp;amp; = T \\left( - n \\pmb{u} \\right) \\\\ &amp;amp; = \\pmb{0}_n - n T \\left( \\pmb{u} \\right) \\\\ &amp;amp; = - n T \\left( \\pmb{u} \\right) \\\\ &amp;amp; = p T \\left( \\pmb{u} \\right) &amp;amp; \\square\\end{align}\\]Combining the different scenarios above, we have indeed found,\\[\\begin{align}\\forall \\: c \\in \\mathbb{Z} : \\left( 1 \\right) \\implies \\left( 2 \\right) &amp;amp;&amp;amp; \\left( A \\right)\\end{align}\\]For \\(c \\in \\left\\{ \\frac{1}{q} : q \\in \\mathbb{Z} \\backslash \\left\\{ 0 \\right\\} \\right\\}\\)We will now see how the above result is also true for \\(c = \\frac{p}{q} : p = 1, q \\in \\mathbb{Z} \\backslash \\left\\{ 0 \\right\\}\\). Ultimately, we will combine this result with the corresponding one for \\(c \\in \\mathbb{Z}\\) to generalize it for all \\(c \\in \\mathbb{Q}\\) which in turn will let us generalize it to \\(c \\in \\mathbb{R}\\) via Dedekind cuts as previously stated.We begin with the statement,\\[\\begin{align}T \\left( \\sum_{a=1}^q \\frac{1}{q} \\pmb{u} \\right) &amp;amp; = T \\left( \\pmb{u} \\right) \\\\\\sum_{a=1}^q T \\left( \\frac{1}{q} \\pmb{u} \\right) &amp;amp; = T \\left( \\pmb{u} \\right) \\\\q T \\left( \\frac{1}{q} \\pmb{u} \\right) &amp;amp; = T \\left( u \\right) \\\\T \\left( \\frac{1}{q} \\pmb{u} \\right) &amp;amp; = \\frac{1}{q} T \\left( u \\right) &amp;amp; \\square\\end{align}\\]Thus,\\[\\begin{align}\\forall \\: c = \\frac{1}{q} : q \\in \\mathbb{Z} \\backslash \\left\\{ 0 \\right\\} : \\left( 1 \\right) \\implies \\left( 2 \\right) &amp;amp;&amp;amp; \\left( B \\right)\\end{align}\\]For \\(c \\in \\mathbb{Q}\\)Let \\(c = \\frac{p}{q} : p \\in \\mathbb{Z}, q \\in \\mathbb{Z} \\backslash \\left\\{ 0 \\right\\}\\). Using the previous results \\(\\left( A \\right)\\) and \\(\\left( B \\right)\\),\\[\\begin{align}T \\left( c \\pmb{u} \\right) &amp;amp; = T \\left( \\frac{p}{q} \\pmb{u} \\right) \\\\ &amp;amp; = T \\left( p \\cdot \\frac{1}{q} \\pmb{u} \\right) \\\\ &amp;amp; = p T \\left( \\frac{1}{q} \\pmb{u} \\right) &amp;amp; \\left[ \\left( A \\right) \\right] \\\\ &amp;amp; = \\frac{p}{q} T \\left( \\pmb{u} \\right) &amp;amp; \\left[ \\left( B \\right) \\right] \\\\ &amp;amp; = c T \\left( \\pmb{u} \\right) &amp;amp; \\square\\end{align}\\]\\[\\begin{align}\\forall \\: c \\in \\mathbb{Q} : \\left( 1 \\right) \\implies \\left( 2 \\right) &amp;amp;&amp;amp; \\left( C \\right)\\end{align}\\]For \\(c \\in \\mathbb{R}\\)Consider an arbitrary \\(c \\in \\mathbb{R}\\). From the section on reals as infinite series of rationals, there exists a sequence of rationals \\(\\left\\{ b_k : k \\in \\mathbb{N} \\right\\} : b_k \\in \\mathbb{Q} \\: \\forall \\: k \\in \\mathbb{N}\\) such that it adds up to \\(c\\),\\[\\displaystyle{ c = \\sum_{k=1}^\\infty b_k }\\]Hence,\\[T \\left( c \\pmb{u} \\right) = T \\left( \\sum_{k=1}^\\infty b_k \\pmb{u} \\right)\\]By additivity,\\[\\begin{align}T \\left( c \\pmb{u} \\right) &amp;amp; = \\sum_{k=1}^\\infty T \\left( b_k \\pmb{u} \\right) \\\\ &amp;amp; = \\sum_{k=1}^\\infty b_k T \\left( \\pmb{u} \\right) &amp;amp; \\left[ \\left( C \\right) \\right] \\\\ &amp;amp; = c T \\left( \\pmb{u} \\right) &amp;amp; \\blacksquare\\end{align}\\]\\[\\forall \\: c \\in \\mathbb{R} : \\left( 1 \\right) \\implies \\left( 2 \\right)\\]SummaryTo summarize the above approach, homogeneity of linear operators on vector spaces built on the base field \\(\\mathbb{R}\\), comes for free from their additivity. We realized this by using the construction of \\(\\mathbb{R}\\) from \\(\\mathbb{Q}\\) which is in turn constructed from \\(\\mathbb{Z}\\).The advantage of expressing reals in terms of integers is that integers can fundamentally be used for counting, which is implicitly applied in property \\(\\left( 1 \\right)\\), additivity. Thus, this bridge from \\(\\left( 1 \\right)\\) to \\(\\left( 2 \\right)\\) allows us to logically proceed in the same direction." }, { "title": "A Brief Geometric Analysis of Harmonic Oscillators: Part 2 (Tensor Algebra)", "url": "/tempus-spatium/geometric-analysis-harmonic-oscillators-part-2/", "categories": "classical mechanics", "tags": "harmonic oscillators, phase space, tensors", "date": "2023-01-01 00:00:00 -0500", "snippet": "In A Brief Geometric Analysis of Harmonic Oscillators, we examined how a convenient choice of phase space for the motion of a harmonic oscillator reveals its periodicity, without explicitly solving the equation of motion (Hooke’s law). Here, we will apply the same intuition but in the language of tensor algebra, introducing greater rigour as well as economy of notation.Prior knowledge is powerConsider the position \\(x \\left( t \\right)\\) of a point-particle obeying Hooke’s law. This obeys the equation of motion,\\[\\ddot{x} + \\frac{k}{m} x = 0\\]where \\(k\\) is the spring constant of the oscillator and \\(m\\) is its mass. For reasons which will soon become clear, we can make the above equation look more symmetric by defining a parameter \\(\\omega = \\sqrt{\\frac{k}{m}}\\) so that we have,\\[\\omega^{-1} \\ddot{x} + \\omega x = 0\\]Now, we invent a new variable (and consequently, and equation of motion) \\(y = \\omega^{-1} \\dot{x}\\). We imagine this to be a new variable which together with position, completely characterizes the instantaneous state of the oscillator. Thus, the state space is a two-dimensional phase space which consists of all \\(x\\) and \\(y\\) coordinates, and is geometrically the linear span of the basis vectors along \\(x\\) and \\(y\\),\\[S = \\text{span} \\left( \\frac{\\partial}{\\partial x}, \\omega \\frac{\\partial}{\\partial \\dot{x}} \\right)\\]Furthermore, this phase space is a Euclidean space i.e. equipped with a Euclidean notion of distance,\\[d \\left( u^i, u^j \\right) = \\sqrt{ \\left( u^i - u^j \\right) \\delta_{i j} \\left( u^j - u^j \\right) }\\]By the polarization identity for real inner products and the property that Euclidean spaces are closed under vector subtraction, we obtain the inner product for any two vectors in our phase space,\\[g \\left( u^i, v^j \\right) = u^i \\delta_{i j} v^j\\]Armed with this knowledge, let us plug in the new variable \\(y\\) into Hooke’s law to find,\\[x = - \\omega^{-1} \\dot{y}\\]Let’s take a look at the two equations of motion we conjured together,\\[\\begin{aligned}y &amp;amp; = \\omega^{-1} \\dot{x} \\\\x &amp;amp; = - \\omega^{-1} \\dot{y}\\end{aligned}\\]The above are a pair of coupled ordinary differential equations! Geometrically, they’re telling us that the velocity vector \\(\\dot{\\pmb{u}} = \\dot{x} \\partial_x + \\dot{y} \\partial_y\\) is given in terms of the position vector \\(\\pmb{u}\\) (in phase space) as,\\[\\dot{\\pmb{u}} = \\omega \\left( \\partial_x \\otimes \\text{d} y - \\partial_y \\otimes \\text{d} x \\right) \\left( x \\partial_x + y \\partial_y \\right)\\]Let us name the linear map acting on the position vector in the above equation as \\(\\pmb{\\omega}\\) 1. Then, we have, succinctly, \\(\\dot{\\pmb{u}} = \\pmb{\\omega} \\left( \\pmb{u} \\right)\\). In the component form, \\(\\dot{u}^i = \\omega^i_{\\phantom{i} j} u^j\\).To make the above equations look a little less opaque, consider their matrix representation (the reason we will not use it is that it is only a representation of the underlying tensorial objects),\\[\\begin{aligned}\\begin{pmatrix} \\dot{x} \\\\ \\dot{y} \\end{pmatrix} &amp;amp; = \\omega \\begin{pmatrix} 0 &amp;amp; 1 \\\\ -1 &amp;amp; 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\&amp;amp; = \\omega \\begin{pmatrix} y \\\\ - x \\end{pmatrix}\\end{aligned}\\]Now, the inner product of the position and velocity vectors turns out to be,\\[\\begin{aligned}g \\left( \\dot{u}^i, u^j \\right) &amp;amp; = \\dot{u}^i \\delta_{i j} u^j \\\\&amp;amp; = \\omega^i_{\\phantom{i} k} u^k \\delta_{i j} u^j \\\\&amp;amp; = u^k \\omega_{j k} u^j\\end{aligned}\\]Notice that \\(\\pmb{\\omega}\\) is antisymmetric i.e. \\(\\omega_{i j} = - \\omega_{j i}\\). Thus,\\[\\begin{align}g \\left( \\dot{u}^i, u^j \\right) &amp;amp; = u^j \\omega_{i j} u^i \\\\&amp;amp; = - u^j \\omega_{j i} u^i\\end{align}\\]But by the commutativity of scalar multiplication applied to the terms in a tensorial expression, we have \\(u^j \\omega_{i j} u^i = u^i \\omega_{i j} u^j\\). Furthermore, by the property that a tensorial expression remains invariant on interchanging dummy indices, we get from the two above results,\\[u^i \\omega_{i j} u^j = - u^i \\omega_{i j} u^j\\]which is only true if \\(u^i \\omega_{i j} u^j = 0\\). Thus, \\(g \\left( \\dot{u}^i, u^j \\right) = 0\\). In other words, the velocity vector is orthogonal to the position vector in the phase space we chose 2.Interpreting the above resultWe intuitively know that circular motion is the only motion where the velocity vector is orthogonal to the position vector all along. Thus, the velocity vector field of the harmonic oscillator resembles concentric circles (as seen here).Mathematics begins where intuition ends, so let us test the above notion precisely. We will do so by switching to polar coordinates \\(\\left( r, \\theta \\right)\\) in the phase space defined by,\\[\\begin{align}r^2 &amp;amp; = g \\left( \\pmb{u}, \\pmb{u} \\right) \\\\\\pmb{u} &amp;amp; = r \\left( \\cos \\left( \\theta \\right) \\partial_x + \\sin \\left( \\theta \\right) \\partial_y \\right)\\end{align}\\]Thus, we have,\\[\\begin{align}\\frac{d}{dt} r^2 &amp;amp; = \\frac{d}{dt} g \\left( \\pmb{u}, \\pmb{u} \\right) \\\\2 r \\dot{r} &amp;amp; = 2 g \\left( \\dot{\\pmb{u}}, u \\right) \\\\2 r \\dot{r} &amp;amp; = 0 \\\\\\implies \\dot{r} &amp;amp; = 0 &amp;amp; [r \\neq 0]\\end{align}\\]This tells us that for a given trajectory in phase space, \\(r\\) is a constant. Consequently, every \\(r\\) characterizes a unique trajectory. This symmetry essentially eliminates one equation of motion from the two we constructed, the other one being,\\[\\begin{align}\\dot{\\pmb{u}} &amp;amp; = \\frac{d}{dt} \\left[ r \\left( \\cos \\left( \\theta \\right) \\partial_x + \\sin \\left( \\theta \\right) \\partial_y \\right) \\right] \\\\\\pmb{\\omega} \\pmb{u} &amp;amp; = r \\frac{d}{dt} \\left( \\cos \\left( \\theta \\right) \\partial_x + \\sin \\left( \\theta \\right) \\partial_y \\right) \\\\ &amp;amp; = r \\begin{pmatrix} \\dot{\\theta} \\left( - \\sin \\left( \\theta \\right) \\partial_x + \\cos \\left( \\theta \\right) \\partial_y \\right) \\\\ + \\cos \\left( \\theta \\right) \\dot{\\partial}_x + \\sin \\left( \\theta \\right) \\dot{\\partial}_y \\end{pmatrix}\\end{align}\\]At this stage, we have time derivatives of basis vectors in phase space. Let us show that they are the zero vector. Notice that had we tracked just the components in the matrix representation, we would be tempted to differentiate only the components, making the implicit assumption that the basis vectors are constant. This is yet another demonstration of the importance of tensor calculus, which keeps track of complete invariant objects, thereby never creating (or destroying) information.\\[\\begin{align}\\dot{\\partial}_i &amp;amp; = \\frac{d}{dt} \\partial_i \\\\ &amp;amp; = \\dot{x}^j \\frac{d}{dx^j} \\partial_i \\\\ &amp;amp; = \\omega^j_{\\phantom{j} k} x^k \\frac{d}{dx^j} \\partial_i \\\\ &amp;amp; = \\omega^j_{\\phantom{j} k} x^k \\Gamma^l_{\\phantom{l} j i} \\partial_l\\end{align}\\]Since we are dealing with a Euclidean space, we can choose coordinates where the connection coefficients vanish everywhere. These are nothing but Riemann normal coordinates, where the metric resembles a Kronecker delta throughout in flat space. Since the phase space in consideration is Euclidean and equipped with such a metric, we can simply cancel out the connection coefficients. Using this result in the main derivation,\\[\\begin{align}\\pmb{\\omega} \\pmb{u} &amp;amp; = r \\dot{\\theta} \\left( - \\sin \\left( \\theta \\right) \\partial_x + \\cos \\left( \\theta \\right) \\partial_y \\right) \\\\ &amp;amp; = \\dot{\\theta} \\omega^{-1} \\pmb{\\omega} \\left[ r \\left( \\cos \\left( \\theta \\right) \\partial_x + \\sin \\left( \\theta \\right) \\partial_y \\right) \\right] \\\\\\pmb{\\omega} \\pmb{u} &amp;amp; = \\dot{\\theta} \\omega^{-1} \\pmb{\\omega} \\pmb{u} \\\\\\implies \\dot{\\theta} &amp;amp; = \\omega\\end{align}\\]On integrating the above equation of motion, we get the solution \\(\\theta \\left( t-t_0 \\right) = \\theta \\left( t_0 \\right) + \\omega \\left( t - t_0 \\right)\\). Since the physical significance of \\(\\theta\\) is in its manifestation in the expression for \\(x\\) through trigonometric functions, we can identify all \\(\\theta\\)’s separated by integral periods, \\(\\displaystyle{ \\theta \\sim \\theta + \\frac{2 \\pi n}{\\omega} }\\). In other words, \\(x \\left( t \\right)\\) returns to the same state after integral multiples of the time period \\(\\displaystyle{ T = \\frac{2 \\pi}{\\omega} }\\).To summarize, we showed that a pointlike harmonic oscillator (a system obeying Hooke’s law) has a periodic evolution, simply by studying the symmetries of the equation of motion involved. We didn’t need to solve the equation explicitly to predict the same, which is a win.However, one may find all the algebraic manipulation we performed unnecessarily complicated. So, let us take a step back and try to think why such a route can be important.Why algebra?Before we ask ‘why’, we must ask ‘what’. What is algebra? Well, the term is used in many senses, but it usually refers to a structure where new objects can be created from given ones, using some kind of composition.In the section earlier, we used quite a bit of algebra, including tensor algebra (the manipulation of tensors). We deduced facts about harmonic oscillators — such as the orthogonality of their position and velocity vectors in phase space — by algebraically amalgamating mathematical objects such as \\(\\pmb{u}\\), \\(\\dot{\\pmb{u}}\\), \\(\\pmb{\\omega}\\), \\(\\pmb{g}\\), etc. This enabled us to conclude that a harmonic oscillator evolves periodically, without completely solving its equation of motion (rather, we broke it down into two, eliminated one by symmetry and solved the remaining first-order ordinary differential equation).This algebraic approach gets especially important in quantum mechanics and field theory, where much of physics can be gleaned from algebra. There is no clear reason why — it simply seems to be the way the universe works. But perhaps, there is more going on here. Perhaps, algebra is so fundamental to physics because like physics, it is about conjuring richer objects from simpler objects. Moreover, algebra being so abstract seems to correspond to physics working well with abstractions, i.e. the key details of systems under concerned situations.However, to better understand this deep nature of physics, we must work out way through the mathematics. I hope to write future posts exploring the relationship between the theme of this post, and classical and quantum field theory. For instance, from the methods discussed above, one may begin to suspect that the fact that a Klein-Gordon field is a system of infinite harmonic oscillators may be algebraically demonstrated from the formal analogy between the Klein-Gordon equation and Hooke’s law.That said, it’s time to wrap up our first post of the year! Thanks for reading and thanks for wanting to know more today than you did yesterday. As said in Matthew 7:7, Ask, and it shall be given you; seek, and ye shall find; knock, and it shall be opened unto you. Interestingly, raising the first index of \\(\\omega^i_{\\phantom{i} j}\\) gives the components of a symplectic form \\(\\omega_{i j}\\). I do not know if the choice of labelling is a coincidence but it is a striking one! &amp;#8617; Had we chosen an arbitrary phase space represented by \\(\\omega^i_{\\phantom{i} j}\\), we would need to choose a metric \\(g_{i j}\\) such that \\(g_{i j} \\omega^i_{\\phantom{i} k} = - g_{i k} \\omega^i_{\\phantom{i} j}\\) so that the above derivation would employ an antisymmetric tensor just as above. However, we would need to be careful that the geometry of the phase space is still Euclidean, which would require a rigorous usage of Riemannian geometry. &amp;#8617; " }, { "title": "Deriving the Lagrangian Density for Newtonian Gravitation", "url": "/tempus-spatium/deriving-lagrangian-newtonian-gravitation/", "categories": "classical field theory", "tags": "gravitation, Klein-Gordon theory, energy-momentum tensor", "date": "2022-12-31 00:00:00 -0500", "snippet": "Some thoughtsHi everyone! It’s been a few months since the last post here. A lot (of good) has happened since then. Before coming to that, I would like to thank all my old readers for sticking by, and new readers for visiting this blog :) I hope you all are doing great and had a wonderful year … Happy New Year!In this period of absence, I’ve been working on university life in a beautiful, new country. It’s been a transforming, memorable and educational experience and I’m excited about the journey up ahead!It’s taken a while balancing time between academics and personal life. This has given me the chance to mingle with and learn from very amiable and knowledgeable individuals. Such experiences have motivated me to keep exploring mathematical physics, and plan a layout for future blog posts.In the upcoming year, I plan to expand the contents of this blog both in breadth and depth. For instance, I hope to employ deeper mathematical tools, intuition in physics, and dedicated posts on philosophy. But it is the time that shall judge this resolution — assuming it exists (half-kidding)!Let us begin this journey into the yet-to-be-known with a little problem in classical field theory. Before asking and answering the question, we will quickly highlight some aspects of Newtonian gravitation. The main exploration begins in Constructing the Lagrangian.The problemGravitation is a field theoryThe theory of Newtonian gravitation is a field theory in the framework of Newtonian mechanics. This means that the Newtonian gravitational field obeys certain fundamental properties, namely, Principle of stationary action Galilean invariance Unfortunately, the Newtonian gravitational field does not obey locality and local Lorentz invariance, making it a non-relativistic classical field theory. These limitations are overcome by Einstein’s general theory of relativity, where the gravitational ‘field’ is the metric of spacetime itself. As this is a substantially different model, we will stick to the non-relativistic classical field theory for the moment.Since we assume that the gravitational field, henceforth written as the gravitational potential \\(\\phi\\), obeys the principle of stationary action, it must also obey the Euler-Lagrange equation, given by,\\[\\frac{\\partial \\mathcal{L}}{\\partial \\phi} - \\nabla_i \\pi^i = 0\\]where \\(\\mathcal{L}\\) is an appropriate Lagrangian density for the theory and \\(\\pi^i\\) is the conjugate momentum tensor, \\(\\displaystyle{ \\pi^i = \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_i \\phi \\right)} }\\).Furthermore, to incorporate manifest Galilean invariance, we will use tensor notation with spacelike indices, as above.Poisson’s equationFrom Gauss’s law for Newtonian gravitation, we have,\\[\\nabla_i g^i = - 4 \\pi G \\rho\\]where \\(g^i\\) is the acceleration due to gravity.The above is the differential form of Gauss’s law for Newtonian gravitation, which can be derived from the integral form using the divergence theorem.Since the gravitational force field is conservative, i.e. the change in gravitational potential energy along a trajectory only depends on its endpoints, the acceleration due to gravity can be written as the negative gradient of the gravitational potential energy,\\[g^i = - \\partial^i \\phi\\]Putting the two above equations together, we get what is known as Poisson’s equation for the gravitational field i.e. gravitational potential,\\[\\nabla_i \\partial^i \\phi - 4 \\pi G \\rho = 0\\]What is a suitable Lagrangian?We have discussed how Newtonian gravitation is a non-relativistic classical field theory with a Lagrangian description and with the field equation being that of Poisson’s equation. This means that the Lagrangian density (called Lagrangian for brevity) for Newtonian gravitation must yield Poisson’s equation as the equation of motion.This begs the question, which Lagrangian is suitable for the above requirements? And how can we construct it using reasonable assumptions?It turns out that an interesting way to answer the above questions is to apply a modified form of the Klein-Gordon theory to the field theory of gravity. Let us see how.Constructing the LagrangianRestricting the LagrangianAn unexpectedly useful way to restrict the form of the Lagrangian for a scalar field such as the gravitational potential, is to assert the symmetry of the field’s energy-momentum tensor. We did this in Scalar Field Lagrangian From Symmetry Considerations to find that the form of the Lagrangian must be severely restricted to that of the Klein-Gordon Lagrangian.Much of the reasoning and mathematical machinery remains the same in this case. The only difference is that here, we have Galilean invariance instead of Lorentz invariance. This translates to asserting the symmetry of the 3-dimensional stress-energy tensor instead of the full 4-dimensional energy-momentum tensor. Thus,\\[\\begin{align}T^{i j} &amp;amp; = \\pi^i \\partial^j \\phi - g^{i j} \\mathcal{L} \\\\T^{i j} &amp;amp; = T^{j i}\\end{align}\\]which is true in the most general case only when,\\[\\begin{align}\\pi^i \\partial^j \\phi &amp;amp; = \\pi^j \\partial^i \\phi \\\\\\pi^i &amp;amp; = \\partial^i \\phi \\\\\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_i \\phi \\right)} &amp;amp; = \\partial^i \\phi\\end{align}\\]which is further generally true when the Lagrangian is of the form,\\[\\mathcal{L} = \\frac{1}{2} \\partial_i \\phi \\partial^i \\phi - V \\left( \\phi \\right)\\]The above is essentially a Klein-Gordon Lagrangian in a potential, but with spacelike indices instead of spacetimelike indices.Poisson meets PoissonThe equation of motion obtained by varying the above Lagrangian with respect to the field, unsurprisingly, resembles the Klein-Gordon equation but with spacelike indices only,\\[\\begin{align}\\frac{\\partial \\mathcal{L}}{\\partial \\phi} - \\nabla_i \\pi^i &amp;amp; = 0 \\\\\\implies \\nabla_i \\partial^i \\phi + \\frac{\\partial V}{\\partial \\phi} &amp;amp; = 0\\end{align}\\]This equation has the form of — drum rolls please — the general Poisson’s equation relating the Laplacian \\(\\nabla_i \\partial^i\\) of a function to another function. It is, therefore, no coincidence that the field equation for the gravitational potential is called Poisson’s equation — it is a special case of the former, general case (this nomenclature is a little tricky, to be fair).All we now have to do is figure out which scalar potential \\(V \\left( \\phi \\right)\\) makes Poisson’s general equation reduce to Poisson’s equation for Newtonian gravitation. Then, we can find the suitable Lagrangian for the theory by plugging this potential into the modified Klein-Gordon Lagrangian we deduced.The scalar potentialMapping the two forms of Poisson’s equation, one from the world of mathematics and the other from that of physics (in a certain sense), we find,\\[\\frac{\\partial V}{\\partial \\phi} = - 4 \\pi G \\rho\\]Obviously, \\(V \\left( \\phi \\right)\\) is of the form below, up to linearity,\\[V \\left( \\phi \\right) = - 4 \\pi G \\rho \\phi\\]The LagrangianPlugging in the above into the form of the Lagrangian we argued for the scalar gravitational potential, we get,\\[\\mathcal{L} = \\frac{1}{2} \\partial_i \\phi \\partial^i \\phi + 4 \\pi G \\rho \\phi\\]Notice that scaling the Lagrangian does not change the equation of motion obtained (which engenders the physics) and scaling by a factor of \\(\\displaystyle{\\frac{1}{4 \\pi G}}\\) yields a Lagrangian seen frequently in the physics literature,\\[\\mathcal{L} = \\frac{1}{8 \\pi G} \\partial_i \\phi \\partial^i \\phi + \\rho \\phi\\]Conclusion (Second Law of Epistemological Thermodynamics)Just like the Second Law of Thermodynamics states that the entropy of the universe can only increase with time, we propose the Second Law of Epistemological Thermodynamics: The number of questions regarding the universe can only increase with time.An immediate result of the above law is that we are not yet done when it comes to deriving a Lagrangian for Newtonian gravitation.The Lagrangian surely yields the correct equation of motion (Poisson’s equation) when varied with respect to the gravitational potential \\(\\phi\\). However, notice what happens when we vary it with with respect to the mass density field \\(\\rho\\),\\[\\phi = 0\\]Oops! We now have two possible paths ahead of us: Promote mass density \\(\\rho\\) to a non-relativistic classical field and modify the Lagrangian to yield Poisson’s equation when varied with respect to both the fields \\(\\phi\\) and \\(\\rho\\). Justify if \\(\\rho\\) is not a classical field and elaborate on its role in Poisson’s equation and the associated Lagrangian, rigorously. We will take one of these paths in a new post, soon. For now, which path will be adopted in the post will be akin to a stochastic process as I am not very sure of the outcome myself. However, I’m studing the topic in my free time and am beginning to see interesting connections between both the possibilities. In fact, they may be simultaneously workable, if we generalize classical fields to include distributions! And this seems to yield Newtonian particle dynamics for free solely from field-theoretic constructs.I’m excited to write posts on the above topics soon. Until then, Bonne Année once more and take care! Thanks for reading and hope you enjoyed :)" }, { "title": "Scalar Field Lagrangian From Symmetry Considerations: Part 2 (Gauge Invariance)", "url": "/tempus-spatium/scalar-field-lagrangian-symmetry-considerations-part-2/", "categories": "classical field theory", "tags": "Klein-Gordon theory, gauge invariance, symmetry", "date": "2022-08-11 00:00:00 -0400", "snippet": "Related concepts from older postsIn Scalar Field Lagrangian From Symmetry Considerations, we derived the Lagrangian for the Klein-Gordon theory of a scalar field \\(\\phi\\) evolving in a potential \\(V\\),\\[\\mathcal{L} = \\frac{1}{2} \\partial_\\mu \\phi \\partial^\\mu \\phi - V \\left( \\phi \\right)\\]We did so, by deriving the field-theoretic energy-momentum tensor using Noether’s theorem applied to the symmetry of the action under small translations in spacetime and borrowing its symmetry from continuum mechanics:\\[\\begin{align}T^{\\mu \\nu} &amp;amp; = \\pi^\\mu \\partial^\\nu \\phi - \\eta^{\\mu \\nu} \\mathcal{L} \\\\T^{\\mu \\nu} &amp;amp; = T^{\\nu \\mu}\\end{align}\\]where \\(\\displaystyle{ \\pi^\\mu = \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} }\\) is the conjugate momentum tensor. The above equations suffice to lead to the aforementioned Lagrangian.Additionally, in Harmonic Oscillators in Scalar Field Theory, we constructed the perturbation theory for Klein-Gordon fields oscillating about the local minima of the potential \\(V \\left( \\phi \\right)\\). We found that only differences in the field matter, and since these are small perturbations, their squares and higher powers vanish. As the equations of motion are obtained by differentiating the Lagrangian, it follows that we must have \\(\\phi^2\\) appear as a potential term, and it does so in the form,\\[\\mathcal{L} = \\frac{1}{2} \\partial_\\mu \\phi \\partial^\\mu \\phi - \\frac{1}{2} m^2 \\phi^2\\]where \\(m\\) is a ‘mass’ of the field, of sorts (this becomes clear only after quantizing the field, which is a subject of quantum field theory).Last but not the least, in the following posts, we discussed the idea of gauge invariance and indexed fields behaving much like coordinates in classical mechanics:Combining Valid Solutions Into New Ones in Classical Field TheoryGauge Invariance in Classical Field TheoryIn this post, we will incorporate gauge invariance into the Klein-Gordon theory and see how it provides deep insights about the field theory (such as the structure of its perturbation theory) for free!LagrangianConsider an indexed family of independent scalar fields, \\(\\left\\{ \\phi_a \\right\\}\\). Put briefly, gauge invariance is the idea that under differentiable transformations of the indexed fields, i.e. gauge transformations \\(\\phi_a \\to \\widetilde{\\phi}_a\\), the structure of the equations of motion for each field must remain preserved.This condition is enforced on the theory simply by replacing the usual geometric covariant derivatives \\(\\nabla_\\mu \\phi_a\\), with gauge covariant derivatives \\(D_\\mu \\phi_a = \\nabla_\\mu \\phi_a - \\phi_b G^b_{\\phantom{b} \\mu a}\\). Here, \\(G^b_{\\phantom{b} \\mu a}\\) are the gauge connection coefficients \\(G^b_{\\phantom{b} \\mu a} = J^b_{\\phantom{b} c} \\partial_\\mu J^c_{\\phantom{c} a}\\) and \\(J^b_{\\phantom{b} a}\\) is the gauge Jacobian \\(J^b_{\\phantom{b} a} = \\frac{\\partial \\widetilde{\\phi}_a}{\\partial \\phi_b}\\).Now, recall the Klein-Gordon Lagrangian,\\[\\mathcal{L} = \\frac{1}{2} \\partial_\\mu \\phi \\partial^\\mu \\phi - V \\left( \\phi \\right)\\]The prescription for gauge invariance is to replace \\(\\phi\\) with \\(\\phi_a\\) and \\(\\partial_\\mu \\phi\\) with \\(D_\\mu \\phi_a\\). Since the quantity on the left hand side has no free indices, all indices on the right hand side must be dummy indices. This severe restriction, with the fact that our new Lagrangian must reduce to the original one in suitable gauges (where \\(\\partial_\\mu J^b_{\\phantom{b} a} = 0\\), as we had seen here), motivates us to propose the new Lagrangian,\\[\\mathcal{L} = \\frac{1}{2} D_\\mu \\phi_a D^\\mu \\phi^a - V \\left( \\phi_a \\right)\\]However, the above is still not quite right; we have a gauge scalar potential \\(V\\) which depends on \\(\\phi_a\\) (which, as far as gauge transformations are concerned, does not look like a scalar i.e. it does not resemble a gauge scalar). The simplest gauge scalar we can construct from \\(\\phi_a\\), which depends on no other field, is \\(\\phi_a \\phi^a\\). Therefore, we write,\\[\\mathcal{L} = \\frac{1}{2} D_\\mu \\phi_a D^\\mu \\phi^a - V \\left( \\phi_a \\phi^a \\right)\\]Some insightsLet us now apply the above ideas to gain new insight on classical field theory.Perturbation theorySimilar to our previous exploration of the perturbation theory for the classical Klein-Gordon model, we ignore contractions of \\(\\phi_a\\) and its powers in the equations of motion. Since the equations of motion are obtained by differentiating the Lagrangian in various terms, it can contain only the contraction of \\(\\phi_a\\) up to its first power, i.e. \\(\\phi_a \\phi^a\\). Such a Lagrangian can be written in the form,\\[\\mathcal{L} = \\frac{1}{2} D_\\mu \\phi_a D^\\mu \\phi^a - \\frac{1}{2} m^2 \\phi_a \\phi^a\\]Single index theoryTo realize the power of the above argument, consider a single indexed field \\(\\phi\\) (we didn’t have to index it explicitly as there is only one!). Now, our argument becomes the following. We cannot have \\(\\phi\\) appear in the Lagrangian, since it is not a gauge scalar (much like \\(\\phi_a\\) is not, for a theory with more gauge indices). As a matter of fact, no odd power of \\(\\phi\\) is a gauge scalar, since an odd power corresponds to a free gauge index,\\[\\phi^{2n+1} = \\phi^{2n} \\cdot \\phi \\leftrightarrow \\left( \\phi_b \\phi^b \\right)^n \\phi_a\\]Therefore, only even powers of \\(\\phi\\) can appear in the Lagrangian, which is a gauge scalar. This internal working of the field \\(\\phi\\) would not come to limelight without the idea of gauge invariance and multiples degrees of freedom - thus providing an excellent theoretical justification for gauge field theories.Armed with this knowledge and the Klein-Gordon Lagrangian from perturbation theory, we assert that a fully gauge-invariant Lagrangian for a single field \\(\\phi\\) has the series representation,\\[\\mathcal{L} = \\frac{1}{2} D_\\mu \\phi D^\\mu \\phi - \\frac{1}{2} m^2 \\phi^2 - \\sum_{n=2}^{\\infty} \\frac{1}{n!} g_n \\phi^{2 n}\\]where \\(\\left\\{ g_n \\vert n = 1, 2, \\dots \\right\\}\\) are a set of coupling constants, with the canonical condition \\(\\displaystyle{ g_1 = \\frac{1}{2} m^2 }\\). Clearly, we obtain the Klein-Gordon perturbation theory from the above Lagrangian if we consider small oscillations of \\(\\phi\\) about stable equilibria (so that \\(\\phi^3\\) and higher odd powers vanish from the equations of motion, effectively removing \\(- \\frac{1}{4!} g_4 \\phi^4\\) and higher even terms from the Lagrangian).Complex scalar field theoryAt last, let us venture towards the scalar field theory of complex-valued fields. In this model, every field \\(\\phi\\) has a corresponding dual \\(\\bar{\\phi}\\) which maps it to a real, \\(\\bar{\\phi} \\phi \\in \\mathbb{R}\\). Here, juxtaposition of complex numbers represents complex multiplication, dictated by a bilinear map corresponding to \\(\\text{SO} \\left( 2 \\right)\\). For readers who are interested, Algebra Done Tensorially: Part 2 (Algebras Over Fields) explores complex algebra from a tensor algebraic perspective.In the notation we have used so far, \\(\\phi^a\\) represents the components of \\(\\phi\\) while \\(\\phi_a\\) represents those of \\(\\bar{\\phi}\\). In particular,\\[\\begin{align}\\phi &amp;amp; = \\phi^a e_a \\\\\\bar{\\phi} &amp;amp; = \\phi_a \\theta^a \\\\\\theta^a \\left( e_b \\right) &amp;amp; = e_b \\left( \\theta^a \\right) = \\delta^a_{\\phantom{a} b}\\end{align}\\]The intuition for the last equation has been elaborated in Demystifying the Definition Of a Covector Basis. Note that in this general picture, it is not necessarily true that \\(\\phi_a = \\delta_{a b} \\phi^b\\). In general, \\(\\phi_a = g_{a b} \\phi^b\\) where \\(g_{a b} \\theta^a \\otimes \\theta^b\\) is the metric tensor \\(g\\).Thus, from the above discussion, the full Lagrangian for complex scalar fields (with gauge scalar potential terms) is,\\[\\mathcal{L} = \\frac{1}{2} D_\\mu \\bar{\\phi} D^\\mu \\phi - \\frac{1}{2} m^2 \\bar{\\phi} \\phi - \\sum_{n=2}^\\infty \\frac{1}{n!} g_n \\left( \\bar{\\phi} \\phi \\right)^n\\]ConclusionThe above Lagrangian is where much of relativistic quantum mechanics and quantum field theory begin. Through a series of posts, we have managed to construct it using purely classical methods. In doing so, we discovered gauge field theories, which are yet another deep chapter of modern quantum mechanics and quantum field theory. Hopefully, these subjects will form a major part of the upcoming posts on this blog.Thank you for reading till here. Feel free to leave a comment or reaction below :)" }, { "title": "Symplectic Forms from Poisson Brackets", "url": "/tempus-spatium/symplectic-forms-poisson-brackets/", "categories": "classical mechanics", "tags": "symplectic forms, Poisson brackets, phase space", "date": "2022-07-08 00:00:00 -0400", "snippet": "Much of analytical mechanics dedicates itself to studying the trajectories of dynamical systems. Newtonian mechanics describes trajectories in physical space. On the other hand, Lagrangian mechanics deals with trajectories in configuration space, which is the space of all generalized position and velocity coordinates; while Hamiltonian mechanics studies trajectories in phase space, which comprises all generalized position and momentum coordinates.A deep mathematical analysis of the Hamiltonian setup reveals a startling geometric structure, namely the geometry of symplectic manifolds. Briefly, it is found that phase space has the characteristics of a differentiable manifold equipped with a closed, nondegenerate differential 2-form. This is a special case of symplectic manifolds, which are similarly equipped with what are known as symplectic forms. The study of symplectic manifolds and their geometry forms the discipline of symplectic geometry.To expose the working of symplectic geometry, one typically begins with Hamiltonian mechanics and fleshes it out with geometric notions such as manifolds, flow, differential forms, sections and so on. This is an extremely enriching exercise, and one that deserves a more detailed exposition than can be incorporated into a single post.The purpose of this post, is to subtly indicate the existence of symplectic forms, using an approach that resembles that of tensor calculus. The machinery of tensors can be imagined to ‘sit on top’ of differential geometry (which in turn relies on topology). We will not build the machinery of tensors yet, but will handwave with its power to merely steal a glimpse at the world of symplectic forms and manifolds.This approach will admittedly have its limitations. It will ignore some questions for the moment and take for granted the idea of phase space behaving like a differentiable manifold in order to allow the usage of tensor-like language in the first place. However, by keeping at hold the deep questions for the future, the hope is to simply begin sketching out symplectic forms in Hamiltonian mechanics, which would be somewhat buried at first, had we covered our ideas with the intricate tapestry of differential geometry.Poisson bracketsInformal recapConsider two observables in phase space, \\(A \\left( q^i, p^j, t \\right)\\) and \\(B \\left( q^i, p^j, t \\right)\\). Here, \\(q^i\\) are the components of the generalized position \\(\\pmb{q}\\), of the concerned system. On the other hand, \\(p^i\\) are the components of conjugate momentum,\\[p^i = \\frac{\\partial L}{\\partial \\dot{q}_i}\\]where \\(q_i = \\delta_{ij} q^j\\). Note that we have and will continue to use the Einstein summation convention, where dummy indices are implied to be summed over.The Poisson bracket is a bilinear, anticommutative operation 1 which acts on two dynamical variables in the manner,\\[\\left\\{ A, B \\right\\} = \\frac{\\partial A}{\\partial q^i} \\frac{\\partial B}{\\partial p_i} - \\frac{\\partial A}{\\partial p^i} \\frac{\\partial B}{\\partial q_i}\\]It is seen that Hamilton’s equations of motion now read,\\[\\begin{align}\\dot{q}^i &amp;amp; = \\left\\{ q^i, H \\right\\} \\\\\\dot{p}^i &amp;amp; = \\left\\{ p^i, H \\right\\}\\end{align}\\]In fact, for any variable \\(A\\),\\[\\begin{align}\\dot{A} &amp;amp; = \\frac{dA}{dt} \\\\ &amp;amp; = \\frac{\\partial A}{\\partial q^i} \\dot{q}^i + \\frac{\\partial A}{\\partial p^i} \\dot{p}^i + \\frac{\\partial A}{\\partial t} \\\\ &amp;amp; = \\frac{\\partial A}{\\partial q^i} \\frac{\\partial H}{\\partial p_i} - \\frac{\\partial A}{\\partial p^i} \\frac{\\partial H}{\\partial q_i} + \\frac{\\partial A}{\\partial t} \\\\ &amp;amp; = \\left\\{ A, H \\right\\} + \\frac{\\partial A}{\\partial t}\\end{align}\\]As bilinear maps on partial derivative operatorsIsolating the Poisson bracket of its arguments, we see that it is a map acting on \\(\\displaystyle{ \\frac{\\partial}{\\partial q^i} }\\) and \\(\\displaystyle{ \\frac{\\partial}{\\partial p^j} }\\). Let us, therefore, break down the Poisson bracket of two variables as,\\[\\left\\{ A, B \\right\\} = \\omega \\left( \\frac{\\partial}{\\partial q^i}, \\frac{\\partial}{\\partial p^j} \\right) \\left( A, B \\right)\\]The reason the above is bilinear with respect to the derivative operators, is,\\[\\begin{align}\\omega \\left( \\sum_\\alpha \\frac{\\partial}{\\partial q_\\alpha^i}, \\sum_\\beta \\frac{\\partial}{\\partial p_\\beta^j} \\right) \\left( A, B \\right) &amp;amp; = \\left( \\sum_\\alpha \\frac{\\partial A}{\\partial q_\\alpha^i} \\right) \\left( \\sum_\\beta \\frac{\\partial B}{\\partial p_{\\beta i}} \\right) - \\left( \\sum_\\beta \\frac{\\partial A}{\\partial p_\\beta^i} \\right) \\left( \\sum_\\alpha \\frac{\\partial B}{\\partial q_{\\alpha i}} \\right) \\\\ &amp;amp; = \\sum_\\alpha \\sum_\\beta \\left( \\frac{\\partial A}{\\partial q_\\alpha^i} \\frac{\\partial B}{\\partial p_{\\beta i}} - \\frac{\\partial A}{\\partial p_\\beta^i} \\frac{\\partial B}{\\partial q_{\\alpha i}} \\right) \\\\ &amp;amp; = \\sum_\\alpha \\sum_\\beta \\omega \\left( \\frac{\\partial}{\\partial q_\\alpha^i}, \\frac{\\partial}{\\partial p_\\beta^j} \\right) \\left( A, B \\right)\\end{align}\\]Here, \\(\\alpha\\) and \\(\\beta\\) are abstract indices used to represent different derivative operators.Symplectic formsThe map \\(\\omega\\) we have defined earlier, will eventually turn out to be nothing but a symplectic form! Let us proceed by deriving its properties and nature, using the ideas laid out to far.Some propertiesWe have already seen, from the definition of \\(\\omega\\), that it is bilinear. It follows from the same definition, that it is anticommutative:\\[\\omega \\left( \\frac{\\partial}{\\partial q^i}, \\frac{\\partial}{\\partial q^j} \\right) = - \\omega \\left( \\frac{\\partial}{\\partial q^j}, \\frac{\\partial}{\\partial q^i} \\right)\\]Symplectic forms are also alternating, since,\\[\\begin{align}\\omega \\left( \\frac{\\partial}{\\partial q^i}, \\frac{\\partial}{\\partial q^j} \\right) \\left( A, B \\right) &amp;amp; = \\frac{\\partial A}{\\partial q^i} \\frac{\\partial B}{\\partial q_i} - \\frac{\\partial A}{\\partial q^i} \\frac{\\partial B}{\\partial q_i} \\\\ &amp;amp; = 0\\end{align}\\]Furthermore, it is easily seen that \\(\\omega\\) is non-degenerate, i.e. if it is \\(0\\) for all values of one argument, then the other argument must be \\(0\\).Let us now write \\(\\omega\\) explicitly as a differential 2-form, to further investigate its properties.Differential 2-form representationRecall the manner in which we have defined \\(\\omega\\),\\[\\omega \\left( \\frac{\\partial}{\\partial q^i}, \\frac{\\partial}{\\partial p^j} \\right) \\left( A, B \\right) = \\frac{\\partial A}{\\partial q^i} \\frac{\\partial B}{\\partial p_i} - \\frac{\\partial A}{\\partial p^i} \\frac{\\partial B}{\\partial q_i}\\]Let us factor out the exact arguments \\(\\displaystyle{ \\frac{\\partial}{\\partial q^i} }\\) and \\(\\displaystyle{ \\frac{\\partial}{\\partial p^j} }\\) in the right hand side of the above expression, as follows.\\[\\omega \\left( \\frac{\\partial}{\\partial q^i}, \\frac{\\partial}{\\partial p^j} \\right) \\left( A, B \\right) = \\frac{\\partial A}{\\partial q^i} \\delta^{i j} \\frac{\\partial B}{\\partial p^j} - \\frac{\\partial A}{\\partial p^i} \\delta^{i j }\\frac{\\partial B}{\\partial q^j}\\]In the component representation, the above may be written as,\\[\\omega \\left( \\frac{\\partial}{\\partial q^i}, \\frac{\\partial}{\\partial p^j} \\right) \\left( A, B \\right) = \\left[ \\begin{pmatrix} \\frac{\\partial}{\\partial q^k} &amp;amp; \\frac{\\partial}{\\partial p^l} \\end{pmatrix} \\begin{pmatrix} 0 &amp;amp; - \\delta_{k j} \\\\ \\delta_{li} &amp;amp; 0 \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial}{\\partial q^i} \\\\ \\frac{\\partial}{\\partial p^j} \\end{pmatrix} \\right] \\left( A, B \\right)\\]Here, each index represents a component in the space represented by \\(\\left( q_i, p_j \\right)\\). It follows that, in an arbitrary basis,\\[\\omega_{i j} = \\text{d} p_i \\otimes \\text{d} q_j - \\text{d} q_i \\otimes \\text{d} p_j\\]I.e.,\\[\\omega_{i j} = \\text{d} p_i \\wedge \\text{d} q_j\\]Some more propertiesNow that we know the precise form of \\(\\omega\\), let us show that it is closed, by computing its exterior derivative:\\[\\begin{align}\\text{d} \\omega &amp;amp; = \\text{d} \\left( \\text{d} p \\wedge \\text{d} q \\right) \\\\ &amp;amp; = \\text{d}^2 p \\wedge \\text{d} q - \\text{d} q \\wedge \\text{d}^2 p \\\\ &amp;amp; = 0 \\wedge \\text{d} q - \\text{d} q \\wedge 0 \\\\ &amp;amp; = 0\\end{align}\\]Here, \\(\\text{d} q\\) is the one-form whose components are \\(q_i\\), and likewise for \\(\\text{d} p\\). We have used the statement \\(d^2 = 0\\), which applies to any \\(k\\)-form. We thus found that the exterior derivative of \\(\\omega\\) is \\(0\\), i.e. it is closed. From a tensorial perspective, the exterior derivative of a \\(k\\)-form \\(A\\) is defined as the antisymmetrized \\(\\left( k+1 \\right)\\)-form,\\[\\left( \\text{d} A \\right)_{i_1 \\dots i_{k+1}} = \\left( k+1 \\right) \\partial_{[ i_1} A_{i_2 \\dots i_{k+1} ]}\\]To understand how the above works on a symplectic manifold, we will have to explicitly use tensors, by treating position and momentum as equivalent kinds of coordinates. I will likely cover the same in a future post.ConclusionTo summarize, we have found how symplectic forms blend into Hamiltonian mechanics, via the structure of Poisson brackets. Subsequently, we explored the important properties of symplectic forms and also their form as maps, in both coordinate-dependent and coordinate-independent representation.For further reading, visit the following: Poisson bracket Symplectic geometry Symplectic manifold Symplectic form Exterior derivative Special Relativity and Flat Spacetime And additionally, one that obeys the Leibniz law and Jacobi identity. &amp;#8617; " }, { "title": "Linearity of Classical Gauge Fields", "url": "/tempus-spatium/linearity-classical-gauge-fields/", "categories": "classical field theory", "tags": "linearity, gauge invariance, symmetry", "date": "2022-05-14 00:00:00 -0400", "snippet": "In Combining Valid Solutions Into New Ones in Classical Field Theory, we showed that any linear combination of valid solutions to classical field equations, is in turn a solution.We then took the idea of solution fields behaving like coordinates further, in Gauge Invariance in Classical Field Theory. Here, we found that it is possible for certain differentiable transformations of solution fields to be new solution fields. To admit such families of solutions (where each choice is called a ‘gauge’) into the Lagrangian, we replace covariant derivatives with the gauge-invariant gauge covariant derivatives. This is functionally equivalent to replacing partial derivatives with covariant derivatives to allow general covariance, in differential geometry.For this post, it is best to read the ones mentioned above as we will rely on the concepts and notation in them. Armed with their ideas, we will now go against the chronology of the said posts i.e. we will begin with gauge fields and derive their linearity.Such a construction is equally logical as the reverse, as gauge invariance is a fundamental feature of classical fields 1 (just as general covariance is, of physics on manifolds). In fact, a classical field is defined to possess such structure among others. 2 Therefore, just as deriving linearity from the fundamental Euler-Lagrange equations is rigorous, so is deriving it from other basic concepts such as gauge invariance (if possible, that is). Let us see how this can be done, and gain a new perspective on linearity in the process.DerivationGauge transformationLet us start with a solution set \\(\\left\\{ \\phi_a \\right\\}\\) for the equations of motion for a Lagrangian \\(\\mathcal{L}\\). We now apply a gauge transformation as,\\[\\phi_a \\to \\widetilde{\\phi}_a\\]The new equations of motion become,\\[\\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\phi}^a} - D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} = 0\\]where,\\[\\begin{align}\\phi^a &amp;amp; = \\delta^{a b} \\phi_b \\\\\\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} &amp;amp; = \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi^a \\right)} \\\\D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} &amp;amp; = \\nabla_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} - \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} b} G^b_{\\phantom{b} \\mu a} \\\\ \\\\G^b_{\\phantom{b} \\mu a} &amp;amp; = J^b_{\\phantom{b} c} \\partial_\\mu J^c_{\\phantom{c} a} \\\\J^b_{\\phantom{b} a} &amp;amp; = \\frac{\\partial \\widetilde{\\phi}_a}{\\partial \\phi_b}\\end{align}\\]Invariance of equations of motionNow, we ask, when are the fields in the new gauge still valid solutions for the equations of motion in the original gauge? The most general such scenario is when gauge covariant derivatives reduce to covariant derivatives under the said gauge transformation, thereby preserving the form of the equations of motion. Here, we find,\\[G^b_{\\phantom{b} \\mu a} = J^b_{\\phantom{b} c} \\partial_\\mu J^c_{\\phantom{c} a} = 0\\]This is further true in general only if \\(\\partial_\\mu J^c_{\\phantom{c} a} = 0\\). Thus,\\[\\partial_\\mu \\frac{\\partial \\widetilde{\\phi}_a}{\\partial \\phi_c} = 0\\]Lastly, for the above to be generally true, the fields in the new gauge must be of the form,\\[\\widetilde{\\phi}_a = C^b_{\\phantom{b} a} \\phi_b \\\\\\]where \\(\\partial_\\mu C^b_{\\phantom{b} a} = 0\\). This is precisely the notion of linearity: linear combinations of solution fields obey the same equations of motion as the original.ConclusionWe have thus found a new way to interpret the above statement on linearity. Namely, linear combinations constitute the most general gauge transformations under which a given Lagrangian is automatically gauge-invariant (as gauge covariant derivatives reduce to covariant derivatives).Thanks for reading! While constructing gauge invariance, we did borrow the coordinate-like behaviour of solution fields seen in linearity-related contexts. However, we did not use the idea of linearity itself to explore gauge invariance, as it is not a prerequisite for such structure. &amp;#8617; The others being, namely: locality, Lorentz invariance and the principle of stationary action. &amp;#8617; " }, { "title": "Gauge Invariance in Classical Field Theory", "url": "/tempus-spatium/gauge-invariance-classical-field-theory/", "categories": "classical field theory", "tags": "gauge invariance, symmetries", "date": "2022-05-10 00:00:00 -0400", "snippet": "In Combining Valid Solutions Into New Ones in Classical Field Theory, we have seen how an arbitrary field \\(\\phi\\) constructed from a class of solutions \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\) for some equations of motion, must obey:\\[\\frac{\\partial \\mathcal{L}}{\\partial \\phi} - \\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} = - \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\]We further interpreted the new field \\(\\phi\\) as being physical only if the above equation resembled the Euler-Lagrange equations. This in general, turned out to be true when \\(\\phi\\) is a linear combination of the solution fields \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\).However, in this post, we shall see how there is a very different way to see the new equations of motion \\(\\phi\\) satisfies: that they can in fact be correct, warranting a modification of the original Lagrangian! Though this may seem strange at first, it is, in fact, extremely crucial to what are called ‘gauge field theories’ in particle physics. We will not go into quantum field theories (QFTs) yet, but we will construct the procedure in the framework of classical field theory, which is simpler and already has a lot of the structure found in QFTs.With that said, let us begin.Fields as coordinates: revisitedThis post begins with the same concept as in Combining Valid Solutions Into New Ones in Classical Field Theory. Namely, we consider an indexed solution set \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\) of scalar fields satisfying the equations of motion for a given Lagrangian density \\(\\mathcal{L}\\). Moreover, we interpret these fields as abstract ‘coordinates’, which can be used to form new coordinates.We now take this analogue of coordinates a step further. Combining the said coordinate fields into new ones shall be parallel to coordinate transformations. Let us write such transformations as:\\[\\phi_{\\left( i \\right)} \\to \\widetilde{\\phi}_{\\left( i \\right)} \\left( \\left\\{ \\phi_{\\left( j \\right)} \\right\\} \\right)\\]We assume such transformations are bijective and differentiable. Thus, the number of coordinates remains the same in such transformations 1 and they are freely convertible to one another. This further hints that these sets of fields are different aspects of the same thing, much like coordinates are arbitrary facets of invariant objects such as tensors.Notation1) We noted in the previous post that despite the solution fields’ index \\(\\left( i \\right)\\) not being physical in the sense of tensor indices, it behaves very similar to one. This will become further evident in this post, and to save us from the hassle of writing the parantheses around the index repeatedly, we will simply omit them.2) However, to distinguish the above indices from the indices \\(i, j, k\\) etc. frequently used to denote spacelike components, we will use \\(a, b, c\\) and so on (nevertheless, we will not need to refer to spacelike indices separately in this post).3) Transformation of coordinate fields and their functions will be accented with tildes. For example, when \\(\\phi_a \\to \\widetilde{\\phi}_a\\), conjugate momenta \\(\\pi^\\mu_{\\phantom{\\mu} a}\\) are mapped as \\(\\pi^\\mu_{\\phantom{\\mu} a} \\to \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a}\\).We do not accent the index itself in the manner of priming regular indices. To demonstrate the reason, consider only one solution \\(\\phi\\), which corresponds to a scalar in the context of coordinate fields. Now, transforming it to some new coordinate \\(\\widetilde{\\phi}\\) does not leave it invariant, unlike for real scalars. Therefore, despite possessing similarity with tensor indices, the coordinate fields’ indices must be handled carefully.4) Analogous to Jacobian tensors in geometry, we define the following scalar quantity: 2\\[J^b_{\\phantom{b} a} = \\frac{\\partial \\widetilde{\\phi}_a}{\\partial \\phi_b}\\]5) We employ the Einstein summation convention. For example,\\[\\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} = J^b_{\\phantom{b} a} \\pi^\\mu_{\\phantom{\\mu} b}\\]6) We define the dual of a coordinate field as,\\[\\phi^a = \\delta^{a b} \\phi_b\\]Therefore, as an example, we write,\\[\\pi^\\mu_{\\phantom{\\mu} a} = \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi^a \\right)}\\]In general, we can raise and lower coordinate field indices respectively using \\(\\delta^{a b}\\) and \\(\\delta_{a b}\\), analogous to the contravariant and covariant metric tensors.Gauge invarianceA question which readers may have asked by this point, is that if solution fields for a given set of Euler-Lagrange equations are like coordinates, what underlying, invariant structure do they represent?This brings us to gauge invariance and field theories obeying them, called gauge theories. To understand them, we first recall what a field \\(\\phi\\) means, in the first place. It is a varying physical parameter with each value corresponding to a a degree of freedom (therefore, there are an uncountably infinite number of them as the parameter here is a real number). We encode the dynamics of the system into the Lagrangian \\(\\mathcal{L}\\), and solutions of the equations of motion obtained give all the possible configurations of the system.Now, it is possible that in a system’s mathematical description, there are extra degrees of freedom which are not physical. In this scenario, some of the degrees of freedom in the equations of motion become redundant, and we can express a given configuration using multiple fields. The particular choice of representing a configuration is called a ‘gauge’ and transformations between different gauges are called ‘gauge transformations’.The idea of gauge invariance is that the dynamics of a system, and hence its action, must remain invariant under gauge transformations, as they are arbitrary artefacts of ‘unphysical’ degrees of freedom. This is analogous to general covariance, the assertion that physics must remain invariant under coordinate transformations as they are artifical constructs.To connect gauge invariance to the previous ideas, let us state how we want to incorporate it. We select a set of distinct and physical solutions \\(\\left\\{ \\phi_a \\right\\}\\) for some equations of motion and switch to a different gauge so that each coordinate field is mapped to a new one. Since field theory must be gauge invariant, the fields in the new gauge must be valid solutions, and thereby satisfy the Euler-Lagrange equations.But the problem is, they do not:\\[\\begin{align}\\nabla_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} &amp;amp; = \\nabla_\\mu \\left( J^b_{\\phantom{b} a} \\pi^\\mu_{\\phantom{\\mu} b} \\right) \\\\ &amp;amp; = J^b_{\\phantom{b} a} \\nabla_\\mu \\pi^\\mu_{\\phantom{b} b} + \\pi^\\mu_{\\phantom{b} b} \\partial_\\mu J^b_{\\phantom{b} a} \\\\ &amp;amp; = J^b_{\\phantom{b} a} \\frac{\\partial \\mathcal{L}}{\\partial \\phi^b} + \\pi^\\mu_{\\phantom{b} b} \\partial_\\mu J^b_{\\phantom{b} a} \\\\ &amp;amp; = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\phi}^a} + \\pi^\\mu_{\\phantom{b} b} \\partial_\\mu J^b_{\\phantom{b} a} \\\\\\therefore \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\phi}^a} - \\nabla_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} &amp;amp; = - \\pi^\\mu_{\\phantom{b} b} \\partial_\\mu J^b_{\\phantom{b} a} \\neq 0\\end{align}\\]Gauge covariant derivativesFaced with the above problem, we are forced to modify the Lagrangian so that the new equations of motion (structurally Euler-Lagrange equations) are the above, thereby introducing gauge invariance.Let us write the general equations of motion in a way which resembles the Euler-Lagrange equations as much as possible:\\[\\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\phi}^a} - D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} = 0\\]where,\\[\\begin{align}D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} &amp;amp; = \\nabla_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} - \\pi^\\mu_{\\phantom{\\mu} b} \\partial_\\mu J^b_{\\phantom{b} a} \\\\ &amp;amp; = \\nabla_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} - J^c_{\\phantom{c} b} \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} c} \\partial_\\mu J^b_{\\phantom{b} a}\\end{align}\\]Further contracting the coefficients of conjugate momenta in the above as,\\[J^c_{\\phantom{c} b} \\partial_\\mu J^b_{\\phantom{b} a} = G^c_{\\phantom{b} \\mu a}\\]we have,\\[D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} = \\nabla_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} - \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} c} G^c_{\\phantom{b} \\mu a}\\]The above quantity is the gauge covariant derivative of conjugate momentum. Here, we are adding correction terms to the usual covariant derivative, much like the latter are correction terms added to partial derivatives:\\[\\nabla_\\mu \\omega_\\nu = \\partial_\\mu \\omega_\\nu - \\omega_\\rho \\Gamma^\\rho_{\\phantom{\\rho} \\mu \\nu}\\]This also demonstrates the correspondance between the connection coefficients \\(\\Gamma^\\rho_{\\phantom{\\rho} \\mu \\nu} = dx^\\rho \\left( \\nabla_\\mu \\partial_\\nu \\right)\\) in geometry and the gauge connection coefficients \\(G^c_{\\phantom{b} \\mu a} = J^c_{\\phantom{c} b} \\partial_\\mu J^b_{\\phantom{b} a}\\) in the previous equation.In retrospect, choosing the indices of the fields to be downstairs was a good choice, for otherwise, the sign of the correction terms in the covariant derivative would be inverted with respect to the geometric case.LagrangianWe have proposed that the general equations of motion for physical fields are,\\[\\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\phi}^a} - D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} = 0\\]Now, let us find a suitable gauge-invariant Lagrangian \\(\\widehat{\\mathcal{L}}\\) which yields equations of motion of the above form (thereby not requiring us to switch the gauge).Firstly, we can say from the form of the equations of motion that the divergence of the new conjugate momenta, say \\(\\widehat{\\pi}^\\mu_{\\phantom{\\mu} a}\\), are\\[\\nabla_\\mu \\widehat{\\pi}^\\mu_{\\phantom{\\mu} a} = D_\\mu \\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a}\\]which implies,\\[\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial \\phi^a} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\phi}^a}\\]As the dependency of both Lagrangians on the field is the same, we have in general,\\[\\widehat{\\mathcal{L}} = \\mathcal{L} \\left( \\phi, \\dots \\right)\\]Since the new Lagrangian is gauge-invariant, the only derivatives of the field it can contain are gauge covariant derivatives. Therefore, we replace any covariant derivatives in the original Lagrangian with gauge covariant derivatives:\\[\\widehat{\\mathcal{L}} = \\mathcal{L} \\left( \\phi, D_\\mu \\phi \\right)\\]We define the gauge covariant derivative of a general scalar field in the following manner (chain rule), so that the properties of derivatives are retained and gauge covariant derivatives can meaningfully reduce to covariant derivatives in any gauge where \\(G^b_{\\phantom{b} \\mu a} = 0\\),\\[\\begin{align}D_\\mu \\phi &amp;amp; = \\frac{\\partial \\phi}{\\partial \\phi_a} D_\\mu \\phi_a \\\\ &amp;amp; = \\frac{\\partial \\phi}{\\partial \\phi_a} \\left( \\partial_\\mu \\phi_a - \\phi_b G^b_{\\phantom{b} \\mu a} \\right) \\\\ &amp;amp; = \\partial_\\mu \\phi - \\phi_b G^b_{\\phantom{b} \\mu a} \\frac{\\partial \\phi}{\\partial \\phi_a}\\end{align}\\]ConclusionWe have, at last, found a Lagrangian which preserves the gauge invariance of physical solution fields. Now, we are left to explore the deep facts it reveals about nature. One of the most striking among them is the conservation of charge, which emerges in scalar electrodynamics from the gauge invariance of complex-valued fields under \\(U \\left( 1 \\right)\\). I shall try to cover it in a future series on quantum mechanics.However, the manner in which the said conservation law emerges (continuity equation) can be seen even in classical field theory. I did not cover it here as it would make the post too long. So, it will be the subject of a new post in the future. Stay tuned! :) Strictly speaking, the said transformations must be intrinsic i.e. they must not refer to any notion of ambient space. Otherwise, we are permitted to transform a set of coordinates into a larger number of coordinates, bijectively. &amp;#8617; Unlike in tensor calculus, we do not have \\(\\widetilde{\\phi}_a = J^b_{\\phantom{b} a} \\phi_b\\). Instead, from the chain rule in calculus, we can only say the same for differentials of the fields, i.e. \\(\\displaystyle{d \\widetilde{\\phi}_a = \\frac{\\partial \\widetilde{\\phi}_a}{\\partial \\phi_b} d \\phi_b = J^b_{\\phantom{b} a} d \\phi_b}\\). However, for the conjugate momenta \\(\\pi^\\mu_{\\phantom{\\mu} a}\\), we can assert \\(\\widetilde{\\pi}^\\mu_{\\phantom{\\mu} a} = J^b_{\\phantom{b} a} \\pi^\\mu_{\\phantom{\\mu} b}\\). Such is the case with any quantity involving the geometric covariant derivatives of the fields. This can be easily demonstrated in inertial coordinates, where geometric covariant derivatives reduce to partial derivatives, which are in turn quotients of appropriate differentials. &amp;#8617; " }, { "title": "Combining Valid Solutions Into New Ones in Classical Field Theory", "url": "/tempus-spatium/combining-solutions-classical-field-theory/", "categories": "classical field theory", "tags": "linearity", "date": "2022-04-23 00:00:00 -0400", "snippet": "A popular theme in some field theories is linearity — wherein valid solutions of the field equations in question can be added and scaled to generate new valid solutions. For example, Maxwell’s equations are linear in vacuum and homogeneous media. A different kind of field equation that is linear, is the time-independent Schrödinger equation for the wavefunction (a complex-valued scalar field). However, we will not go into quantum mechanics or quantum field theory for now.In classical field theory, we build a unified framework which reproduces the myriad of different field theories in different physical contexts. It is only assumed that the classical fields obey some basic principles, such as locality, Lorentz invariance and the principle of stationary action. 1This makes us wonder, that in the general construct of classical field theory, when are solutions of field equations linear? And can they be combined in ways other than adding and scaling, to generate new solutions? Let us find for ourselves.For simplicity, we will stick to real-valued scalar fields in inertial coordinates. However, the results can be extended to tensor fields in arbitrary coordinate systems.Solutions as coordinatesSuppose we have a Lagrangian density \\(\\mathcal{L}\\) and a set of scalar fields \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\) which independently obey the Euler-Lagrange equations for the given Lagrangian:\\[\\frac{\\partial \\mathcal{L}}{\\partial \\phi_{\\left( i \\right)}} = \\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)}\\]The parentheses around the index \\(i\\) remind us that it is not a tensorial index, but a label for each scalar field.In the above equations, we may interpret the role of \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\) as a set of independent coordinates respecting the said equations of motion. Therefore, we have,\\[\\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi_{\\left( j \\right)}} = \\delta_{\\left( i \\right) \\left( j \\right)}\\]Now, we consider a new field \\(\\phi\\) that is a function of the solution set \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\),\\[\\phi = \\phi \\left( \\left\\{ \\phi_{\\left( i \\right)} \\right\\} \\right)\\]Therefore, we can frame our problem as a twofold question:1) When is the following true?\\[\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = \\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)}\\]2) If the above is true, when is \\(\\phi\\) permitted to be of the following form?\\[\\phi = \\sum_i C_{\\left( i \\right)} \\phi_{\\left( i \\right)}\\]Where \\(\\left\\{ C_{\\left( i \\right)} \\right\\}\\) are constants i.e. \\(\\partial_\\mu C_{\\left( i \\right)} = 0\\).The ‘solution’To answer the two questions above, we will, respectively, Expand the Euler-Lagrange equations for the new coordinate \\(\\phi\\) and investigate when it is true. Find the situation in which \\(\\phi\\) reduces to a linear combination of the solutions \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\). The multivariable chain rule from calculus will be used throughout.Expanding the Euler-Lagrange equationsLet us begin by expanding the left hand side of the equations of motion \\(\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = \\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)}\\), in terms of the solutions \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\),\\[\\begin{align}\\frac{\\partial \\mathcal{L}}{\\partial \\phi} &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\phi_{\\left( i \\right)}} \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi} \\\\ &amp;amp; = \\sum_i \\nabla_\\mu \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\right) \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\end{align}\\]Now, expand \\(\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\right) \\phi}\\) and later find the divergence of the right hand side of the equations of motion,\\[\\begin{align}\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\frac{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)}{\\partial \\left( \\partial_\\mu \\phi \\right)} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\frac{\\partial \\left( \\partial_\\mu \\phi \\right)}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\frac{\\partial}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\sum_j \\frac{\\partial \\phi}{\\partial \\phi_{\\left( j \\right)}} \\partial_\\mu \\phi_{\\left( j \\right)} \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\sum_j \\frac{\\partial}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left( \\frac{\\partial \\phi}{\\partial \\phi_{\\left( j \\right)}} \\partial_\\mu \\phi_{\\left( j \\right)} \\right) \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\sum_j \\delta_{\\left( i \\right) \\left( j \\right)} \\frac{\\partial}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left( \\frac{\\partial \\phi}{\\partial \\phi_{\\left( j \\right)}} \\partial_\\mu \\phi_{\\left( j \\right)} \\right) \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\frac{\\partial}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left( \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} \\partial_\\mu \\phi_{\\left( i \\right)} \\right) \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} \\frac{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left[ \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} \\delta^\\nu_{\\phantom{\\nu} \\mu} \\right]^{-1} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\nu \\phi_{\\left( i \\right)} \\right)} \\left( \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} \\right)^{-1} \\delta^\\mu_{\\phantom{\\mu} \\nu} \\\\ &amp;amp; = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\end{align}\\]That was lengthy, but in the end, we have a relatively compact result! In the last step, we took the inverse of the partial derivative \\(\\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}}\\) as simply its reciprocal. This is because for the independent set of functions \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\), we have \\(\\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} = \\frac{d \\phi}{d \\phi_{\\left( i \\right)}}\\), which is a difference quotient.Finally, we find the divergence of the expression obtained in accordance with the Euler-Lagrange equations,\\[\\begin{align}\\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} &amp;amp; = \\nabla_\\mu \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi} \\\\ &amp;amp; = \\sum_i \\nabla_\\mu \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\right) \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi} + \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\end{align}\\]Plugging in the expression for the first term in the previous expansion \\(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}\\),\\[\\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} = \\frac{\\partial \\mathcal{L}}{\\partial \\phi} + \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\]Thus, we have obtained the equations of motion of a general coordinate \\(\\phi \\left( \\left\\{ \\phi_{\\left( i \\right)}\\right\\} \\right)\\) constructed from a set of solution fields. However, for \\(\\phi\\) to be a valid solution, it must obey the Euler-Lagrange equations for the given Lagrangian, which requires us to set:\\[\\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi} = 0\\]Only if the above is true, can we say that \\(\\phi\\) is a valid solution.LinearityIn the above constraint, note the appearance of canonical 4-momentum coordinates \\(\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)}\\). In general, these are not zero. One possibility which ensures the constraint always holds good is:\\[\\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi} = 0\\]From the reciprocal law for differentiation, we have,\\[\\left( \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} \\right)^{-2} \\partial_\\mu \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} = 0\\]Again, generaly, \\(\\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} \\neq 0\\) so we have,\\[\\begin{align}\\partial_\\mu \\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} &amp;amp; = 0 \\\\\\frac{\\partial \\phi}{\\partial \\phi_{\\left( i \\right)}} &amp;amp; = C_{\\left( i \\right)} \\: \\vert \\: \\partial_\\mu C_{\\left( i \\right)} = 0\\end{align}\\]It is easily seen the most general situation where the above is true is,\\[\\phi = \\sum_i C_{\\left( i \\right)} \\phi_{\\left( i \\right)}\\]Therefore, if we want our constraint to be true for all solutions, they must be combined only in the above form i.e. as linear combinations.ConclusionSummaryLet us summarize the results by answering the original questions:1) When does \\(\\phi \\left( \\left\\{ \\phi_{\\left( i \\right)} \\right\\} \\right)\\) obey the Euler-Lagrange equations? When the following constraint is obeyed:\\[\\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi} = 0\\]2) When is \\(\\phi\\) a linear combination of the solutions \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\)? Well, for some unspecified \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\), we could be in a stroke of luck and automatically have the above constraint to be true. However, if we want it to be true for arbitrary solutions, we must only look at linear combinations of these solutions to generate new solutions.In other words, there are always \\(\\phi\\)’s which are linear combinations of \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\) (and an infinite number of them), although there could be other \\(\\phi\\)’s which are non-trivial, non-linear combinations of the solutions.New notation for solutionsIn the notation for the solutions \\(\\left\\{ \\phi_{\\left( i \\right)} \\right\\}\\), we used parentheses around the index as doing otherwise would make them look dubiously like one-forms. Then, we learnt that the most general way to mix these solutions into new ones is to combine them linearly,\\[\\phi = \\sum_i C_{\\left( i \\right)} \\phi_{\\left( i \\right)} \\: \\vert \\: \\partial_\\mu C_{\\left( i \\right)} = 0\\]Therefore, in the abstract sense, each solution \\(\\phi_{\\left( i \\right)}\\) is, in fact, behaving like a basis vector, with the coefficients \\(C_{\\left( i \\right)}\\) forming the components! However, the index \\(i\\) here is abstract and not related to the coordinates \\(x^\\mu\\) as in objects like \\(\\partial_\\mu = \\frac{\\partial}{\\partial x^\\mu}\\), so it is still better to retain the parentheses. Nevertheless, we can apply the Einstein summation convention here as it need not be restricted to ‘honest’ indices:\\[\\phi = C^{\\left( i \\right)} \\phi_{\\left( i \\right)}\\]where \\(C^{\\left( i \\right)} = \\delta^{\\left( i \\right) \\left( j \\right)} C_{\\left( j \\right)}\\).The application of the Einstein summation convention here is further justified by the fact that in the context of our derivations, upper and lower \\(\\left( i \\right)\\) indices have repeatedly appeared along with the summation operation \\(\\sum \\limits_{i}\\). For example,\\[\\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} = \\frac{\\partial \\mathcal{L}}{\\partial \\phi} + \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\]Hence, the summation symbol becomes redundant and we can write:\\[\\nabla_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} = \\frac{\\partial \\mathcal{L}}{\\partial \\phi} + \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi_{\\left( i \\right)} \\right)} \\partial_\\mu \\frac{\\partial \\phi_{\\left( i \\right)}}{\\partial \\phi}\\]Finishing noteAll in all, we see a linear structure in classical field theory, namely in solutions to field equations of the form of the Euler-Lagrange equations. This is the beginning of the portal into the application of linear algebraic notions in classical field theory. The bridge between linear algebra (and the more general tensor algebra) and field theory is an exciting place, so hopefully, we’ll tread it in the future!I also hope you enjoyed this reading post (thanks for doing! :) Cheers. We can add further structure to field theories by introducing what are called ‘gauge symmetries’. This is explored in the post Gauge Invariance in Classical Field Theory. &amp;#8617; " }, { "title": "The Real Reason Nothing Travels Faster Than Light", "url": "/tempus-spatium/real-reason-nothing-travels-faster-than-light/", "categories": "relativity", "tags": "spacetime interval", "date": "2022-04-16 00:00:00 -0400", "snippet": "The problemOne of the celebrated results of Einstein’s special theory of relativity is that no physical object can travel faster than light, no matter the frame of reference. A popular explanation for this is that as an object approaches the speed of light, its relativistic energy \\(E = \\gamma mc^2\\) 1 tends to infinity. Thus, it would require infinite energy to propel a body (with mass, at least) to the speed of light or beyond.However, as a thought experiment, what if we consider a body with mass that is somehow already travelling at the speed of light or even faster? What if we do not concern ourselves, at least for the moment, with the origin of this hypothetical motion (i.e. the dynamics), but only look at the situation (the kinematics)? If we can disprove the kinematics, it is stronger than disproving the dynamics as we are eliminating the very possibility of a body travelling faster than light, irrelevant of any apparent methods to achieve the same.A popular approach to disprove the kinematics of faster-than-light motion is to calculate observables such as the body’s relativistic energy \\(E = \\gamma mc^2\\) and momentum \\(p = \\gamma m v\\), and demonstrate that their values make no physical sense (they turn out to be imaginary as \\(\\gamma\\) is imaginary for \\(v&amp;gt;c\\)).Yet, this philosophy is not satisfying in that there are instances where the breaking down of mathematics does not necessarily indicate the breaking down of physics. Here is an analogy: consider a straight line in a two-dimensional Cartesian coordinate system with the equation of motion \\(y = mx + C\\). When the line is vertical with respect to the x-axis, we have \\(m = \\pm \\infty\\), which is absurd. But that does not mean a line vertical with respect to a chosen axis is a geometrical impossibility. In fact, on being careful, one observes that the derivation of \\(y=mx+C\\) invariably assumes in some form that the line in question is not vertical. It is not surprising that this equation may, therefore, break down in a scenario excluded from the original assumptions.Similarly, the flaw in the following logic,\\[\\gamma \\left( v \\right) \\notin \\left[ 1, \\infty \\right) \\forall \\: v&amp;gt;c \\implies v \\ngtr c\\]is that perhaps the way measurable quantities like \\(E\\) and \\(p\\) depend on \\(\\gamma\\) is an artefact of knowingly or unknowingly assuming \\(v&amp;lt;c\\). Unless we are very cautious, there is freedom for ‘reality’ being something like (for particles with mass): 2\\[E \\left( v \\right) = \\begin{cases} \\gamma mc^2 &amp;amp; v &amp;lt; c \\\\ E_+ \\left( v \\right) &amp;amp; v&amp;gt;c \\end{cases}\\]\\[\\lim_{v \\to c^-} E \\left( v \\right) = \\lim_{v \\to c^+} E_+ \\left( v \\right) = \\infty\\]\\[E \\in \\mathbb{R}^+ \\cup \\left\\{ 0 \\right\\}\\]Note that we have assumed \\(E \\left( v \\right)\\) is non-negative throughout, for it to represent a physically sensible total energy. In this overall picture, it is not surprising that \\(E \\left( v \\right)\\) for \\(v&amp;gt;c\\) may assume strange values if we forcefully plug in such \\(v\\)’s for the \\(v &amp;lt; c\\) case.Another way to think of the whole problem is that the manner in which we assign physical meaning to observables based on their values, is tautological — it tells us nothing about the universe in itself. On the other hand, physical arguments have an inherent logical structure that prevails ‘beneath’ the observables. To analytically demonstrate the incoherence of an observable with a certain value, we must investigate the corresponding physical scenario and find its logical inconsistency by reasoning. Otherwise, there always exists the pathological possibility that we have overlooked certain assumptions which are leading to the mathematical breakdown (and not the physics).Therefore, in this post, we will develop such an analytical proof for the statement ‘no body can travel faster than light’, within the framework of special relativity. But before building the main argument, let us review some key concepts.Spacetime intervalBesides the principle of relativity, the other important postulate of special relativity is that the speed of light in vacuum, \\(c\\), is the same in all inertial frames.Let us work in local coordinates \\(x^\\mu = \\left( ct, x^i \\right)\\). Suppose we track a pulse of light with the coordinates \\(x^i\\). Then, we have,\\[\\begin{align}c^2 &amp;amp; = \\frac{dx_i}{dt} \\frac{dx^i}{dt} \\\\c^2 dt^2 &amp;amp; = dx_i dx^i \\\\c^2 dt^2 - dx_i dx^i &amp;amp; = 0\\end{align}\\]For \\(c\\) to be a Lorentz invariant, the above equation must be true in all inertial frames, i.e.,\\[c^2 dt^2 - dx_i dx^i = c^2 d{t^\\prime}^2 - dx_{i^\\prime} dx^{i^\\prime} = 0\\]The above invariant is precisely the infinitesimal spacetime interval squared,\\[ds^2 = dx_\\nu dx^\\nu = dx^\\mu \\eta_{\\mu \\nu} dx^\\nu\\]if we define the metric tensor \\(\\eta_{\\mu \\nu}\\) in local coordinates as, 3\\[\\begin{align} \\eta_{00} &amp;amp; = 1 \\\\ \\eta_{ij} &amp;amp; = - g_{ij} \\\\ \\eta_{i 0} &amp;amp; = \\eta_{0 i} = 0 \\end{align}\\]where \\(g_{ij}\\) is the spatial metric tensor for the chosen coordinate system. The last equation says that in local coordinates, space and time are orthogonal. This is true in all inertial frames (in local coordinates). Hence, the timelike and spacetimelike components of the metric are invariant, although the spacelike components \\(-g_{ij}\\) transform like a rank-2 tensor.On the other hand, in arbitrary coordinates, \\(\\eta_{\\mu \\nu}\\) transforms as a whole like the components of a rank-2 tensor. Therefore, \\(ds^2 = dx^\\mu \\eta_{\\mu \\nu} dx^\\nu\\) is always invariant, even when it is not zero. This also allows us to meaningfully venture beyond the case \\(ds^2 = 0\\) and consider \\(ds^2 \\neq 0\\).Note that given two events A and B, the spacetime interval between them is defined in the manner,\\[\\Delta s^2 = \\int_A^B ds^2 = \\Delta_A^B x_\\mu \\Delta_A^B x^\\mu\\]Let us now investigate when \\(ds^2\\) is zero or non-zero, and what these physically represent.World linesWe now generalize our problem from tracking a pulse of light to tracking any physical body, with the coordinates \\(x^i\\). Then, the world line (trajectory in spacetime) of a body is defined, in local coordinates, as the function \\(x^i \\left( x^0 \\right) = x^i \\left( ct \\right)\\). 4Null world linesA null world line is one along which \\(ds^2\\) is null, or zero. As we have already seen, \\(ds^2=0\\) for bodies travelling at the speed of light \\(c\\).Timelike world lineTimelike world lines satisfy \\(ds^2 &amp;gt; 0\\). They are called ‘timelike’ as \\(ds^2 = c^2 dt^2 - dx_i dx^i &amp;gt;0\\) when the timelike part \\(c^2 dt^2\\) is greater than the spacelike part \\(dx_i dx^i\\).As \\(ds^2\\) is an invariant, if \\(ds^2&amp;gt;0\\) in one inertial frame, so must it be in all other frames. In other words, a timelike world line is timelike no matter the frame of reference.Spacelike world lineSimilar to timelike world lines, we may define spacelike world lines for which \\(ds^2&amp;lt;0\\) (the spacelike part of \\(ds^2\\) is greater than the timelike part \\(c^2 dt^2\\)).Again, if \\(ds^2&amp;lt;0\\) in one frame, this must be true in all frames.The paradoxConsider a body moving along a spacelike world line in some frame. We have,\\[ds^2 = c^2 dt^2 - dx_i dx^i &amp;lt; 0\\]Now, we rewrite the spacelike part as,\\[\\begin{align}dx_i dx^i &amp;amp; = \\frac{dx_i}{dt} \\frac{dx^i}{dt} dt^2 \\\\ &amp;amp; = v^2 dt^2\\end{align}\\]Therefore, the infinitesimal spacetime interval squared is,\\[\\begin{align}ds^2 &amp;amp; = c^2 dt^2 - v^2 dt^2 \\\\ds^2 &amp;amp; = \\left( c^2 - v^2 \\right) dt^2\\end{align}\\]In the body’s own frame, \\(v=0\\) (as the body is at rest relative to itself). Hence,\\[ds^2 = c^2 dt^2 \\geq 0\\]However, \\(ds^2 &amp;lt; 0\\) in the original frame, and it must be true in all frames, including the body’s own! This results in a paradox which can only be resolved by concluding that the assumption that there can exist a frame where \\(ds^2 &amp;lt; 0\\), is in fact false.We do not see this contradiction in the cases \\(ds^2 &amp;lt; 0\\) and \\(ds^2 = 0\\). In the latter case, however, we need to be careful as it does not make sense to consider observables in the body’s own frame, when it is travelling at the speed of light in some reference frame (as frames moving along null trajectories are not truly reference frames).Thus, we can safely say that indeed, no body can travel faster than light in any inertial frame, as it not only breaks mathematics, but also logic!Conclusion (instantaneously inertial frames)There is some unfinished business in our argument. The argument primarily stands on the invariance of \\(ds^2\\), derived from the invariance of the speed of light in inertial frames. The body we are tracking may very well be moving non-uniformly, which begs the question of why the sign of \\(ds^2\\) for its world line must be the same in an inertial reference frame, and its own not-necessarily-inertial frame.This is where the idea of instantaneously inertial frames comes in. If we ‘split’ the non-uniform motion of a body into infinitesimal steps, the motion along each step behaves just like uniform motion.Or, we can construct the argument in the reverse fashion. Suppose we restrict ourselves to uniform motion. Then, the correctness of our argument is straightforward as the body’s own frame is inertial. Now, nothing stops us from considering inertial motion lasting for tiny time intervals, with acceleration in between. For all these intervals, the law ‘the body cannot travel faster than light’ can be verified individually. As the ‘gaps’ of acceleration tend to zero, the motion of the body tends to continuous non-uniform motion, and the law holds at each instant.Of course, rigorously demonstrating the above formally involves calculus as we cannot simply assume that the discontinuous motion in question, does, in fact, reduce to continuous motion in the appropriate limit. The entire exercise, if done rigorously, is unexpectedly rich as it involves measure theory, analysis and topology! I will perhaps explore it in the future in a series of posts.Until then, I leave it to the reader to question how much rigorour is necessary to prove, in a foolproof manner, that no physical body can travel faster than light. Asking such questions can be, surprisingly or not, of immense practical value. On a related note (but in the context of pure mathematics), Terence Tao said, in his elegant lectures on analysis, Moreover,as you learn analysis you will develop an “analytical way of thinking”,which will help you whenever you come into contact with any new rulesof mathematics, or when dealing with situations which are not quitecovered by the standard rules … … You will develop a sense of why a rule in mathematics(e.g., the chain rule) works, how to adapt it to new situations, and whatits limitations (if any) are; this will allow you to apply the mathematicsyou have already learnt more confidently and correctly. Here, \\(\\gamma = \\frac{1}{\\sqrt{1 - \\frac{v^2}{c^2}}}\\) is the Lorentz factor, \\(m\\) is the mass of the body (a Lorentz invariant) and \\(c\\) is the speed of light in vacuum. &amp;#8617; For particles without mass, \\(E=pc\\) from the energy-momentum relation \\(E^2 = m^2 c^4 + p^2 c^2\\). As their energy is completely kinetic, these particles only travel along null world lines i.e. with \\(v=c\\). &amp;#8617; This is the ‘mostly minus’ or particle physicist’s sign convention for the metric, \\(\\left( +, -, -, - \\right)\\). The opposite choice is the ‘mostly plus’ or relativist’s sign convention. &amp;#8617; In general, a world line is of the form \\(x^i \\left( \\lambda \\right)\\) where \\(\\lambda\\) is any scalar parameter (not necessarily time). &amp;#8617; " }, { "title": "Conservation of Mass in Classical Mechanics", "url": "/tempus-spatium/conservation-mass-classical-mechanics/", "categories": "classical mechanics", "tags": "mass, Noether theorem, symmetries, affine space", "date": "2022-03-30 00:00:00 -0400", "snippet": "In this post, we will derive the conservation of total mass of a closed system from simpler physical facts, within the framework of classical mechanics. We will first recall the key concepts which will be used to build the argument, namely: The conservation of total linear momentum The principle of relativity Then, we will construct the main argument on the basis of the above.Conservation of total linear momentumNoether’s theoremConsider a system of particles with generalized coordinates \\(\\left\\{ \\pmb{q}_{i} \\right\\}\\) and conjugate momenta \\(\\left\\{ \\pmb{p}_{i} \\right\\}\\). Let the system be infinitesimally displaced as,\\[\\pmb{q}_{i} \\to \\pmb{q}_{i} + \\epsilon \\: \\pmb{Q}\\]where \\(\\epsilon\\) is a very small constant and \\(\\pmb{Q}\\) is an arbitrary 3-vector. The variation in the generalized coordinates is,\\[\\delta \\pmb{q}_{i} = \\epsilon \\: \\pmb{Q}\\]and the variation in the generalized velocities is similarly,\\[\\delta \\dot{\\pmb{q}}_{i} = \\epsilon \\: \\dot{\\pmb{Q}}\\]The corresponding variation in the Lagrangian of the system can be computed using the above; the Euler-Lagrange equations; and the chain rule for derivatives,\\[\\begin{align}\\delta L &amp;amp; = \\sum_i \\frac{\\partial L}{\\partial \\pmb{q}_{i}} \\cdot \\delta \\pmb{q}_{i} + \\sum_i \\frac{\\partial L}{\\partial \\dot{\\pmb{q}}_{i}} \\cdot \\delta \\dot{\\pmb{q}}_{i} \\\\ &amp;amp; = \\sum_i \\dot{\\pmb{p}}_{i} \\cdot \\delta \\pmb{q}_{i} + \\sum_i \\pmb{p}_{i} \\cdot \\delta \\dot{\\pmb{q}}_{i} \\\\ &amp;amp; = \\frac{d}{dt} \\left( \\sum_i \\pmb{p}_{i} \\cdot \\delta \\pmb{q}_{i} \\right) \\\\ &amp;amp; = \\epsilon \\: \\frac{d}{dt} \\left( \\sum_i \\pmb{p}_{i} \\cdot \\pmb{Q} \\right)\\end{align}\\]If the system is symmetric under the said transformation, its action remains invariant, which implies the variation of its Lagrangian must be zero. It thus follows,\\[\\frac{d}{dt} \\left( \\sum_i \\pmb{p}_{i} \\cdot \\pmb{Q} \\right) = 0\\]We have thus found a time-invariant quantity associated with the said symmetry. This is Noether’s theorem.Symmetry under linear translationSuppose we translate a system linearly along a vector \\(\\pmb{X}\\). The infinitesimal transformation dictated by \\(\\pmb{X}\\) is then of the form,\\[\\pmb{x}_{i} \\to \\pmb{x}_{i} + \\epsilon \\: \\pmb{X}\\]Here, we have switched from q’s to x’s as the Cartesian coordinate system naturally incorporates linear translation via direct addition of coordinates. Taking \\(\\dot{\\pmb{X}} = 0\\), Noether’s theorem gives us,\\[\\frac{d}{dt} \\left( \\sum_i \\pmb{p}_{i} \\right) \\cdot \\pmb{X} = 0\\]If the above is true for all \\(\\pmb{X} \\in \\mathbb{A}^3\\) where \\(\\mathbb{A}^3\\) is the affine 3-dimensional space, we must have,\\[\\frac{d}{dt} \\sum_i \\pmb{p}_{i} = \\pmb{0}\\]Thus, the total momentum, specifically linear momentum, of a system is conserved when it is symmetric under infinitesimal linear translations.Form of linear momentumFor a system of particles symmetric under infinitesimal linear translations, its Lagrangian looks like so in Cartesian coordinates:\\[L \\left( \\left\\{ \\pmb{x}_{i} \\right\\}, \\left\\{ \\dot{\\pmb{x}}_{i} \\right\\} \\right) = \\sum_i \\frac{1}{2} m_i \\dot{\\pmb{x}}_{i} \\cdot \\dot{\\pmb{x}}_{i} - V \\left( \\left\\{ \\pmb{x}_{i} - \\pmb{x}_j \\right\\} \\right)\\]where \\(\\left\\{ m_i \\right\\}\\) are the masses of the particles. The Lagrangian does not explicitly depend on time as we are considering closed systems only. Secondly, the potential energy is a function of only the relative positions of the particles with respect to each other, i.e. \\(V : \\mathbb{A}^4 \\to \\mathbb{R}\\) (thus, translating the system does not affect its potential energy).Now, the conjugate momentum (here linear momentum) of each particle is,\\[\\begin{align}\\pmb{p}_{i} &amp;amp; = \\frac{\\partial L}{\\partial \\dot{\\pmb{x}_{i}}} \\\\ &amp;amp; = \\frac{\\partial}{\\partial \\dot{\\pmb{x}_{i}}} \\sum_j \\frac{1}{2} m_j \\dot{\\pmb{x}}_j \\cdot \\dot{\\pmb{x}}_j \\\\ &amp;amp; = \\sum_j \\frac{1}{2} m_j \\cdot 2 \\: \\delta_{ij} \\: \\dot{\\pmb{x}}_j \\\\ &amp;amp; = m_i \\dot{\\pmb{x}_i}\\end{align}\\]Thus, the conservation of total linear momentum takes the form,\\[\\frac{d}{dt} \\sum_i \\pmb{p}_i = \\frac{d}{dt} \\sum_i m_i \\dot{\\pmb{x}}_i = \\pmb{0}\\]Principle of relativityDefinitionAn important feature of classical mechanics is that changing the frame of reference leaves the form of physical laws invariant. When we switch from one inertial frame to another, a Galilean transformation is applied to all quantities pertaining to the former, to find their description in the latter.Galilean transformations are specifically the transformations that leave intervals in space, and intervals in time invariant. In other words, they preserve the structure of \\(\\mathbb{A}^4\\).The invariance of physical laws under such transformations is called the principle of relativity.Galilean groupThere are 3 independent kinds of Galilean transformations.Firstly, we have shift of the origin, so that,\\[\\pmb{x}_i \\to \\pmb{x}_i + \\pmb{s}, \\: \\dot{\\pmb{s}} = \\pmb{0}\\]Secondly, we have boosts:\\[\\pmb{x}_i \\to \\pmb{x}_i + t \\: \\pmb{v}, \\: \\dot{\\pmb{v}} = \\pmb{0}\\]Lastly, we have rotations,\\[\\pmb{x}_i \\to \\pmb{R} \\: \\pmb{x}_i, \\: \\dot{\\pmb{R}} = \\pmb{O}\\]where \\(\\pmb{R}\\) is a linear transformation and \\(\\pmb{O}\\) is the null operator. For lengths to be preserved, \\(\\pmb{R}\\) must be linear transformations satisfying the below:\\[\\begin{align}\\left( \\pmb{R} \\: \\pmb{x}_i \\right) \\cdot \\left( \\pmb{R} \\: \\pmb{x}_i \\right) &amp;amp; = \\pmb{x}_i \\cdot \\pmb{x}_i \\\\\\pmb{x}_i^T \\: \\pmb{R}^T \\pmb{R} \\: \\pmb{x}_i &amp;amp; = \\pmb{x}_i^T \\pmb{x}_i \\\\\\pmb{R}^T \\pmb{R} &amp;amp; = \\pmb{I} \\\\\\end{align}\\]where \\(T\\) denotes ‘transpose’ and \\(\\pmb{I}\\) is the identity operator.Any Galilean transformation is a direct product of the above transformations. Together, they form the Galilean group \\(G\\) over \\(\\mathbb{A}^4\\).Conservation of total massConsider an inertial frame in which the total linear momentum of a system is conserved:\\[\\frac{d}{dt} \\sum_i m_i \\dot{\\pmb{x}}_i = \\pmb{0}\\]Now, we will apply a Galilean transformation on the original frame. By the principle of relativity, the conservation of total linear momentum stays true in the new frame. Note that the shift of origin does not manifest in the linear momenta as the latter are first derivatives of position. Using these facts and the conservation of total linear momentum in the initial frame,\\[\\begin{align}\\frac{d}{dt} \\sum_i m_i \\: \\pmb{R} \\left( \\dot{\\pmb{x}}_i + \\pmb{v} \\right) &amp;amp; = \\pmb{0} \\\\\\frac{d}{dt} \\sum_i m_i \\pmb{R} \\: \\dot{\\pmb{x}}_i + \\frac{d}{dt} \\sum_i m_i \\: \\pmb{R} \\: \\pmb{v} &amp;amp; = \\pmb{0}\\end{align}\\]As \\(\\pmb{R}\\) is a linear transformation and \\(\\dot{R} = \\pmb{O}\\) and \\(\\dot{\\pmb{v}} = \\pmb{0}\\),\\[\\begin{align}\\pmb{R} \\: \\frac{d}{dt} \\sum_i m_i \\dot{\\pmb{x}}_i + \\frac{d}{dt} \\left( \\sum_i m_i \\right) \\pmb{R} \\: \\pmb{v} &amp;amp; = \\pmb{0} \\\\\\pmb{R} \\times \\pmb{0} + \\frac{d}{dt} \\left( \\sum_i m_i \\right) \\pmb{R} \\: \\pmb{v} &amp;amp; = \\pmb{0} \\\\\\frac{d}{dt} \\left( \\sum_i m_i \\right) \\pmb{R} \\: \\pmb{v} &amp;amp; = \\pmb{0} \\\\\\end{align}\\]Since the boost \\(\\pmb{v}\\) and rotation \\(\\pmb{R}\\) are arbitrary,\\[\\frac{d}{dt} \\left( \\sum_i m_i \\right) = 0\\]Presto, the total mass of a closed system is conserved!ConclusionFrom the above exercise, we see that the conservation of mass is not an isolated experimental fact in classical mechanics. In fact, it follows from two very fundamental ideas, the conservation of total linear momentum and principle of relativity. The former in turn emerges from the symmetry of systems under infinitesimal linear translations. The principle of relativity, on the other hand, is a consequence of the equations of motion being second-order, thereby making the dynamics of a system invariant no matter the reference frame." }, { "title": "Components of Covariant Derivative of a Tensor Field", "url": "/tempus-spatium/components-covariant-derivative-tensor-field/", "categories": "tensor calculus", "tags": "covariant derivatives, tensors", "date": "2022-03-06 00:00:00 -0500", "snippet": "Let us find the covariant derivative of a rank \\((p, q)\\) tensor field \\(\\pmb{\\phi}\\) by applying the Leibniz law repeatedly:\\[\\nabla_\\rho \\left( \\pmb{\\phi} \\otimes \\pmb{\\psi} \\right) = \\left( \\nabla_\\rho \\pmb{\\phi} \\right) \\otimes \\pmb{\\psi} + \\pmb{\\phi} \\otimes \\left( \\nabla_\\rho \\pmb{\\psi} \\right)\\]Note that we will be implicitly using a Levi-Civita connection on a pseudo-Riemannian manifold.Derivation\\[\\begin{align}\\nabla_\\rho \\pmb{\\phi} &amp;amp; = \\nabla_\\rho \\left( \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\right) \\\\ &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\left( \\nabla_\\rho \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\right) \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\left( \\nabla_\\rho \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\right)\\end{align}\\]In the above, when \\(\\nabla_\\rho\\) acts explicitly on the components of \\(\\pmb{\\phi}\\), the components behave like scalars, thereby allowing us to replace the covariant derivative with the partial derivative \\(\\partial_\\rho\\). Let us now apply the Leibniz law,\\[\\begin{align}\\nabla_\\rho \\pmb{\\phi} &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp;+ \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\left[ \\sum_{i=1}^p \\underset{k=1}{\\overset{i-1}{\\bigotimes}} \\partial_{\\mu_k} \\otimes \\left( \\nabla_\\rho \\partial_{\\mu_i} \\right) \\underset{k=i+1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_k} \\right] \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\left[ \\sum_{j=1}^q \\underset{k=1}{\\overset{j-1}{\\bigotimes}} \\text{d} x^{\\nu_k} \\otimes \\left( \\nabla_\\rho \\text{d}x^j \\right) \\underset{k=j+1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_k} \\right]\\end{align}\\]Now, we use the following definitions of the connection coefficients,\\[\\begin{align}\\nabla_\\rho \\partial_\\mu &amp;amp; = \\Gamma^\\sigma_{\\phantom{\\sigma} \\rho \\mu} \\partial_\\sigma \\\\\\nabla_\\rho \\text{d}x^\\nu &amp;amp; = \\Gamma^\\nu_{\\phantom{\\nu} \\rho \\sigma} \\text{d}x^\\sigma\\end{align}\\]as well as the linearity of the tensor product,\\[\\begin{align}\\bigotimes_k x^\\sigma \\partial_\\sigma &amp;amp; = x^\\sigma \\bigotimes_k \\partial_\\sigma \\\\ \\bigotimes_k \\theta_\\sigma \\text{d}x^\\sigma &amp;amp; = \\theta_\\sigma \\bigotimes_k \\text{d}x^\\sigma\\end{align}\\]to get:\\[\\begin{align}\\nabla_\\rho \\pmb{\\phi} &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\left[ \\sum_{i=1}^p \\Gamma^{\\sigma}_{\\phantom{\\sigma} \\rho \\mu_i} \\underset{k=1}{\\overset{i-1}{\\bigotimes}} \\partial_{\\mu_k} \\otimes \\partial_\\sigma \\underset{k=i+1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_k} \\right] \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; - \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\left[ \\sum_{j=1}^q \\Gamma^{\\nu_j}_{\\phantom{\\nu_j} \\rho \\sigma} \\underset{k=1}{\\overset{j-1}{\\bigotimes}} \\text{d} x^{\\nu_k} \\otimes \\text{d}x^\\sigma \\underset{k=j+1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_k} \\right] \\\\ \\\\ &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\sum_{i=1}^p \\Gamma^{\\sigma}_{\\phantom{\\sigma} \\rho \\mu_i} \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{k=1}{\\overset{i-1}{\\bigotimes}} \\partial_{\\mu_k} \\otimes \\partial_\\sigma \\underset{k=i+1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_k} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; - \\sum_{j=1}^q \\Gamma^{\\nu_j}_{\\phantom{\\nu_j} \\rho \\sigma} \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{k=1}{\\overset{j-1}{\\bigotimes}} \\text{d} x^{\\nu_k} \\otimes \\text{d}x^\\sigma \\underset{k=j+1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_k}\\end{align}\\]Let us exchange the indices \\(\\sigma\\) and \\(\\mu_i\\), and \\(\\sigma\\) and \\(\\nu_j\\) wherever they are dummy indices:\\[\\begin{align}\\nabla_\\rho \\pmb{\\phi} &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\sum_{i=1}^p \\Gamma^{\\mu_i}_{\\phantom{\\mu_i} \\rho \\sigma} \\phi^{\\mu_1 \\dots \\mu_{i-1} \\sigma \\mu_{i+1} \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_{i-1} \\sigma \\mu_{i+1} \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{k=1}{\\overset{i-1}{\\bigotimes}} \\partial_{\\mu_k} \\otimes \\partial_{\\mu_i} \\underset{k=i+1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_k} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; - \\sum_{j=1}^q \\Gamma^{\\sigma}_{\\phantom{\\sigma} \\rho \\nu_j} \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_{j-1} \\sigma \\nu_{j+1} \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{k=1}{\\overset{j-1}{\\bigotimes}} \\text{d} x^{\\nu_k} \\otimes \\text{d}x^{\\nu_j} \\underset{k=j+1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_k} \\\\ \\\\ &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; + \\sum_{i=1}^p \\Gamma^{\\mu_i}_{\\phantom{\\mu_i} \\rho \\sigma} \\phi^{\\mu_1 \\dots \\mu_{i-1} \\sigma \\mu_{i+1} \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_{i-1} \\sigma \\mu_{i+1} \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j} \\\\ &amp;amp; - \\sum_{j=1}^q \\Gamma^{\\sigma}_{\\phantom{\\sigma} \\rho \\nu_j} \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_{j-1} \\sigma \\nu_{j+1} \\dots \\nu_q} \\underset{i=1}{\\overset{p}{\\bigotimes}} \\partial_{\\mu_i} \\underset{j=1}{\\overset{q}{\\bigotimes}} \\text{d} x^{\\nu_j}\\end{align}\\]Thus, by factoring out the components, we get,\\[\\begin{align} \\nabla_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} &amp;amp; = \\partial_\\rho \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\\\ &amp;amp; + \\sum_{i=1}^p \\Gamma^{\\mu_i}_{\\phantom{\\mu_i} \\rho \\sigma} \\phi^{\\mu_1 \\dots \\mu_{i-1} \\sigma \\mu_{i+1} \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_{i-1} \\sigma \\mu_{i+1} \\dots \\mu_p} \\nu_1 \\dots \\nu_q} \\\\ &amp;amp; - \\sum_{j=1}^q \\Gamma^{\\sigma}_{\\phantom{\\sigma} \\rho \\nu_j} \\phi^{\\mu_1 \\dots \\mu_p}_{\\phantom{\\mu_1 \\dots \\mu_p} \\nu_1 \\dots \\nu_{j-1} \\sigma \\nu_{j+1} \\dots \\nu_q}\\end{align}\\]Tuple index notationTo make the above notation less messy, let us use the tuple index notation: Tuples of indices are replaced by their capital letter. For example, \\(\\mu_1 \\dots \\mu_p\\) becomes \\(M\\) and \\(\\nu_1 \\dots \\nu_q\\) becomes \\(N\\). A subset of a tuple, running up to some index \\(i-1 \\leq p\\), i.e. \\(\\mu_1 \\dots \\mu_{i-1}\\), is written as \\(M_i^-\\). Similarly, a subset running from an index \\(i+1\\), \\(\\mu_{i+1} \\dots \\mu_p\\) is written as \\(M_i^+\\). From the above, it follows that we can write \\(M\\) as \\(M_i^- \\mu_i M_i^+\\). Using the above notation, we can write the covariant derivative of a tensor, in the component form, as,\\[\\nabla_\\rho \\phi^M_{\\phantom{M} N} = \\partial_\\rho \\phi^M_{\\phantom{M} N} + \\sum_{i=1}^p \\Gamma^{\\mu_i}_{\\phantom{\\mu_i} \\rho \\sigma} \\phi^{M_i^- \\sigma M_i^+}_{\\phantom{M_i^- \\sigma M_i^+} N} - \\sum_{j=1}^q \\Gamma^\\sigma_{\\phantom{\\sigma} \\rho \\nu_j} \\phi^M_{\\phantom{M} N_j^- \\sigma N_j^+}\\]" }, { "title": "Algebra Done Tensorially: Part 3 (Complex Numbers and Quaternions)", "url": "/tempus-spatium/complex-numbers-quaternions/", "categories": "representation theory", "tags": "complex numbers, quaternions, algebras, tensors", "date": "2022-02-24 00:00:00 -0500", "snippet": "Welcome to Part 3 of ‘Algebra Done Tensorially’. It’s been a while since the previous posts, so let us resume our investigation of algebras without further ado. For readers who haven’t read earlier posts in this series yet, I’d recommend you to read Part 1 and Part 2 first :) Parts Topics Part 1 (Bilinear Products) tensors, bilinear products Part 2 (Algebras Over Fields) linear maps, algebra, degrees of freedom Part 3 (Complex Numbers and Quaternions) complex numbers, quaternions, gamma matrices Part 4 (Clifford Algebras) in progress Part 5 (Lie Algebras) in progress Recap: vector algebrasIn the previous post, we had seen that algebras on rank \\(1\\) vector spaces are the only ones that are non-redundant in terms of the degrees of freedom of the elements of the vector space (revisit the argument here). This makes things much simpler, as we don’t have to bother about general higher-dimensional algebras (for them, the J tensor is richer in information than the degrees of freedom).Therefore, we are left with the notion of bilinear vector products which act on a vector space as a linear transformation by making the other argument implicit:\\[\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} ki} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\]According to the way we had defined the term ‘algebra’, the ordered pair \\(\\left( \\phi^k, B^{i^\\prime}_{\\phantom{i^\\prime} ki} \\right)\\) forms an algebra. But the above relation tells us there is an invertible map \\(\\left( \\phi^k, B^{i^\\prime}_{\\phantom{i^\\prime} ki} \\right) \\mapsto \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\). The map is invertible as there exists an implicit relationship between \\(\\phi^k\\) and \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\). Namely, \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\) is constrained to have the same degrees of freedom as \\(\\phi^k\\).Due to the existence of such a map from an algebra to a Jacobian, the latter is all one needs to construct its vector algebra, which is the reason representation theory works conveniently!Complex numbersLet us begin by applying the ideas covered so far to one of the most important structures in mathematics, that of complex numbers.2-dimensional orthogonal groupComplex numbers are one of the most well-known algebras, in use ever since people tried to solve ‘unsolvable’ polynomial equations, such as \\(i^2 = -1\\). But they are now understood from a more fundamental perspective: group theory. It turns out that the complex numbers are much more than the set \\(\\mathbb{C}\\). Coupled with the notions of addition and multiplication, this set forms the additive and multiplicative group of complex numbers, respectively, \\(\\left( \\mathbb{C}, + \\right)\\) and \\(\\left( \\mathbb{C}, \\times \\right)\\). These two groups are isomorphic to the two-dimensional translation group \\(\\text{T} \\left( 2 \\right)\\) and the orthogonal group \\(\\text{O} \\left( 2 \\right)\\), respectively. The latter itself comprises the group of all scalars and rotations on \\(\\mathbb{R}^2\\), i.e. \\(\\left( \\mathbb{R}, \\times \\right)\\) and \\(\\text{SO} \\left( 2 \\right)\\).The algebra of complex numbers deals with their multiplicative group. Under multiplication, the action of complex numbers is that of scaling and rotating \\(\\mathbb{C}\\). As the name ‘orthogonal group’ suggests, this transformation leaves initially orthogonal elements of \\(\\mathbb{C}\\) orthogonal. Two complex numbers \\(u, v \\in \\mathbb{C}\\) are said to be orthogonal when \\(\\left\\langle u, v \\right\\rangle = \\Re \\left( u^* v \\right) = \\Re \\left( u v^* \\right) = 0\\). Equivalently, for a vector space isomorphic to \\(\\mathbb{C}\\), such as \\(\\mathbb{R}^2\\), orthogonality is defined analogously using the appropriate inner product.However, the elements of \\(\\text{O} \\left( 2 \\right)\\) can be defined in a manner which will be useful for later generalization. Namely, when some \\(\\pmb{\\Lambda} \\in \\text{O} \\left( 2 \\right)\\) is characterized by a \\(\\pmb{\\phi} \\in \\mathbb{R}^2\\), the following quantity is invariant:\\[\\frac{\\left\\langle \\pmb{\\phi}, \\pmb{\\phi} \\right\\rangle}{\\det \\left( \\pmb{\\Lambda} \\right)}\\]Now, we will investigate the action of complex numbers on vectors belonging to \\(\\mathbb{R}^2\\).AlgebraIn the convenient Cartesian coordinate system, an orthogonal Jacobian \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\) is always of the form,\\[\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} = \\begin{pmatrix} a &amp;amp; -b \\\\ b &amp;amp; a \\end{pmatrix}\\]Indeed,\\[\\begin{align}\\frac{\\left\\langle \\left( a, b \\right), \\left( a, b \\right) \\right\\rangle}{\\det \\left( \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\right)} &amp;amp; = \\frac{a^2+b^2}{a^2+b^2} \\\\ &amp;amp; = 1 \\\\ &amp;amp; = \\text{constant}\\end{align}\\]As required, the above Jacobian has only \\(2\\) degrees of freedom, the same as the dimension of the vector space it acts on. Now, the idea is to pack the degrees of freedom \\(\\left( a, b \\right)\\) into a vector with those coordinates. In other words, every vector in the vector space represented by the variable ordered pair \\(\\left( a, b \\right)\\) represents the corresponding Jacobian of the form seen above. Here, ‘representation’ implies a one-to-one map.\\[\\phi^k = \\begin{pmatrix} a \\\\ b \\end{pmatrix}\\]Therefore, our problem becomes that of determining the coefficients \\(B^{i^\\prime}_{\\phantom{i^\\prime} ki}\\). Recall that \\(\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} ki}\\) produces a rotation and scaling described by \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\) that acts on a vector on the right. For this reason, it is necessary to place \\(\\phi^k\\) on the left of \\(B^{i^\\prime}_{\\phantom{i^\\prime} ki}\\) as a row matrix, even though it is a vector (typically represented as column matrices in some basis). This notational inconsistency will not emerge when we repeat this exercise in arbitrary coordinates (using tensors, as expected), but for now, we’ll have to keep this little caveat in mind.In the relation \\(\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} ki} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\) , we get two matrix equations, one for each \\(i^\\prime\\) (corresponding to rows, as \\(i^\\prime\\) is upstairs),\\[\\begin{align}\\begin{pmatrix} a &amp;amp; b \\end{pmatrix} \\begin{pmatrix} B^{0^\\prime}_{\\phantom{0^\\prime}00} &amp;amp; B^{0^\\prime}_{\\phantom{0^\\prime}01} \\\\ B^{0^\\prime}_{\\phantom{0^\\prime}10} &amp;amp; B^{0^\\prime}_{\\phantom{0^\\prime}11} \\end{pmatrix} &amp;amp; = \\begin{pmatrix} a &amp;amp; -b \\end{pmatrix} \\\\\\begin{pmatrix} a &amp;amp; b \\end{pmatrix} \\begin{pmatrix} B^{1^\\prime}_{\\phantom{1^\\prime}00} &amp;amp; B^{1^\\prime}_{\\phantom{1^\\prime}01} \\\\ B^{1^\\prime}_{\\phantom{1^\\prime}10} &amp;amp; B^{1^\\prime}_{\\phantom{1^\\prime}11} \\end{pmatrix} &amp;amp; = \\begin{pmatrix} b &amp;amp; a \\end{pmatrix}\\end{align}\\]The only solution for \\(B^{i^\\prime}_{\\phantom{i^\\prime} ki}\\) which holds for arbitrary \\(\\left( a, b \\right)\\) are immediately found to be,\\[\\begin{align}\\begin{pmatrix} B^{0^\\prime}_{\\phantom{0^\\prime}00} &amp;amp; B^{0^\\prime}_{\\phantom{0^\\prime}01} \\\\ B^{0^\\prime}_{\\phantom{0^\\prime}10} &amp;amp; B^{0^\\prime}_{\\phantom{0^\\prime}11} \\end{pmatrix} &amp;amp; = \\begin{pmatrix} 1 &amp;amp; 0 \\\\ 0 &amp;amp; -1 \\end{pmatrix} \\\\\\begin{pmatrix} B^{1^\\prime}_{\\phantom{1^\\prime}00} &amp;amp; B^{1^\\prime}_{\\phantom{1^\\prime}01} \\\\ B^{1^\\prime}_{\\phantom{1^\\prime}10} &amp;amp; B^{1^\\prime}_{\\phantom{1^\\prime}11} \\end{pmatrix} &amp;amp; = \\begin{pmatrix} 0 &amp;amp; 1 \\\\ 1 &amp;amp; 0 \\end{pmatrix}\\end{align}\\]At this point, you might be thinking: what did we just do? So, it’s a good time to recall what the components of a bilinear product really mean:\\[\\pmb{\\mathcal{B}} \\left( \\pmb{e}_k, \\pmb{e}_i \\right) = B^{i^\\prime}_{\\phantom{i^\\prime} ki} \\pmb{e}_{i^\\prime}\\]Armed with the above knowledge, let’s find the products \\(\\pmb{\\mathcal{B}} \\left( \\pmb{e}_k, \\pmb{e}_i \\right)\\) for all \\(\\left( k, i \\right)\\) from the components \\(B^{i^\\prime}_{\\phantom{i^\\prime} ki}\\) we just found:\\[\\begin{align}\\pmb{\\mathcal{B}} \\left( \\pmb{e}_0, \\pmb{e}_0 \\right) &amp;amp; = B^{0^\\prime}_{\\phantom{0^\\prime}00} \\pmb{e}_0 + B^{1^\\prime}_{\\phantom{1^\\prime}00} \\pmb{e}_1 = \\pmb{e}_0 \\\\\\pmb{\\mathcal{B}} \\left( \\pmb{e}_0, \\pmb{e}_1 \\right) &amp;amp; = B^{0^\\prime}_{\\phantom{0^\\prime}01} \\pmb{e}_0 + B^{1^\\prime}_{\\phantom{1^\\prime}01} \\pmb{e}_1 = \\pmb{e}_1 \\\\\\pmb{\\mathcal{B}} \\left( \\pmb{e}_1, \\pmb{e}_0 \\right) &amp;amp; = B^{0^\\prime}_{\\phantom{0^\\prime}10} \\pmb{e}_0 + B^{1^\\prime}_{\\phantom{1^\\prime}10} \\pmb{e}_1 = \\pmb{e}_1 \\\\\\pmb{\\mathcal{B}} \\left( \\pmb{e}_1, \\pmb{e}_1 \\right) &amp;amp; = B^{0^\\prime}_{\\phantom{0^\\prime}11} \\pmb{e}_0 + B^{1^\\prime}_{\\phantom{1^\\prime}11} \\pmb{e}_1 = - \\pmb{e}_0 \\\\\\end{align}\\]To better see the ‘magic’ here, let us tabulate the above products,\\[\\begin{matrix}\\pmb{\\mathcal{B}} &amp;amp; \\pmb{e}_0 &amp;amp; \\pmb{e}_1 \\\\\\pmb{e}_0 &amp;amp; \\pmb{e}_0 &amp;amp; \\pmb{e}_1 \\\\\\pmb{e}_1 &amp;amp; \\pmb{e}_1 &amp;amp; - \\pmb{e}_0\\end{matrix}\\]Do you recognize the similarity of the above table with the multiplication table for complex numbers? :)\\[\\begin{matrix}\\times &amp;amp; 1 &amp;amp; i \\\\1 &amp;amp; 1 &amp;amp; i \\\\i &amp;amp; i &amp;amp; - 1\\end{matrix}\\]Bingo! We have derived the algebra of complex numbers purely from their underlying geometry, which has to do with the orthogonal group.Let us now look at the 4-dimensional extension of complex numbers: quaternions.Quaternions4-dimensional orthogonal groupLet \\(\\mathbb{H}\\) be the set of quaternions. The multiplicative group of quaternions, \\(\\left( \\mathbb{H}, \\times \\right)\\) is isomorphic to the 4-dimensional orthogonal group, \\(\\text{O} \\left( 4 \\right)\\).Let us generalize the invariant quantity with regard to \\(\\text{O} \\left( 2 \\right)\\). For a member of \\(\\pmb{\\Lambda} \\in \\text{O} \\left( 4 \\right)\\) characterized by some \\(\\pmb{\\phi} \\in \\mathbb{R}^4\\), we have the invariant,\\[\\begin{align}\\frac{\\left\\langle \\pmb{\\phi}, \\pmb{\\phi} \\right\\rangle}{\\det \\left( \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\right)} &amp;amp; = 1 \\\\\\implies \\det \\left( \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\right) &amp;amp; = \\left\\langle \\pmb{\\phi}, \\pmb{\\phi} \\right\\rangle = \\sum_{k=0}^3 \\left( \\phi^k \\right)^2\\end{align}\\]Which \\(4 \\times 4\\) matrix characterized by 4 numbers, say \\(a, b, c, d\\), necessarily has the determinant \\(a^2+b^2+c^2+d^2\\)? Instead of doing trial and error over the rather large space of \\(4 \\times 4\\) matrices, let us cheat and contract the 4 real numbers to 2 complex numbers \\(a+ib, \\: c+id\\). These numbers implicitly contain their \\(2 \\times 2\\) matrices of the form,\\[a + ib \\equiv \\begin{pmatrix} a &amp;amp; -b \\\\ b &amp;amp; a \\end{pmatrix}\\]Now, we can ask, which \\(2 \\times 2\\) matrix characterized by the two complex numbers has a determinant of:\\[a^2 + b^2 + c^2 + d^2 = \\left( a + ib \\right) \\left( a - ib \\right) + \\left( c + id \\right) \\left( c - id \\right)\\]The simplest solution is to form a \\(2 \\times 2\\) matrix with the above factors so that the usual expression for the determinant is the same as the above,\\[\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\equiv \\begin{pmatrix} \\left( a + ib \\right) &amp;amp; - \\left( c+id \\right) \\\\ \\left( c - id \\right) &amp;amp; \\left( a - ib \\right) \\end{pmatrix}\\]Expanding each complex entry in the above matrix as \\(2 \\times 2\\) matrices, we get,\\[\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} = \\begin{pmatrix} a &amp;amp; -b &amp;amp; -c &amp;amp; -d \\\\ b &amp;amp; a &amp;amp; d &amp;amp; -c \\\\ c &amp;amp; d &amp;amp; a &amp;amp; b \\\\ -d &amp;amp; c &amp;amp; -b &amp;amp; a \\end{pmatrix}\\]AlgebraJust as before, we want to characterize the above Jacobian using its degrees of freedom, as,\\[\\phi^k = \\begin{pmatrix} a \\\\ b \\\\ c \\\\ d \\end{pmatrix}\\]Now, we have to find the set of four \\(4 \\times 4\\) matrices, \\(B^{i^\\prime}_{\\phantom{i^\\prime} k i}\\) so that \\(\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} k i} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\),\\[\\begin{align}\\begin{pmatrix} a &amp;amp; b &amp;amp; c &amp;amp; d \\end{pmatrix} \\: \\pmb{B}^{0^\\prime}_{\\phantom{0^\\prime} ki} &amp;amp; = \\begin{pmatrix} a &amp;amp; -b &amp;amp; -c &amp;amp; -d \\end{pmatrix} \\\\\\begin{pmatrix} a &amp;amp; b &amp;amp; c &amp;amp; d \\end{pmatrix} \\: \\pmb{B}^{1^\\prime}_{\\phantom{1^\\prime} ki} &amp;amp; = \\begin{pmatrix} b &amp;amp; a &amp;amp; d &amp;amp; -c \\end{pmatrix} \\\\\\begin{pmatrix} a &amp;amp; b &amp;amp; c &amp;amp; d \\end{pmatrix} \\: \\pmb{B}^{2^\\prime}_{\\phantom{2^\\prime} ki} &amp;amp; = \\begin{pmatrix} c &amp;amp; d &amp;amp; a &amp;amp; b \\end{pmatrix} \\\\\\begin{pmatrix} a &amp;amp; b &amp;amp; c &amp;amp; d \\end{pmatrix} \\: \\pmb{B}^{3^\\prime}_{\\phantom{3^\\prime} ki} &amp;amp; = \\begin{pmatrix} -d &amp;amp; c &amp;amp; -b &amp;amp; a \\end{pmatrix}\\end{align}\\]The appropriate \\(B^{i^\\prime}_{\\phantom{i^\\prime} k i}\\) can be found to be,\\[\\begin{align}B^{0^\\prime}_{\\phantom{0^\\prime} k i} &amp;amp; = \\begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 \\end{pmatrix} \\\\B^{1^\\prime}_{\\phantom{1^\\prime} k i} &amp;amp; = \\begin{pmatrix} 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\ 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 \\\\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\end{pmatrix} \\\\B^{2^\\prime}_{\\phantom{2^\\prime} k i} &amp;amp; = \\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\ 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\end{pmatrix} \\\\B^{3^\\prime}_{\\phantom{3^\\prime} k i} &amp;amp; = \\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\ 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\\\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\ -1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\end{pmatrix}\\end{align}\\]Lastly, let us compute the bilinear products of every pair of basis vectors as \\(\\pmb{\\mathcal{B}} \\left( \\pmb{e}_k, \\pmb{e}_i \\right) = B^{i^\\prime}_{\\phantom{i^\\prime} k i} \\pmb{e}_{i^\\prime}\\) and tabulate the results,\\[\\begin{matrix}\\pmb{\\mathcal{B}} &amp;amp; \\pmb{e}_0 &amp;amp; \\pmb{e}_1 &amp;amp; \\pmb{e}_2 &amp;amp; \\pmb{e}_3 \\\\\\pmb{e}_0 &amp;amp; \\pmb{e}_0 &amp;amp; \\pmb{e}_1 &amp;amp; \\pmb{e}_2 &amp;amp; \\pmb{e}_3 \\\\\\pmb{e}_1 &amp;amp; \\pmb{e}_1 &amp;amp; - \\pmb{e}_0 &amp;amp; \\pmb{e}_3 &amp;amp; - \\pmb{e}_2 \\\\\\pmb{e}_2 &amp;amp; \\pmb{e}_2 &amp;amp; - \\pmb{e}_3 &amp;amp; - \\pmb{e}_0 &amp;amp; \\pmb{e}_1 \\\\\\pmb{e}_3 &amp;amp; \\pmb{e}_3 &amp;amp; \\pmb{e}_2 &amp;amp; - \\pmb{e}_1 &amp;amp; - \\pmb{e}_0\\end{matrix}\\]This corresponds to the Hamilton product table:\\[\\begin{matrix}\\times &amp;amp; 1 &amp;amp; i &amp;amp; j &amp;amp; k \\\\1 &amp;amp; 1 &amp;amp; i &amp;amp; j &amp;amp; k \\\\i &amp;amp; i &amp;amp; - 1 &amp;amp; k &amp;amp; - j \\\\j &amp;amp; j &amp;amp; - k &amp;amp; - 1 &amp;amp; i \\\\k &amp;amp; k &amp;amp; j &amp;amp; - i &amp;amp; - 1\\end{matrix}\\]Gamma matricesDefinitionFrom the relation \\(\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} k i} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\), we see that \\(\\left\\{ B^{i^\\prime}_{\\phantom{i^\\prime} k i} \\right\\}_k\\) forms a basis for \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\) in the space of \\(\\phi^k\\). This basis may be written as a set of dual vectors, which in the Cartesian basis are represented as the gamma matrices:\\[\\pmb{\\gamma}_k = B^{i^\\prime}_{\\phantom{i^\\prime} k i} \\: \\pmb{\\theta}^i \\otimes \\pmb{e}_{i^\\prime}\\]Note that even though the gamma matrices above look like Jacobians, they transform like dual vectors. In the above equation, a gamma matrix (or more appropriately, tensor) has only one free index, downstairs.Complex numbersFor complex numbers, the components of the [dual] gamma matrices are found to be:\\[\\begin{align}\\pmb{\\gamma}_0 &amp;amp; = \\begin{pmatrix} 1 &amp;amp; 0 \\\\ 0 &amp;amp; 1 \\end{pmatrix} \\\\\\pmb{\\gamma}_1 &amp;amp; = \\begin{pmatrix} 0 &amp;amp; -1 \\\\ 1 &amp;amp; 0 \\end{pmatrix}\\end{align}\\]QuaternionsFor quaternions, using the components of \\(B^{i^\\prime}_{\\phantom{i^\\prime} k i}\\) and the definition of the gamma matrices, we find,\\[\\begin{align}\\pmb{\\gamma}_0 &amp;amp; = \\begin{pmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\end{pmatrix} \\\\\\pmb{\\gamma}_1 &amp;amp; = \\begin{pmatrix} 0 &amp;amp; -1 &amp;amp; 0 &amp;amp; 0 \\\\ 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\ 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\end{pmatrix} \\\\\\pmb{\\gamma}_2 &amp;amp; = \\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; -1 &amp;amp; 0 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 \\\\ 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\end{pmatrix} \\\\\\pmb{\\gamma}_3 &amp;amp; = \\begin{pmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; -1 \\\\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\ 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\ -1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\end{pmatrix} \\\\\\end{align}\\]From any parameterized JacobianGiven a Jacobian parameterized by a vector space as \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\left( \\phi^k \\right)\\), we have,\\[\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\left( \\phi^k \\right) = \\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} ki} = \\phi^k \\gamma_{k \\phantom{i^\\prime} i}^{\\phantom{k} i^\\prime}\\]where \\(\\gamma_{k \\phantom{i^\\prime} i}^{\\phantom{k} i^\\prime} = \\pmb{e}_{i^\\prime} \\otimes \\pmb{\\theta}^{i^\\prime} \\left( \\pmb{\\gamma}_k \\right)\\).Suppose we want to find the particular \\(k^\\text{th}\\) gamma matrix instead of summing over \\(k\\). To distinguish such dummy indices, let us enclose them in parentheses as \\(\\left( k \\right)\\). Now,\\[\\phi^{\\left( k \\right)} \\gamma_{\\left( k \\right) \\phantom{i^\\prime} i}^{\\phantom{\\left( k \\right)} i^\\prime} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\left( \\phi^j \\right)\\]such that,\\[\\phi^j = \\begin{cases} \\phi^{\\left( k \\right)} &amp;amp; j=k \\\\ 0 &amp;amp; j \\neq k \\end{cases}\\]i.e., \\(\\phi^j = \\delta^j_{\\phantom{j} \\left( k \\right)} \\phi^{\\left( k \\right)}\\). Now, we have,\\[\\phi^{\\left( k \\right)} \\gamma_{\\left( k \\right) \\phantom{i^\\prime} i}^{\\phantom{\\left( k \\right)} i^\\prime} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\left( \\delta^j_{\\phantom{j} \\left( k \\right)} \\phi^{\\left( k \\right)} \\right)\\]Note that here, \\(\\phi^{\\left( k \\right)}\\) acts as a scalar. Since scaling a tensor scales its components and \\(\\phi^k\\) are constrained to be the components of \\(\\pmb{\\Lambda}\\),\\[\\begin{align}\\phi^{\\left( k \\right)} \\gamma_{\\left( k \\right) \\phantom{i^\\prime} i}^{\\phantom{\\left( k \\right)} i^\\prime} &amp;amp; = \\phi^{\\left( k \\right)} \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\left( \\delta^j_{\\phantom{j} \\left( k \\right)} \\right) \\\\\\gamma_{k \\phantom{i^\\prime} i}^{\\phantom{k} i^\\prime} &amp;amp; = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} \\left( \\delta^j_{\\phantom{j} k} \\right)\\end{align}\\]Towards generalizationThrough the two popular examples of complex numbers and quaternions, we have seen how to deduce the algebra generated by a group, using its representation as a Jacobian of some form.Some problematic aspects of our methods used so far are: The matrix representation for the action of \\(\\text{O} \\left( 4 \\right)\\) on \\(\\mathbb{R}^4\\) was found by contracting its 4 degrees of freedom into 2 independent complex numbers, without justification. Is there a more ‘honest’ method to find the form of \\(\\pmb{\\Lambda} \\left( \\pmb{\\phi} \\right)\\)? Moreover, how can we find \\(\\pmb{\\Lambda}\\) in arbitrary coordinates, as opposed to the convenient Cartesian coordinates? We had to find the components of \\(B^{i^\\prime}_{\\phantom{i^\\prime} k i}\\) by brute force. Is there a more efficient, symbolical method to directly express the whole tensor \\(B^{i^\\prime}_{\\phantom{i^\\prime} k i}\\) in terms of known objects such as the given Jacobian \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\)? The solution for the above problems is briefly to generalize the algebra of complex numbers and quanternions to a family of algebras called Clifford algebras. These can be defined in a coordinate-independent manner motivated by rotations. We shall elaborate on this in the next post, Algebra Done Tensorially: Part 4 (Clifford Algebras)To sum it all up, what we’ve learnt so far is that given a group and its representation via a parameterizing vector space, we can figure out the algebra generated by the elements of the group, which are Jacobians acting on the vector space. This gives us a geometric picture of algebra." }, { "title": "Thomson&#39;s Lamp and Multivaluedness", "url": "/tempus-spatium/thomson-lamp-multivaluedness/", "categories": "analysis", "tags": "supertasks", "date": "2022-02-08 00:00:00 -0500", "snippet": "A SupertaskA supertask is a countably infinite sequence of tasks or events, which occurs in a finite amount of time. The word ‘supertask’ was coined by the twentieth-century philosopher James F. Thomson. He went on to provide an example of a supertask that soon became his namesake philosophical puzzle.The problem may be stated as: we are given a hypothetical lamp with a timer. When the timer is started, the lamp is turned on. After the passage of \\(1\\) minutes, the lamp is turned off. After \\(\\frac{1}{2}\\) minutes, the lamp is turned back off, then again to on after \\(\\frac{1}{4}\\) minutes, and so on.These time intervals can be added as a converging infinite series, which adds up to precisely \\(2\\) minutes:\\[\\begin{align}\\text{Let } S &amp;amp; = \\sum_{n=0}^\\infty \\frac{1}{2^n} \\\\\\therefore \\frac{1}{2} S &amp;amp; = \\frac{1}{2} \\sum_{n=0}^\\infty \\frac{1}{2^n} \\\\ &amp;amp; = \\sum_{n=0}^\\infty \\frac{1}{2} \\cdot \\frac{1}{2^n} \\\\ &amp;amp; = \\sum_{n=0}^\\infty \\frac{1}{2^{n+1}} \\\\ &amp;amp; = \\sum_{n=1}^\\infty \\frac{1}{2^n} \\\\ \\implies S - \\frac{1}{2} S &amp;amp; = \\sum_{n=0}^\\infty \\frac{1}{2^n} - \\sum_{n=1}^\\infty \\frac{1}{2^n} \\\\ \\frac{1}{2} S &amp;amp; = \\frac{1}{2^0} \\\\ \\implies S &amp;amp; = 2\\end{align}\\]The question is, what is the state of the lamp exactly when two minutes have elapsed?Discrete state updateDiagramsAs there are many plots for this post, I thought of showing them together as Desmos embeds below. The two embeds are for the discrete and continuous approaches, respectively:ReparameterizationFirstly, let us restate the problem of Thomson’s lamp in mathematical terms. We are given a discrete system (the lamp) \\(L\\) with a degree of freedom or state, say \\(\\sigma\\). \\(\\sigma\\) has exactly \\(2\\) possible values, corresponding to ‘off’ and ‘on’. Let us label these states as \\(\\sigma = 0\\) and \\(\\sigma=1\\) respectively. Therefore \\(\\sigma \\in \\left\\{ 0, 1 \\right\\}\\).Since \\(L\\) updates its state discretely, we can count its updates using a ‘state parameter’ \\(n\\) which increments with each state update as \\(n=0\\), \\(n=0\\), \\(n=2\\) and so on. For any step \\(n\\), we have a state \\(\\sigma \\left( n \\right)\\). We are given the initial condition \\(\\sigma \\left( 0 \\right) = 1\\).Now, we have a continuously-updating system \\(T\\), namely the timer. It returns a parameter \\(t\\), measured in minutes. At \\(t=0\\), \\(n=0\\). When \\(t\\) becomes \\(1\\), \\(n\\) updates to \\(1\\). After the passage of \\(\\frac{1}{2}\\) minutes, i.e. at \\(t = 1 + \\frac{1}{2}\\), \\(n=2\\). Similarly, at \\(t = 1+ \\frac{1}{2} + \\frac{1}{4}\\), \\(n = 3\\). We see a pattern emerging which can be summarized as:\\[\\left[ t \\left( n \\right) \\right]_{n-1 \\to n} = \\sum_{k=0}^{n} \\frac{1}{2^k}\\]The subscript \\(n-1 \\to n\\) in L.H.S. indicates that precisely when the step parameter updates from a previous value (\\(n-1\\)) to \\(n\\), \\(t\\) will be measured to be its specified value. However, until the next state update, \\(n\\) does not update, whereas \\(t\\) continuously increases as it is a continuously-changing parameter. As a consequence, \\(t \\left( n \\right)\\) is not an injective function, i.e. for a given \\(n\\), there are many \\(t\\). For this reason, it is better to express any parameter in terms of \\(t\\) and not \\(n\\). Let us begin by finding \\(n \\left( t \\right)\\).Firstly, recall the floor function \\(\\left\\lfloor \\: \\right\\rfloor : \\mathbb{R} \\mapsto \\mathbb{Z}\\) which for a real argument \\(x\\) returns the greatest integer less than or equal to \\(x\\). For example, \\(\\left\\lfloor 3.141 \\right\\rfloor = 3\\) and \\(\\left\\lfloor -2.718 \\right\\rfloor = -3\\).Now, we will find \\(n \\left( \\tau \\right)\\) for any \\(t = \\tau\\) such that at that instant, \\(n-1 \\to n\\). Then, we will find \\(n \\left( t \\right)\\) for any \\(t\\) by employing the floor function.\\[\\begin{align}\\tau \\left( n \\right) &amp;amp; = \\sum_{k=0}^{n} \\frac{1}{2^k} \\\\\\frac{1}{2} \\tau \\left( n \\right) &amp;amp; = \\sum_{k=0}^n \\frac{1}{2^{k+1}} = \\sum_{k=1}^{n+1} \\frac{1}{2^k} \\\\\\therefore \\tau \\left( n \\right) - \\frac{1}{2} \\tau \\left( n \\right) &amp;amp; = \\sum_{k=0}^{n} \\frac{1}{2^k} - \\sum_{k=1}^{n+1} \\frac{1}{2^k} \\\\\\frac{1}{2} \\tau \\left( n \\right) &amp;amp; = \\frac{1}{2^0} - \\frac{1}{2^n} = 1 - \\frac{1}{2^n} \\\\\\tau \\left( n \\right) &amp;amp; = 2 - \\frac{1}{2^{n-1}} \\\\\\therefore \\frac{1}{2^{n-1}} &amp;amp; = 2 - \\tau \\left( n \\right) \\\\2^{1-n} &amp;amp; = 2 - \\tau \\left( n \\right) \\\\1-n &amp;amp; = \\log_2 \\left( 2 - \\tau \\left( n \\right) \\right) \\\\n \\left( \\tau \\right) &amp;amp; = 1 - \\log_2 \\left( 2 - \\tau \\right)\\end{align}\\]In the time interval \\(t \\in \\left( \\tau \\left(n \\right), \\tau \\left( n+1 \\right) \\right)\\), \\(n\\) does not update at all. Similarly, \\(\\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor\\) does not update too. The reason is that the mentioned interval excludes the times \\(t = \\tau \\left( n \\right)\\) and \\(t = \\tau \\left( n+1 \\right)\\), and at these instants,\\[\\begin{align}\\left\\lfloor \\log_2 \\left( 2 - \\tau \\left( n \\right) \\right) \\right\\rfloor &amp;amp; = \\left\\lfloor \\log_2 \\left( 2 - \\left( 2 - 2^{1-n} \\right) \\right) \\right\\rfloor \\\\ &amp;amp; = \\left\\lfloor \\log_2 \\left( 2^{1-n} \\right) \\right\\rfloor \\\\ &amp;amp; = \\left\\lfloor 1-n \\right\\rfloor \\\\ &amp;amp; = 1-n \\\\\\left\\lfloor \\log_2 \\left( 2 - \\tau \\left( n+1 \\right) \\right) \\right\\rfloor &amp;amp; = \\left\\lfloor \\log_2 \\left( 2 - \\left( 2 - 2^{1-n-1} \\right) \\right) \\right\\rfloor \\\\ &amp;amp; = \\left\\lfloor \\log_2 \\left( 2^{-n} \\right) \\right\\rfloor \\\\ &amp;amp; = \\left\\lfloor -n \\right\\rfloor \\\\ &amp;amp; = -n\\end{align}\\]As no integer lies between \\(-n\\) and \\(1-n\\), \\(\\left\\lfloor \\log_2 \\left( 2 - \\tau \\right) \\right\\rfloor\\) does not update in the open interval \\(\\left( n, n+1 \\right)\\). Due to the way the floor function is defined, the function \\(\\left( 1-\\left\\lfloor \\log_2 \\left( 2 - \\tau \\right) \\right\\rfloor \\right)\\) remains at the same value as at \\(t = \\tau \\left( n \\right)\\), namely \\(\\left( 1 - \\log_2 \\left( 2 - \\tau \\right) \\right)\\). At \\(t = \\tau \\left( n+1 \\right)\\), however, \\(\\left( 1-\\left\\lfloor \\log_2 \\left( 2 - \\tau \\right) \\right\\rfloor \\right)\\) updates to \\(\\left( 1- \\log_2 \\left( 2 - \\tau \\left( n+1 \\right) \\right) \\right) = n \\left( \\tau \\right) + 1 = n \\left( t \\right)\\). Thus, we find that in general,\\[n \\left( t \\right) = 1-\\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor\\]Thus, we have parameterized the step parameter in terms of \\(t\\). Now let us parameterize the state function \\(\\sigma\\) similarly.State functionWhen Thomson’s lamp is first turned on along with the timer, \\(n = 0\\) and \\(\\sigma = 1\\). In the next state update, \\(n=1\\) and the lamp is switched off, giving \\(\\sigma = 0\\). We notice that \\(\\sigma \\left( n \\right)\\) keeps alternating between \\(1\\) and \\(0\\). Such a function can be written as a modulo operation,\\[\\sigma \\left( n \\right) = \\text{mod} \\left( n+1, 2 \\right)\\]Where \\(\\text{mod} \\left( a, b \\right)\\) is defined as the remainder obtained on long division of \\(a\\) by \\(b\\). In conjunction with Donald Knuth’s definition of the modulo operation, since we have the strict relation \\(a = b \\left\\lfloor \\frac{a}{b} \\right\\rfloor + \\text{mod} \\left( a, b \\right)\\), we can extend this relation implicitly into the reals and define the modulo operation as,\\[\\text{mod} \\left( a, b \\right) = a - b \\left\\lfloor \\frac{a}{b} \\right\\rfloor\\]Therefore, we have,\\[\\sigma \\left( n \\right) = n + 1 - 2 \\left\\lfloor \\frac{n+1}{2} \\right\\rfloor\\]Substituting \\(n = n \\left( t \\right) = 1-\\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor\\) will now give us \\(\\sigma \\left( t \\right)\\) explicitly:\\[\\begin{align}\\sigma \\left( t \\right) &amp;amp; = 1-\\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor + 1 - 2 \\left\\lfloor \\frac{1-\\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor + 1}{2} \\right\\rfloor \\\\\\sigma \\left( t \\right) &amp;amp; = 2 - \\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor - 2 \\left\\lfloor \\frac{2-\\left\\lfloor \\log_2 \\left( 2 - t \\right) \\right\\rfloor}{2} \\right\\rfloor\\end{align}\\]Not the prettiest-looking function, but it does serve our purpose, judging by its plot in the first diagram.At long last, we can state the problem of Thomson’s lamp in terms of mathematical elements: what is the value of \\(\\sigma \\left( 2 \\right)\\)? The expression \\(\\log_2 \\left( 2 - t \\right)\\) appears frequently in the formula for \\(\\sigma \\left( t \\right)\\), and at \\(t=2\\), it is undefined. Therefore, it does not make sense to ask what \\(\\sigma \\left( 2 \\right)\\) is!A ‘closer’ lookEven though \\(\\sigma \\left( 2 \\right)\\) is undefined, we may still want to ask what happens to \\(\\sigma \\left( n \\right)\\) as \\(n \\to \\infty\\). As for every \\(\\sigma \\left( n \\right)\\) there is a \\(\\sigma \\left( n+1 \\right)\\), \\(\\sigma\\) must update countably infinite times up to \\(t = 2\\). This, in fact, raises a contradiction. In James Thomas’ own words, It seems impossible to answer this question. It cannot be on, because I did not ever turn it on without at once turning it off. It cannot be off, because I did in the first place turn it on, and thereafter I never turned it off without at once turning it on. But the lamp must be either on or off. This is a contradiction.Here’s how I like to interpret the above result: at \\(t=2\\), \\(\\sigma \\left( 2 \\right)\\) breaks down. Since we are fitting a countably infinite number of state updates right up to \\(t=2\\), at \\(t=2\\), the Thomson’s lamp updates at an infinite rate, manifested in \\(\\log_2 \\left( 2-t \\right)\\) shooting to \\(- \\infty\\). But if infinite update events are squeezed into the same instant \\(t=2\\), it no longer makes sense to ask what particular value \\(\\sigma\\) has, because it is multivalued! In order to be able to ask what value \\(\\sigma\\) has at a given time, \\(\\sigma\\) must return one and only one value at that instant, i.e. \\(\\sigma \\left( t \\right)\\) must be injective.But this isn’t where the story ends. To demonstrate the infinite rate of state update at \\(t=2\\) a bit more rigorously, we need to find the derivative of \\(\\sigma \\left( t \\right)\\) and its limit at \\(t=2\\). However, the expression for \\(\\sigma \\left( t \\right)\\) makes things rather messy. Finding its discrete derivative is no easy task; on the other hand, differentiating it is impossible as it is not a continuous function for \\(t \\in \\mathbb{W}\\).A workaround solution is to slightly broaden the problem, but keep the nature of the singularity at \\(t=2\\) the same, so that both cases essentially involve the same mechanics. Namely, we will upgrade \\(\\sigma \\left( t \\right)\\) to a continuous parameter \\(\\sigma:\\mathbb{R} \\mapsto \\left[ 0,1 \\right]\\), but for \\(t \\in \\mathbb{W}\\), \\(\\sigma \\left( t \\right) \\in \\left\\{ 0, 1 \\right\\}\\). In other words, we will replace \\(\\sigma \\left( t \\right)\\) with a smooth and possibly, differentiable, function which passes through the same points as the original wherever state update is involved in the latter. Therefore, the nature of asking what \\(\\sigma \\left( 2 \\right)\\) is, remains the same as \\(t=2\\) is common to both the domains.Real analytic solutionA smoother pathWe will ‘smoothen’ \\(\\sigma \\left( t \\right)\\) in a number of steps. These steps arise as a consequence of \\(\\sigma\\) not remaining static between state updates, rather transitioning smoothly between the states fixed by the discrete evolution: We will eliminate the floor function from \\(n \\left( t \\right)\\) by the above reasoning. Therefore,\\[n \\left( t \\right) = 1 - \\log_2 \\left( 2 - t \\right)\\] Since \\(\\sigma \\left( n \\right)\\) uses the \\(\\text{mod} \\left( \\cdot, 2 \\right)\\) function, we will smoothen the latter first. As we will only be concerned with retaining the values concerned with integral preimages of the function, we simply need to find a smooth function which satisfies:\\[\\begin{align}\\text{mod} \\left( 0, 2 \\right) &amp;amp; = 0 \\\\\\text{mod} \\left( 1, 2 \\right) &amp;amp; = 1 \\\\\\text{mod} \\left( 2, 2 \\right) &amp;amp; = 0 \\\\\\vdots\\end{align}\\]A suitable candidate is a sinusoidal function which oscillates between \\(0\\) and \\(1\\) with the initial value \\(0\\) at \\(t=0\\). The advantage of using a sinusoidal function is that it is differentiable. A little bit of playing around shows that under this approach, the simplest possible choice is,\\[\\text{mod} \\left( x, 2 \\right) = \\frac{1}{2} \\left( 1 - \\cos \\left( \\pi x \\right) \\right)\\]Substituting the above into the equation \\(\\sigma \\left( n \\right) = \\text{mod} \\left( n+1, 2 \\right)\\), we get,\\[\\sigma \\left( t \\right) = \\frac{1}{2} \\left[ 1 + \\cos \\left( \\pi \\left( 1 - \\log_2 \\left( 2 - t \\right) \\right) \\right) \\right]\\]Rate of updateThe rate of continuous update is simply its derivative. Using the standard rules of differentiation, we obtain the rate of update as,\\[\\dot{\\sigma} \\left( t \\right) = - \\frac{\\pi}{2 \\ln \\left( 2 \\right)} \\frac{1}{2-t} \\sin \\left( \\pi \\left( 1 - \\log_2 \\left( 2-t \\right) \\right) \\right)\\]Since \\(\\cos : \\mathbb{R} \\mapsto \\left[ -1, 1 \\right]\\), we may say that,\\[\\displaystyle{ \\lim_{t \\to 2} \\sin \\left( \\pi \\left( 1 - \\log_2 \\left( 2-t \\right) \\right) \\right) }\\]is undefined, but not diverging. On the other hand, \\(\\displaystyle{ \\lim_{t \\to 2} \\frac{1}{2-t} }\\) is diverging. Therefore, \\(\\displaystyle{ \\lim_{t \\to 2} \\dot{\\sigma} \\left( 2 \\right) }\\) is diverging. From this, we can justify the intuition that at \\(t=2\\), \\(\\sigma \\left( t \\right)\\) is a vertical line with respect to the time-axis, thereby exhibiting multivaluedness.ConclusionNow that we have solved the problem analytically, it is important to know why we used mathematics here at all. Firstly, we didn’t have to use mathematics to investigate this puzzle. It is a logical puzzle, and the role of mathematics here is to simply construct an analysis using tools we already know to be logically consistent.However, the approach of employing mathematical machinery is enlightening in that the logical problem translates into an analytical one, involving how functions behave and when they break down. Since the calculus of how functions change is a familiar language today, it could be a helpful exercise to express our logical reasoning in this language. It potentially gives us insight into both the mathematics and the logic." }, { "title": "A Brief Geometric Analysis of Harmonic Oscillators", "url": "/tempus-spatium/geometric-analysis-harmonic-oscillators/", "categories": "classical mechanics", "tags": "harmonic oscillators, phase space", "date": "2022-01-21 00:00:00 -0500", "snippet": "Phase spaceConsider a one-dimensional harmonic oscillator evolving according to the equation of motion:\\[m \\ddot{x} + kx = 0\\]It is possible to investigate the essential properties of \\(x \\left( t \\right)\\) using a geometric intuition solely by playing with the equation of motion, rather than directly finding the general solution analytically.Firstly, for future ease, we define a parameter \\(\\omega = \\sqrt{\\frac{k}{m}}\\). Performing this substitution in the equation of motion and dividing both sides by \\(\\sqrt{mk}\\), we find,\\[\\omega^{-1} \\ddot{x} + \\omega x = 0\\]Now, we define a new coordinate \\(y\\) such that,\\[y = \\omega^{-1} \\dot{x}\\]Substituting \\(\\ddot{x} = \\omega \\dot{y}\\) in the equation of motion, we have,\\[x = - \\omega^{-1} \\dot{y}\\]The space characterized by \\(\\left( x, y \\right)\\) is known as phase space. We imagine a system to trace a trajectory in its phase space through its evolution. The components of the velocity vector concerned are precisely given by the two above equations. Written in vector form,\\[\\begin{pmatrix} \\dot{x} \\\\ \\dot{y} \\end{pmatrix} = \\omega \\begin{pmatrix} y \\\\ - x \\end{pmatrix}\\]Therefore, we have transformed the original equation of motion, a one-dimensional second-order differential equation to a two-dimensional first-order differential equation.Velocity vector fieldThe state of a system at any instant is given by the position vector:\\[\\pmb{r} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\]The interesting thing about the vector equation of motion is that the velocity vector \\(\\dot{\\pmb{r}}\\) obtained from it is always perpendicular to the position vector \\(\\pmb{r}\\),\\[\\begin{aligned}\\dot{\\pmb{r}} \\cdot \\pmb{r} &amp;amp; = \\dot{x} x + \\dot{y} y \\\\ &amp;amp; = \\omega y \\: x - \\omega x \\: y \\\\ &amp;amp; = 0\\end{aligned}\\]The second important feature is that for constant \\(r = \\lvert \\pmb{r} \\rvert\\), the magnitude of velocity \\(u = \\lvert \\dot{\\pmb{r}} \\rvert\\) is also constant:\\[\\begin{aligned}u &amp;amp; = \\sqrt{\\dot{x}^2 + \\dot{y}^2} \\\\ &amp;amp; = \\sqrt{\\omega^2 y^2 + \\omega^2 x^2} \\\\ &amp;amp; = \\sqrt{\\omega^2 \\left( x^2 + y^2 \\right)} \\\\ &amp;amp; = \\sqrt{\\omega^2 r^2} \\\\ &amp;amp; = \\omega r\\end{aligned}\\]The only motion where the velocity vector is perpendicular to the position vector is that of circular motion about the origin. Furthermore, since \\(u\\) is constant for a given radius \\(r\\), the speed of revolution about the origin is also constant along a given trajectory.Therefore, all possible trajectories of harmonic oscillators in phase space form concentric circles. The velocity vectors are tangential to these circles, and together for all circles, form a velocity vector field resembling the below:Velocity vector fieldFor readers who might be interested, the above plot can be generated in SageMathCell using the Sage code below:x, y = var(&#39;x y&#39;)plot = plot_vector_field((y, -x), (x,-3,3), (y,-3,3))plot.show(aspect_ratio=1)(Here, we have used \\(\\omega = 1\\) for simplicity of the code).SolutionMerry-go-roundDue to the circular symmetry of the velocity vector field, we can reduce the two-dimensional equation of motion to a one-dimensional problem, while still keeping it first-order.To do so, we will use the \\(r\\) coordinate used earlier, \\(r = \\sqrt{x^2 + y^2}\\) and define a new coordinate \\(\\theta\\) such that,\\[\\begin{aligned}x &amp;amp; = r \\cos \\theta \\\\y &amp;amp; = r \\sin \\theta\\end{aligned}\\]Before proceeding to write the vector equation of motion in terms of these new variables, let us prove that for a given trajectory, \\(\\dot{r} = 0\\). This will reinforce the idea that a given \\(r\\) characterizes a single trajectory (since it remains constant throughout).Recall that \\(u = \\lvert \\dot{\\pmb{r}} \\rvert\\) is given by \\(u = \\omega r\\). Therefore,\\[\\dot{r} = \\omega^{-1} \\dot{u}\\]Now,\\[\\begin{aligned}\\frac{d}{dt} \\left( \\pmb{r} \\cdot \\pmb{r} \\right) &amp;amp; = 2 \\dot{\\pmb{r}} \\cdot \\pmb{r} \\\\\\frac{d}{dt} \\left( r^2 \\right) &amp;amp; = 2 \\dot{\\pmb{r}} \\cdot \\pmb{r}\\end{aligned}\\]But we know that \\(\\dot{\\pmb{r}} \\cdot \\pmb{r} = 0\\). So,\\[\\begin{aligned}\\frac{d}{dt} \\left( r^2 \\right) &amp;amp; = 0 \\\\2 r \\dot{r} = 0\\end{aligned}\\]Since \\(r\\) is arbitrary, \\(\\dot{r} = 0\\).In the phase space characterized by \\(\\left( r, \\theta \\right)\\), this is an equation of motion, i.e. \\(\\dot{r} = 0\\). It tells us that for a given trajectory, \\(r\\) is constant, which justifies the claim that each trajectory is in turn described by some \\(r\\).But \\(\\dot{r} = 0\\) doesn’t tell us much as an equation of motion. We are yet to compute \\(\\dot{\\theta}\\), which will give us a second equation of motion, and with more information about the evolution of the harmonic oscillator. This is why we said that we are reducing the two-dimensional problem to a one-dimensional one.Equations of motionEquipped with the knowledge that \\(\\dot{r} = 0\\), let us recall the original vector equation of motion and use the \\(\\theta\\) coordinate defined earlier:\\[\\begin{aligned}\\frac{d}{dt} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\omega \\begin{pmatrix} y \\\\ - x \\end{pmatrix} \\\\\\frac{d}{dt} \\begin{pmatrix} r \\cos \\theta \\\\ r \\sin \\theta \\end{pmatrix} = \\omega \\begin{pmatrix} r \\sin \\theta \\\\ - r \\cos \\theta \\end{pmatrix}\\end{aligned}\\]Since \\(\\dot{r} = 0\\), we can bring out \\(r\\) from the derivative and cancel it out on both sides,\\[\\begin{aligned}\\frac{d}{dt} \\begin{pmatrix} \\cos \\theta \\\\ \\sin \\theta \\end{pmatrix} = \\omega \\begin{pmatrix} \\sin \\theta \\\\ - \\cos \\theta \\end{pmatrix} \\\\\\dot{\\theta} \\begin{pmatrix} - \\sin \\theta \\\\ \\cos \\theta \\end{pmatrix} = \\omega \\begin{pmatrix} \\sin \\theta \\\\ - \\cos \\theta \\end{pmatrix} \\\\\\dot{\\theta} \\begin{pmatrix} - 1 \\\\ 1 \\end{pmatrix} = \\omega \\begin{pmatrix} 1 \\\\ - 1 \\end{pmatrix} \\\\\\begin{pmatrix} - \\dot{\\theta} \\\\ \\dot{\\theta} \\end{pmatrix} = \\begin{pmatrix} \\omega \\\\ - \\omega \\end{pmatrix}\\end{aligned}\\]The two equations above are really telling the same thing: \\(\\dot{\\theta} = - \\omega\\). The solution of this equation of motion is trivial:\\[\\theta \\left( t \\right) = \\theta_0 - \\omega t\\]where \\(\\theta_0\\) is a constant. Likewise,\\[r \\left( t \\right) = r_0\\]Thus, a given trajectory of radius \\(r_0\\) resembles circular motion about the origin with frequency \\(\\frac{\\omega}{2 \\pi}\\). Since each point in the phase space encodes the state of the system, this means that the harmonic oscillator itself displaces in this periodic fashion. The original \\(x\\) coordinate encodes this displacement, so \\(x \\left( t \\right)\\) must be some periodic function. Furthermore, as \\(x\\) is a projection of \\(r\\) onto the \\(x\\) axis in phase space, \\(x \\left( t \\right)\\) must be a projection of circular motion, i.e. sinusoidal.ConclusionAfter all our substitutions and tricks, what we have learnt is that \\(x \\left( t \\right)\\) is sinusoidal for a harmonic oscillator. We did so without explicitly finding \\(x \\left( t \\right)\\). Instead, we tracked the evolution of the system in phase space, and solved for more convenient coordinates \\(r \\left( t \\right)\\) and \\(\\theta \\left( t \\right)\\)." }, { "title": "Vector Subtraction Is More Fundamental Than Addition", "url": "/tempus-spatium/vector-subtraction-more-fundamental-than-addition/", "categories": "geometry", "tags": "affine space", "date": "2022-01-05 00:00:00 -0500", "snippet": "When we first learn mathematics in school, we are taught the four fundamental operations of arithmetic: addition, subtraction, multiplication and division. It is a common practice to teach addition first. After all, we’re adding things all the time in real life: apples to apples, oranges to oranges and so on. Only then are we taught that the reverse operation is necessary to model the removal of apples from apples: subtraction. 1As a result, the idea that addition is perhaps more fundamental than subtraction is induced in us (it did, at least for me). But it turns out that geometrically speaking, subtraction is a more fundamental concept than addition! Let’s see how.Euclidean spacesEuclidean spaces are the most basic geometric spaces used in classical geometry. Without going into generalized dot products (i.e. inner products), a Euclidean n-space is a vector space isomorphic to \\(\\mathbb{R}^n\\), equipped with a symmetric function called the distance function \\(d : \\mathbb{R}^n \\mapsto \\mathbb{R}\\) defined according to Pythagoras’ theorem in \\(n\\) dimensions. For \\(\\pmb{u}, \\pmb{v} \\in \\mathbb{R}^n\\) in Cartesian coordinates,\\[d \\left( \\pmb{u}, \\pmb{v} \\right) = \\sqrt{ \\sum_{i=1}^n \\left( v_i - u_i \\right)^2}\\]Every Euclidean space has an origin \\(\\pmb{0}\\), represented by the coordinates \\(\\left( 0, 0, \\overset{n \\text{ times}}{\\dots}, 0 \\right)\\). Then, the norm of a vector in Euclidean space is defined as \\(\\lvert \\pmb{u} \\rvert = d \\left( \\pmb{u}, \\pmb{0} \\right) = d \\left( \\pmb{0}, \\pmb{u} \\right)\\). I.e., in Cartesian coordinates,\\[\\lvert \\pmb{u} \\rvert = \\sqrt{\\sum_{i=1}^n u_i^2}\\]Furthermore, a Euclidean space is equipped with a symmetric bilinear form, the dot product \\(\\cdot : \\mathbb{R^n} \\times \\mathbb{R}^n \\mapsto \\mathbb{R}\\), defined using the polarization identity as,\\[\\pmb{u} \\cdot \\pmb{v} = \\frac{1}{2} \\left( \\lvert \\pmb{u} + \\pmb{v} \\rvert^2 - \\lvert \\pmb{u} \\rvert^2 - \\lvert \\pmb{v} \\rvert^2 \\right)\\]In Cartesian coordinates,\\[\\pmb{u} \\cdot \\pmb{v} = \\sum_{i=1}^n u_i v_i\\]A ‘shift’ in perspectiveAn example of a Euclidean space used for over 2000 years is the geocentric picture of the Cosmos, which treats the earth as the origin of space. Copernicus’ heliocentric model shifted the origin to the sun. Finally, Newton made the ‘origin’ of the universe indefinite, but he did believe it to exist somewhere out there. All these beliefs emerge from the assumption that space is an absolute structure underlying objects. However, this is incorrect. And this was realized by Galileo Galilei, who lived between the lifespans of Copernicus and Newton.Galileo realized that the space we live in is not endowed with an absolute ‘origin’ whose location is irrefutable. For instance, the laws of science, as he realized, are the same in all inertial frames, and different inertial frames have different origins. He exposes his realization beautifully in a thought experiment narrated by Salviati, his alter ego in Dialogue Concerning the Two Chief World Systems.Therefore, we realize that physical space does not distinguish between coordinate systems centred at different origins. Or, the parameters living in physical space are invariant under shifting the origin of the concerned reference frame.This idea not only has important implications in physics, but also in mathematics. The purpose of mathematics is to study structures which are internally consistent. Since physical space forms such a consistent structure, it is the job of mathematics to build on it, using a formal approach. This is why the idea of affine spaces becomes important.When addition is removedGalileo’s enlightenment may be summarized as physical space simply not having a unique notion of addition. This is motivated by the idea that adding a vector to an entire vector space corresponds to translating the vector space along the vector. This, in turn, implies shifting the origin, which Galileo found to do nothing to physical laws.Therefore, physical space is a structure that remains invariant under translations. More generally, we consider the idea of an affine n-space, which is a vector space \\(\\mathbb{A}^n\\) that remains invariant under the action of the additive group of \\(\\mathbb{R}^n\\), \\(\\left( \\mathbb{R}^n, + \\right)\\). Thus, adding vectors in an affine space makes no sense — we can repeatedly add arbitrary vectors without changing the meaning of the original vector.Stated differently, in \\(\\mathbb{A}^n\\), one can identify all points, \\(\\pmb{u} \\leftrightarrow \\pmb{u} + \\pmb{v} : \\pmb{u}, \\pmb{v} \\in \\mathbb{A}^n\\). Here, ‘identification’ is a kind of equivalence relation on the original, Euclidean space \\(\\mathbb{R}^n\\).Some immediate consequences of there being no notion of addition in \\(\\mathbb{A}^n\\) are: Suppose for a given vector \\(\\pmb{u} \\in \\mathbb{A}^n\\), one finds a vector \\(- \\pmb{u}\\) such that \\(\\pmb{u} + \\left( - \\pmb{u} \\right) = \\pmb{0}\\). Since we can add any vector \\(\\pmb{v}\\) to this result without changing its meaning, there cannot be a unique \\(\\pmb{0}\\), i.e. origin. Likewise, \\(- \\pmb{u}\\) is not defined. The norm of a vector is not defined for similar reasons: \\(\\lvert \\pmb{u} \\rvert \\leftrightarrow \\lvert \\pmb{u} + \\pmb{v} \\rvert\\). Even the dot product of two vectors is not defined, as each vector can be added to a fixed third vector \\(\\pmb{v} \\in \\mathbb{R}^n\\), so that, \\[\\begin{align}\\pmb{u} \\cdot \\pmb{v} &amp;amp; \\leftrightarrow \\left( \\pmb{u} + \\pmb{w} \\right) \\cdot \\left( \\pmb{v} + \\pmb{w} \\right) \\\\ &amp;amp; = \\pmb{u} \\cdot \\pmb{v} + \\left( \\pmb{u} + \\pmb{v} \\right) \\cdot \\pmb{w} + \\pmb{w} \\cdot \\pmb{w}\\end{align}\\]The above statements quantitatively state and generalize Galileo’s enlightening intuition.The sole survivorDespite affine spaces rejecting most important properties of Euclidean spaces, one important notion survives: that of subtraction (and consequently, a distance function). This is because when an element of \\(\\left( \\mathbb{R}^n, + \\right)\\) acts on \\(\\mathbb{A}^n\\), the same element is added to all vectors in the affine space \\(\\mathbb{A}^n\\). Therefore, subtracting any vector from another in \\(\\mathbb{A}^n\\) cancels out the factor added via the additive group \\(\\left( \\mathbb{R}^n, + \\right)\\). I.e. for \\(\\pmb{u}, \\pmb{v} \\in \\mathbb{A}^n\\) and \\(\\pmb{w} \\in \\mathbb{R}^n\\),\\[\\begin{align}\\pmb{v} - \\pmb{u} &amp;amp; \\leftrightarrow \\left( \\pmb{v} + \\pmb{w} \\right) - \\left( \\pmb{u} + \\pmb{w} \\right) \\\\ &amp;amp; = \\pmb{v} - \\pmb{u} + \\pmb{w} - \\pmb{w} \\\\ &amp;amp; = \\pmb{v} - \\pmb{u}\\end{align}\\]The key here is that since \\(\\pmb{w} \\in \\mathbb{R}^n\\) and not \\(\\mathbb{A}^n\\), \\(\\pmb{w} - \\pmb{w} = \\pmb{0}\\) is unique. As a result, the \\(-\\) operation in \\(\\mathbb{A}^n\\) is defined!A refinementWe’ve so far learnt that subtraction is immune to the generality introduced by choosing affine spaces over Euclidean spaces. However, in the process, an idea has been used informally without making it more rigorous for application to affine spaces: that of subtraction itself!If you read this post till here, you might’ve asked: if there isn’t a unique notion of \\(- \\pmb{u}\\) given some \\(\\pmb{u} \\in \\mathbb{A}^n\\), what does the expression \\(\\pmb{v} - \\pmb{u}\\) mean? Well, it doesn’t mean adding \\(- \\pmb{u}\\) to \\(\\pmb{v}\\) anymore! Rather, \\(-\\) is an operator on its own, \\(- : \\mathbb{A}^n \\times \\mathbb{A}^n \\mapsto \\mathbb{R}^n\\), defined as,\\[\\left( \\pmb{v} - \\pmb{u} \\right)_i = v_i - u_i\\]It must be made clear that the subtraction operator appearing on the right hand side is scalar subtraction, while the one on the left is vector addition. The difference between the two is that for a scalar, \\(-k = \\left( -1 \\right) k\\), but for a vector in an affine space, \\(- \\pmb{u} \\neq \\left( -1 \\right) \\pmb{u}\\). That’s because in an affine space, since adding vectors doesn’t yield a unique value, neither does repeatedly adding a vector to itself (which is nothing but scalar multiplication of the vector).ConclusionThe previous paragraph may give rise to a further question: why must we restrict ourselves to a scalar version of subtraction which works as \\(-k = \\left( -1 \\right) k\\)? If we don’t, the components of vectors in affine space are no longer elements of \\(\\mathbb{R}\\), but \\(\\mathbb{A}\\). But there is no unique bilinear map \\(\\mathbb{R}^n \\mapsto \\mathbb{A}\\). As a result, we cannot say, for instance, that the first component of a vector in \\(\\mathbb{R}^n\\) with coordinates \\(\\left( x, y, z \\right)\\) is indeed \\(x\\) and not \\(x+h\\) for some arbitrary \\(h\\). As a consequence, the following fact about affine spaces, expressed in Cartesian coordinates, is destroyed,\\[\\pmb{u} = \\sum_{i=1}^n u_i \\pmb{e}_i\\]where \\(\\pmb{e}_i\\) is the tuple with the entry \\(1\\) in the \\(i^{\\text{th}}\\) place, and \\(0\\) elsewhere.With the above identity being destroyed, the idea of choosing basis vectors to represent any vector is also destroyed. Or, the information about a vector is no longer conserved between different coordinate systems! But information must be a coordinate-independent entity, as coordinates only reflect a particular perspective.All-in-all, we have a neat picture of subtraction: it is more fundamental than addition, upto vectors. By ‘fundamental’, we mean ‘well-defined in a large number of scenarios’.So, that’s it, folks! Thanks a lot for reading up to here! If you like reading my posts, do leave a thumbs up in the comments section, it greatly motivates me to write more posts like these! :) Although not directly related, pedagogy in computer science works similarly. We’re first taught to add elements to data structures such as arrays. The idea of popping (i.e. removing) elements from structures is not only taught later, but also programatically more intricate. For example, popping elements often requires functions, whereas concatenating lists can almost universally be performed using the inbuilt + operator. &amp;#8617; " }, { "title": "Scalar Field Lagrangian From Symmetry Considerations", "url": "/tempus-spatium/scalar-field-lagrangian-symmetry-considerations/", "categories": "classical field theory", "tags": "Klein-Gordon theory, energy-momentum tensor, Noether theorem, symmetries", "date": "2021-12-01 00:00:00 -0500", "snippet": "Welcome to another post on, well, you guessed it: scalar field theory. Today, we will be deriving a Lagrangian density for scalar fields that appears almost everywhere in physics (namely in the Klein-Gordon and related theories).Common arguments for assuming the form of the Lagrangian in question either take motivation from too specific systems such as chains (which are promoted to scalar fields using a procedure that does not work for higher-rank field theories), or extremely formal analogies with the point-particle Lagrangian. I will, instead, use an approach I find easy to remember.Since the method has to do with the energy-momentum tensor for scalar fields and what it physically means, most of this post elaborates on the same. However, if you are aware of the logic and properties of the energy-momentum tensor, feel free to skip to the end.Canonical 4-momentum fieldRecall the special relativistic Euler-Lagrange equation for a scalar field \\(\\phi\\) with Lagrangian \\(\\mathcal{L}\\),\\[\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = \\partial_\\mu \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)}\\]It is perfectly permissible to work with the above equation in inertial coordinates, i.e. coordinates where the Christoffel symbols vanish. As a result, we can, in such coordinates, treat \\(\\partial_\\mu\\) like a tensor, and hence, the adjacent term too,\\[\\begin{align}\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_{\\mu^\\prime} \\phi \\right)} &amp;amp; = \\frac{\\partial \\left( \\partial_{\\mu} \\phi \\right)}{\\partial \\left( \\partial_{\\mu^\\prime} \\phi \\right)} \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_{\\mu} \\phi \\right)} \\\\ &amp;amp; = \\frac{\\partial}{\\partial \\left( \\partial_{\\mu^\\prime} \\phi \\right)} \\left( \\Lambda^{\\mu^\\prime}_{\\phantom{\\mu^\\prime} \\mu} \\: \\partial_{\\mu^\\prime} \\phi \\right) \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_{\\mu} \\phi \\right)} \\\\ &amp;amp; = \\Lambda^{\\mu^\\prime}_{\\phantom{\\mu^\\prime} \\mu} \\: \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_{\\mu} \\phi \\right)}\\end{align}\\]Indeed, \\(\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_{\\mu} \\phi \\right)}\\) transforms like a vector, as second-order terms vanish in inertial coordinates. However, it’s not hard to promote the quantity to a tensor in arbitrary coordinates: we simply replace partial derivatives \\(\\partial_\\mu\\) with covariant derivatives \\(\\nabla_\\mu\\), so that all second-order terms involving the Christoffel symbols cancel out. However, let us stick to inertial coordinates as it’s much simpler to manipulate expressions in them and later switch to coordinate-independent quantities.The vectorial quantity we just found will appear repeatedly in this post, so let us simply call it the canonical 4-momentum field \\(\\pi^\\mu\\) in lieu of conjugate momentum in the Lagrangian mechanics of point particles 1 .Continuity equationThe Euler-Lagrange equations tell the following story: the dependence of the dynamics of a scalar field on the field, \\(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}\\), is linearly related to the divergence of the canonical 4-momentum field \\(\\pi^\\mu\\),\\[\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = \\partial_\\mu \\pi^\\mu\\]If the dynamics of a system are invariant under changes in \\(\\phi\\), \\(\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = 0\\) and hence \\(\\partial_\\mu \\pi^\\mu = 0\\). If we expand the last equation using the mostly-plus convention for the metric and negate both sides,\\[\\partial_0 \\pi^0 + \\partial_i \\pi^i = 0\\]This is the continuity equation. \\(\\pi^0\\) can be thought of as the local energy density, and \\(\\pi^i\\) the corresponding flux. By the divergence theorem, \\(\\partial_0 \\displaystyle{ \\int d^3 x \\: \\pi^0 = 0 }\\) as \\(\\displaystyle{ \\int d^3 x \\left( \\pmb{\\nabla} \\cdot \\pmb{\\pi} \\right) }\\) is a constant surface integral. Hence, the total energy \\(\\displaystyle{ \\int d^3 x \\: \\pi^0 = 0 }\\) is conserved.However, when \\(\\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\neq 0\\), the above is no more true. What, then, is conserved in a general scalar field theory?Noether currentIt might feel strange at first that when \\(\\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\neq 0\\), the energy \\(\\displaystyle{ \\int d^3 x \\: \\pi^0 }\\) is not conserved. But from the point of view of Noether’s theorem, that makes sense. Say the dynamics (i.e. Lagrangian) of a field are not symmetric with respect to the field. Since the field may explicitly depend on the time coordinate, energy is not conserved.But what happens if we consider symmetries of spacetime itself? Can we then impose conservation laws on arbitrary fields with arbitrary dynamics? As it turns out, yes.Minkowski spacetime has many symmetries, but let us start with symmetry under translations in spacetime: for small translations of a field in spacetime \\(\\delta x^\\rho\\), the variation of the Lagrangian is zero, \\(\\delta \\mathcal{L} = 0\\).Since the translation is small, we can expand \\(\\phi\\) to first-order,\\[\\delta \\phi = \\delta x^\\mu \\: \\partial_\\mu \\phi\\]The corresponding variation of the Lagrangian is,\\[\\begin{align}\\delta \\mathcal{L} &amp;amp; = \\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\delta \\phi + \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} \\delta \\left( \\partial_\\mu \\phi \\right) \\\\ &amp;amp; = \\partial_\\mu \\pi^\\mu \\delta x^\\nu \\: \\partial_\\nu \\phi + \\pi^\\mu \\partial_\\mu \\left( \\delta \\phi \\right) \\\\ &amp;amp; = \\partial_\\mu \\pi^\\mu \\delta x^\\nu \\: \\partial_\\nu \\phi + \\pi^\\mu \\partial_\\mu \\left( \\delta x^\\nu \\: \\partial_\\nu \\phi \\right) \\\\ &amp;amp; = \\partial_\\mu \\pi^\\mu \\delta x^\\nu \\: \\partial_\\nu \\phi + \\pi^\\mu \\delta x^\\nu \\partial_\\mu \\partial_\\nu \\phi \\\\ &amp;amp; = \\left[ \\partial_\\mu \\left( \\pi^\\mu \\partial_\\nu \\phi \\right) + \\partial_\\nu \\mathcal{L} \\right] \\delta x^\\nu \\\\ &amp;amp; = \\partial_\\nu \\mathcal{L} \\: \\delta x^\\nu = 0\\end{align}\\]In the last step, we wrote \\(\\delta \\mathcal{L} = \\partial_\\nu \\mathcal{L} \\: \\delta x^\\nu\\) since only the coordinates really change under a translation. Now, as \\(\\delta x^\\nu\\) is arbitrary,\\[\\begin{align}\\partial_\\mu \\left( \\pi^\\mu \\partial_\\nu \\phi \\right) - \\partial_\\nu \\mathcal{L} &amp;amp; = 0 \\\\\\partial_\\mu \\left( \\pi^\\mu \\partial_\\nu \\phi \\right) - \\delta^\\mu_{\\phantom{\\mu} \\nu} \\partial_\\mu \\mathcal{L} &amp;amp; = 0 \\\\\\partial_\\mu \\left( \\pi^\\mu \\partial_\\nu \\phi \\right) - \\partial_\\mu \\left( \\delta^\\mu_{\\phantom{\\mu} \\nu} \\mathcal{L} \\right) &amp;amp; = 0 \\\\\\partial_\\mu \\left[ \\pi^\\mu \\partial_\\nu \\phi - \\delta^\\mu_{\\phantom{\\mu} \\nu} \\mathcal{L} \\right] &amp;amp; = 0 \\\\\\end{align}\\]Voila, we have found a conserved Noether current for scalar fields!Energy-momentum tensorDefinitionIn the above derivation, \\(\\left[ \\pi^\\mu \\partial_\\nu \\phi - \\delta^\\mu_{\\phantom{\\mu} \\nu} \\mathcal{L} \\right]\\) forms the components of the canonical energy-momentum tensor with a lowered index,\\[T^\\mu_{\\phantom{\\mu} \\nu} = \\pi^\\mu \\partial_\\nu \\phi - \\delta^\\mu_{\\phantom{\\mu} \\nu} \\mathcal{L}\\]Generally, the energy-momentum tensor (motivated by the dynamics of discrete sets of particles) is employed as a \\(\\left( 2, 0 \\right)\\) or \\(\\left( 0, 2 \\right)\\) tensor. We can readily obtain the same by raising/lowering indices in the tensor above:\\[T^{\\mu \\nu} = \\pi^\\mu \\partial^\\nu \\phi - \\eta^{\\mu \\nu} \\: \\mathcal{L}\\]where \\(\\partial^\\nu \\phi = \\eta^{\\rho \\nu} \\partial_\\rho \\phi\\). This tensor too has vanishing divergence,\\[\\begin{align}\\partial_\\mu T^{\\mu \\nu} &amp;amp; = \\partial_\\mu \\left( \\eta^{\\nu \\rho} \\: T^\\mu_{\\phantom{\\mu} \\rho} \\right) \\\\ &amp;amp; = \\eta^{\\nu \\rho} \\partial_\\mu T^\\mu_{\\phantom{\\mu} \\rho} \\\\ &amp;amp; = 0\\end{align}\\]which is a set of \\(4\\) equations, one for each \\(\\nu\\) 2 .Physical interpretationTo understand the physical meaning of the energy-momentum tensor, let us express its vanishing divergence as a continuity equation. Recall that:\\[\\partial_\\mu T^{\\mu \\nu} = 0\\]We can expand the above into timelike and spacelike parts,\\[\\partial_0 T^{0 \\nu} + \\partial_i T^{i \\nu} = 0\\]We may interpret this as the continuity equation for \\(\\pi^\\nu\\). Then, \\(T^{i \\nu}\\) is the flux of \\(\\pi^\\nu\\) through a surface of constant \\(x^i\\). Or in spacetime, \\(T^{\\mu \\nu}\\) is the flux of \\(\\pi^\\nu\\) through a surface of constant \\(x^\\mu\\).So, this is what the energy-momentum tensor really encodes: the flux of canonical 4-momentum in spacetime 3 .SymmetryIn the above picture of the energy-momentum tensor, we find an interesting property: the energy-momentum tensor is totally symmetric, i.e. \\(T^{\\mu \\nu} = T^{\\nu \\mu}\\). Let’s see why.Recall that \\(T^{\\mu \\nu}\\) is the flux of \\(\\pi^\\nu\\) perpendicular to \\(\\pmb{e}_\\mu\\). Now consider what it means for \\(\\pi^\\nu\\) to have a flux perpendicular to \\(\\pmb{e}_\\mu\\) : energy density \\(\\pi^0\\) is being transported along \\(\\pmb{e}_\\mu\\) with conjugate-momentum \\(\\pi^i\\) in the \\(\\pmb{e}_\\nu\\) direction. But due to conjugate-momentum in the direction \\(\\pmb{e}_\\nu\\), there must flux perpendicular to the same, so \\(T^{\\nu \\mu} = T^{\\mu \\nu}\\).Scalar field LagrangianRecalling the energy-momentum tensor for a scalar field and its symmetry, we can assert,\\[\\pi^\\mu \\partial^\\nu \\phi - \\eta^{\\mu \\nu} \\mathcal{L} = \\pi^\\nu \\partial^\\mu \\phi - \\eta^{\\nu \\mu} \\mathcal{L}\\]But the metric is symmetric, hence,\\[\\pi^\\mu \\partial^\\nu \\phi = \\pi^\\nu \\partial^\\mu \\phi\\]Since this is true for any \\(\\pi^\\mu\\) and \\(\\partial^\\nu \\phi\\),\\[\\pi^\\mu = \\partial^\\mu \\phi\\]It is now straightforward to guess the general form of \\(\\mathcal{L}\\) by expanding both sides above,\\[\\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\mu \\phi \\right)} = \\eta^{\\mu \\nu} \\partial_\\nu \\phi\\]which, after some playing around, is found to be possible only if \\(\\mathcal{L}\\) is of the form,\\[\\mathcal{L} = \\frac{1}{2} \\partial_\\mu \\phi \\: \\eta^{\\mu \\nu} \\: \\partial_\\nu \\phi - V \\left( \\phi \\right)\\]ConclusionBy positing the Lagrangian for a scalar field in the above manner, we justified the expression for \\(\\mathcal{L}\\) based on the symmetry of \\(T^{\\mu \\nu}\\). This tensor was itself derived from the symmetry of the dynamics of a scalar field under translations in spacetime. These are very fundamental ideas that do not depend on the final form of \\(\\mathcal{L}\\). Technically, the quantity described is a momentum density field. However, we will simply refer to it as momentum, much like the Lagrangian density is commonly called the Lagrangian. &amp;#8617; We could bring the metric out from the partial derivative as in inertial coordinates, the metric is constant. In non-inertial coordinates, we’d instead work with covariant derivatives, and assuming metric compatibility of the connection, the metric can, again, be treated as a constant. Therefore, the expression for the energy-momentum tensor in curvilinear coordinates and even general relativity (where it is dubbed the ‘canonical’ energy-momentum tensor to distinguish it from the energy-momentum tensor appearing in the Einstein field equations) is trivially obtainable: we simple replace the Minkowski metric with a metric tensor. &amp;#8617; In general relativity, the canonical energy-momentum tensor does not necessarily encode momentum flux like the energy-momentum tensor appearing in the Einstein field equations. However, the distinction is not important in special relativity. &amp;#8617; " }, { "title": "Factorials as Invariant Points", "url": "/tempus-spatium/factorials-invariant-points/", "categories": "analysis", "tags": "gamma function, Laplace transforms", "date": "2021-11-14 00:00:00 -0500", "snippet": "In ‘Deriving the Gamma Function from Scratch’, we investigated the analytic origin of the extended factorial. Namely, it comes from the complex solution for the functional equation of the discrete factorial function.Today, we will take a more general route and interpret factorials as invariant points under a certain transformation, in order to solve for those points and obtain an explicit expression.Extended factorial: recapFunctional equationThe functional equation of the extended factorial is, as we know,\\[z! = z \\left( z-1 \\right)!\\]with the boundary condition \\(z! = 1\\).Complex solutionAs seen in the previously mentioned post, the complex solution of the above functional equation is given as,\\[z! = \\mathcal{L} \\left\\{ t^z \\right\\} \\left( 1 \\right) = \\int_0^\\infty t^z e^{-t} dt\\]where \\(\\mathcal{L}\\) is the Laplace transform operator in the time domain \\(t\\) defined as,\\[\\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right) = \\int_0^\\infty u \\left( z, t \\right) e^{-st} dt\\]Invariant pointsTransformationsGiven a complex function \\(f \\left( z \\right) : \\mathbb{C} \\mapsto \\mathbb{C}\\), a transformation is an operation \\(\\mathcal{T}\\) which can be completely expressed in terms of linear operators in the said function space, \\(\\widehat{\\mathcal{L}} \\in \\mathbb{C}^\\mathbb{C}\\). Linear transformations are special transformations \\(\\widehat{\\mathcal{T}}\\) which can be expressed as the sums and scalar products of some linear operators. It turns out from these two definitions that a transformation can always be expressed as a [not necessarily linear] function of linear transformations.More precisely, a transformation is a multivector in an uncountably-infinite dimensional Hilbert space in which linear transformations form a basis. And a linear transformation is a vector in that space. But I get ahead of myself.SymmetriesA symmetry of a transformation \\(\\mathcal{T}\\) is a quantity \\(\\sigma \\left( z \\right)\\) such that \\(\\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) = \\sigma \\left( z \\right)\\). The set of points \\(\\left( z, \\sigma \\left( z \\right) \\right)\\), or simply the entire function \\(\\sigma \\left( z \\right)\\), are said to be the invariant points under the transformation \\(\\mathcal{T}\\). They can be used to construct the symmetry operator of \\(\\mathcal{T}\\),\\[\\begin{align}\\mathcal{S} &amp;amp; = \\sigma \\left( \\cdot \\right) \\\\\\mathcal{T} \\left\\{ \\mathcal{S} \\right\\} &amp;amp; = \\mathcal{S}\\end{align}\\]Laplace transformsThe Laplace transform is a functional \\(\\mathcal{L} : \\mathcal{T} \\left\\{ \\mathcal{D} \\right\\} \\mapsto \\mathcal{V}\\) where: \\(\\mathcal{D}\\) is the differential operator; \\(\\mathcal{T} \\left\\{ \\mathcal{D} \\right\\}\\) is any differential operator; and \\(\\mathcal{V}\\) is an operator which maps a function space to itself.In other words, the Laplace transform is a map from differential operators to operators formed from ordinary functions of the form \\(v \\left( z \\right)\\).\\[\\mathcal{L} \\left\\{ \\mathcal{D}^n \\left\\{ \\cdot \\right\\} \\right\\} \\left( s \\right) = s^n \\mathcal{L} \\left\\{ \\cdot \\right\\} \\left( s \\right) - \\sum_{k=0}^{n-1} s^{k} \\mathcal{D}^{n-k} \\left\\{ \\cdot \\right\\} \\left( 0^- \\right)\\]There are infinite allowed choices for a converging Laplace transform, but the one we used, as is common practice, turns out to be one of the most useful and simple ones. It also leaves the space of complex functions closed, allowing us to construct an inverse Laplace transform operator.Finding invariant pointsGiven a transformation \\(\\mathcal{T}\\), how can we find an \\(\\mathcal{S}\\) or equivalently, some \\(\\sigma \\left( z \\right)\\) ?The trick is to convert \\(\\mathcal{T}\\) to a differential operator and use the Laplace transform. If \\(\\mathcal{T}\\) is already a differential operator, the job is easier. If not, some work is required, which is a subject of functional analysis.Suppose a given differential operator \\(\\mathcal{T}\\) has some \\(\\mathcal{S}\\). We identify that \\(\\mathcal{S}\\) is formed from an ordinary function \\(\\sigma \\left( z \\right)\\). Invoking the idea of Laplace transforms, we can define an invertible map \\(\\mathcal{L} : \\mathcal{T} \\mapsto \\mathcal{S}\\) (here, \\(\\mathcal{T}\\) and \\(\\mathcal{S}\\) represent the spaces of all possible \\(\\mathcal{T}\\)’s and \\(\\mathcal{S}\\)’s) so that,\\[\\mathcal{S} = \\mathcal{L} \\left\\{ \\mathcal{T} \\right\\}\\]Thus, the symmetry operator of a differentiable transformation is a Laplace transform.Back to factorialsAs invariant pointsRecall the functional equation for factorials,\\[z! = z \\left( z-1 \\right)!\\]Now consider a related transformation defined as:\\[\\mathcal{T} \\left\\{ f \\right\\} \\left( z \\right) = z f \\left( z-1 \\right)\\]invariant points of the above transformation then obey:\\[\\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) = z \\sigma \\left( z-1 \\right) = \\sigma \\left( z \\right)\\]which is precisely the functional equation of factorials.Differential operatorNow, we must turn \\(\\mathcal{T}\\) to a differential operator so that we can apply \\(\\mathcal{L}\\) to it. To do so, we switch domain from \\(z\\) to \\(t\\) (the ‘time domain’) so that the below is satisfied for some function \\(u \\left( z, t \\right)\\) :\\[\\mathcal{T} \\left\\{ f \\right\\} \\left( z \\right) = \\mathcal{D}_t \\left\\{ u \\left( z, t \\right) \\right\\}\\]For invariant points \\(\\sigma \\left( z \\right)\\),\\[\\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) = \\mathcal{D}_t \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} = \\sigma \\left( z \\right)\\]Laplace transformThe linear map \\(\\mathcal{L} : \\mathcal{T} \\mapsto \\mathcal{S}\\) is a Laplace transform for some unknown value \\(s\\),\\[\\begin{align}\\sigma \\left( z \\right) &amp;amp; = \\mathcal{L} \\left\\{ \\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) \\right\\} \\left( s \\right) \\\\ &amp;amp; = \\mathcal{L} \\left\\{ \\mathcal{D}_t \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} \\right\\} \\left( s \\right) \\\\ &amp;amp; = s \\: \\mathcal{L} \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} \\left( s \\right) - u_\\sigma \\left( z, 0^- \\right) \\\\\\therefore \\mathcal{L} \\left\\{ \\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) \\right\\} \\left( s \\right) &amp;amp; = \\frac{1}{s} \\left( \\sigma \\left( z \\right) + u_\\sigma \\left( z, 0^- \\right) \\right) : = \\sigma \\left( z \\right) \\\\\\implies s &amp;amp; = 1 , \\\\u_\\sigma \\left( z, 0^- \\right) &amp;amp; = 0, \\\\\\sigma \\left( z \\right) &amp;amp; = \\mathcal{L} \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} \\left( 1 \\right)\\end{align}\\]SolutionFrom the above, we can find \\(\\sigma \\left( z \\right)\\) if we can find \\(u_\\sigma \\left( z, t \\right)\\). From the statements below,\\[\\begin{align}\\sigma \\left( z \\right) = \\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) &amp;amp; = z \\sigma \\left( z-1 \\right) \\\\\\mathcal{T} \\left\\{ \\sigma \\right\\} \\left( z \\right) &amp;amp; = \\mathcal{D}_t \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\}\\end{align}\\]\\[\\sigma \\left( z \\right) = \\mathcal{L} \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} \\left( 1 \\right)\\]we can ascertain,\\[\\begin{align}\\mathcal{D}_t \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} &amp;amp; = z \\: \\mathcal{L} \\left\\{ u_\\sigma \\left( z-1, t \\right) \\right\\} \\left( 1 \\right) \\\\\\mathcal{D}_t \\left\\{ \\mathcal{L} \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} \\left( 1 \\right) \\right\\} &amp;amp; = \\mathcal{L} \\left\\{ z \\: u_\\sigma \\left( z-1, t \\right) \\right\\} \\left( 1 \\right) \\\\\\mathcal{L} \\left\\{ \\mathcal{D}_t \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} \\right\\} \\left( 1 \\right) &amp;amp; = \\mathcal{L} \\left\\{ z \\: u_\\sigma \\left( z-1, t \\right) \\right\\} \\left( 1 \\right) \\\\\\therefore \\mathcal{D}_t \\left\\{ u_\\sigma \\left( z, t \\right) \\right\\} &amp;amp; = z \\: u_\\sigma \\left( z-1, t \\right)\\end{align}\\]As in the previous post on the extended factorial, we realize that \\(u_\\sigma \\left( z, t \\right) = t^z\\) solves the above equation as \\(\\frac{\\partial}{\\partial t} t^z = z \\: t^{z-1}\\). Furthermore, it is indeed true that \\(u_\\sigma \\left( z, 0^- \\right) = 0\\).Thus, we have,\\[z! = \\sigma \\left( z \\right) = \\mathcal{L} \\left\\{ t^z \\right\\} \\left( 1 \\right) = \\int_0^\\infty t^z e^{-t} dt\\]ConclusionWith respect to the previous post on the extended factorial (Deriving the Gamma Function from Scratch), our most recent derivation is essentially identical to the previous one. However, there’s more than symbol manipulation going on here: We’re thinking of factorials as special points which remain invariant under the transformation introduced. This is a very general way of stating functional equations. In fact, by stating the problem in this manner, we don’t always have to know the exact functional equation, though we didn’t have to encounter such a scenario here. By virtue of being a symmetry operation, we know that the factorial must be a Laplace transform. This wasn’t justified in the previous derivation. The new procedure automatically let us infer that \\(s=1\\) and \\(u \\left( z, 0^- \\right) = 0\\), whereas before, we assumed those were true and matched the final result by brute force. That said, two things still remain mysterious: Why does the peculiar integral we use for the Laplace transform work as a linear map \\(\\mathcal{T} \\mapsto \\mathcal{S}\\) ? Why is there a time domain \\(t\\) which allows us to write \\(\\mathcal{T} = \\mathcal{D}_t \\left\\{ u \\left( \\cdot, t \\right) \\right\\}\\) such that \\(\\mathcal{T}\\) itself is independent of \\(t\\) i.e. \\(\\mathcal{D}_t \\mathcal{T} = \\mathcal{D}_t^2 \\left\\{ u \\left( \\cdot, t \\right) \\right\\} = 0\\) ? The above questions have very interesting answers that require a juicy dive into complex analysis, measure theory and unexpectedly, algebraic geometry (hint: the last equation, \\(\\mathcal{D}_t^2 = 0\\) smells very much of exterior derivatives). I will try to cover those topics in future posts ;)Thanks for reading." }, { "title": "Harmonic Oscillators in Scalar Field Theory", "url": "/tempus-spatium/harmonic-oscillators-scalar-field-theory/", "categories": "classical field theory", "tags": "harmonic oscillators, energy-momentum tensor, linearity", "date": "2021-11-06 00:00:00 -0400", "snippet": "Once again, let us see how classical scalar field theory naturally gives rise to common natural phenomena. Today, we will derive the evolution of harmonic oscillators.Scalar formScalar fieldsGiven a scalar field (in flat spacetime) \\(\\phi \\left( x^\\rho \\right)\\) with the following Lagrangian density,\\[\\mathcal{L} = \\frac{1}{2} \\partial_\\alpha \\phi \\: \\eta^{\\alpha \\beta} \\: \\partial_\\beta \\phi - V \\left( \\phi \\right)\\]We obtain the equation of motion,\\[\\square \\phi + \\frac{\\partial V}{\\partial \\phi} = 0\\]Klein-Gordon equationTaylor-expanding the term \\(\\frac{\\partial V}{\\partial \\phi} = V^\\prime \\left( \\phi \\right)\\),\\[V^\\prime \\left( \\phi \\right) = \\sum_{k=0}^\\infty \\frac{1}{k!} \\: V^{\\left( k+1 \\right)} \\left( 0 \\right) \\: \\phi^k\\]The equation of motion we have seen previously is a wave equation, i.e. its solutions are oscillating waves. If we are given a solution \\(\\phi\\) with a small amplitude so that the square and higher powers of \\(\\phi\\) become negligible, \\(V^\\prime \\left( \\phi \\right)\\) assumes the form,\\[\\begin{align}V^\\prime \\left( \\phi \\right) &amp;amp; \\approx \\sum_{k=0}^1 \\frac{1}{k!} \\: V^{\\left( k+1 \\right)} \\left( 0 \\right) \\: \\phi^k \\\\ &amp;amp; = V^{\\prime} \\left( 0 \\right) + V^{\\prime \\prime} \\left( 0 \\right) \\: \\phi\\end{align}\\]If the oscillating field \\(\\phi\\) is stable, it must vibrate about a local minima of the potential. Without loss of generality, let this minima exist at \\(\\phi = 0\\) (if it does not, we can add to \\(\\phi\\) a constant scalar field so that the minima exists at \\(\\phi_{\\text{new}} = 0\\), without altering the equations of motion). Hence, setting \\(V^\\prime \\left( 0 \\right) = 0\\),\\[V^\\prime \\left( \\phi \\right) = V^{\\prime \\prime} \\left( 0 \\right) \\: \\phi\\]Since we are considering the limiting case where the amplitude \\(\\phi_{\\text{max}} \\to 0\\), the equality sign above is appropriate. Plugging the result into the equation of motion,\\[\\begin{aligned}\\square \\phi + V^{\\prime \\prime} \\left( 0 \\right) \\: \\phi &amp;amp; = 0 &amp;amp; \\text{i.e.,} \\\\\\square \\phi + m^2 \\phi &amp;amp; = 0\\end{aligned}\\]The above equation is known as the Klein-Gordon equation, named after its discoverers. \\(V^{\\prime \\prime} \\left( 0 \\right)\\) is analogous to a spring constant \\(m^2\\), which is the force \\(- \\frac{\\partial V}{\\partial \\phi}\\) per unit displacement \\(\\phi\\). The reason for writing the spring constant as \\(m^2\\) and not as \\(m\\) will become apparent later.LinearityThe Klein-Gordon equation is linear, i.e. valid solutions labelled as \\(\\phi_{\\left( i \\right)}\\) may be added to yield new valid solutions, as long as they have the same \\(m^2\\) (i.e. the same \\(m\\) upto magnitude),\\[\\begin{aligned}\\square \\sum_i \\phi_{\\left( i \\right)} &amp;amp; = \\sum_i \\square \\phi_{\\left( i \\right)} \\\\ &amp;amp; = \\sum_i m^2 \\phi_{\\left( i \\right)} \\\\ &amp;amp; = m^2 \\sum_i \\phi_{\\left( i \\right)}\\end{aligned}\\]Tensor formStress-energy tensorWhilst the scalar form of the Klein-Gordon equation is simple, a certain sense of direction is ingrained in its mechanism, which is not directly encoded in the scalar form. Consider a generalized spring: when it is displaced from the stable configuration by a certain amount, a proportional restoring force acts in the opposite direction of the displacement. Even though a minus sign in the expression \\(\\square \\phi = - k \\phi\\) indicates this, the formula is not manifestly vectorial, or higher-order (which is required to contain explicit information on direction).To fix this, we will derive the evolution of harmonic oscillators in a different manner. A complete description of a field’s momentum (which has a sense of direction with respect to the field’s dynamics) is provided by its stress-energy tensor \\(T_{\\mu \\nu}\\),\\[T_{\\mu \\nu} = \\eta_{\\mu \\rho} \\frac{\\partial \\mathcal{L}}{\\partial \\left( \\partial_\\rho \\phi \\right)} \\partial_\\nu \\phi - \\mathcal{L} \\: \\eta_{\\mu \\nu}\\]The above tensor satisfies the quintessential properties of a stress-energy tensor: symmetry, conservation and the dimensions of energy density. Now, from the Lagrangian for our scalar field,\\[\\begin{align}T_{\\mu \\nu} &amp;amp; = \\eta_{\\mu \\rho} \\: \\partial^\\rho \\phi \\: \\partial_\\nu \\phi - \\left( \\frac{1}{2} \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - V \\left( \\phi \\right) \\right) \\eta_{\\mu \\nu} \\\\T_{\\mu \\nu} &amp;amp; = \\partial_\\mu \\phi \\: \\partial_\\nu \\phi - \\left( \\frac{1}{2} \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - V \\left( \\phi \\right) \\right) \\eta_{\\mu \\nu}\\end{align}\\]The trace \\(T\\) of the above stress-energy tensor is,\\[\\begin{aligned}T &amp;amp; = \\eta^{\\mu \\nu} \\: T_{\\mu \\nu} \\\\ &amp;amp; = \\eta^{\\mu \\nu} \\left[ \\partial_\\mu \\phi \\: \\partial_\\nu \\phi - \\left( \\frac{1}{2} \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - V \\left( \\phi \\right) \\right) \\eta_{\\mu \\nu} \\right] \\\\ &amp;amp; = \\eta^{\\mu \\nu} \\partial_\\mu \\phi \\: \\partial_\\nu \\phi - \\eta_{\\mu \\nu} \\eta^{\\mu \\nu} \\left( \\frac{1}{2} \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - V \\left( \\phi \\right) \\right) \\\\ &amp;amp; = \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - 4 \\left( \\frac{1}{2} \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - V \\left( \\phi \\right) \\right) \\\\ &amp;amp; = \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi - 2 \\: \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi + 4 V \\left( \\phi \\right) \\\\\\end{aligned}\\]Or,\\[T = 4 V \\left( \\phi \\right) - \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi\\]Cauchy strain tensorFor infinitesimal displacements of a continuous field \\(\\phi\\), its deformation is encoded in the Cauchy strain tensor \\(\\epsilon_{\\rho \\sigma}\\),\\[\\epsilon_{\\rho \\sigma} = \\partial_{( \\rho} \\: u_{\\sigma )} = \\frac{1}{2} \\left( \\partial_\\rho u_\\sigma + \\partial_\\sigma u_\\rho \\right)\\]where \\(u_\\mu\\) is the displacement gradient defined as,\\[\\begin{aligned}u_\\mu &amp;amp; = \\text{d} \\phi \\left( \\frac{d}{dx^\\mu} \\right) \\\\ &amp;amp; = \\frac{d \\phi}{dx^\\mu} \\\\\\end{aligned}\\]Assuming \\(\\phi\\) is not velocity-dependent,\\[\\begin{aligned}u_\\mu &amp;amp; = \\frac{d \\phi}{dx^\\mu} \\\\ &amp;amp; = \\frac{dx^\\nu}{dx^\\mu} \\partial_\\nu \\phi \\\\ &amp;amp; = \\delta^\\nu_{\\phantom{\\nu} \\mu} \\: \\partial_\\nu \\phi \\\\ &amp;amp; = \\partial_\\mu \\phi\\end{aligned}\\]Therefore, the Cauchy strain tensor takes the form,\\[\\epsilon_{\\rho \\sigma} = \\frac{1}{2} \\left( \\partial_\\rho \\partial_\\sigma \\phi + \\partial_\\sigma \\partial_\\rho \\phi \\right)\\]The trace of the above tensor is a coordinate-independent scalar \\(\\epsilon\\),\\[\\begin{aligned}\\epsilon &amp;amp; = \\eta^{\\rho \\sigma} \\epsilon_{\\rho \\sigma} \\\\ &amp;amp; = \\frac{1}{2} \\eta^{\\rho \\sigma} \\left( \\partial_\\rho \\partial_\\sigma \\phi + \\partial_\\sigma \\partial_\\rho \\phi \\right) \\\\ &amp;amp; = \\frac{1}{2} \\left( \\eta^{\\rho \\sigma} \\partial_\\rho \\partial_\\sigma \\phi + \\eta^{\\rho \\sigma} \\partial_\\sigma \\partial_\\rho \\phi \\right) \\\\ &amp;amp; = \\eta^{\\rho \\sigma} \\partial_\\rho \\partial_\\sigma \\phi \\\\\\end{aligned}\\]Or,\\[\\epsilon = \\square \\phi\\]Solutions as eigenfunctions of partial derivativesOnce, again, we will consider solutions of \\(\\phi\\) with small amplitude, so that,\\[\\begin{aligned}\\frac{\\partial V}{\\partial \\phi} &amp;amp; \\approx V^{\\prime \\prime} \\left( 0 \\right) \\phi = m^2 \\phi \\\\\\square \\phi &amp;amp; \\approx - m^2 \\phi\\end{aligned}\\]Therefore, we can simplify the trace of the stress-energy tensor as,\\[\\begin{aligned}T &amp;amp; = 4 V \\left( \\phi \\right) - \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi \\\\ &amp;amp; = 4 \\int \\frac{\\partial V}{\\partial \\phi} d \\phi - \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi \\\\ &amp;amp; = 4 \\left( \\frac{1}{2} m^2 \\phi^2 \\right) - \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi \\\\ &amp;amp; \\approx - \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi\\end{aligned}\\]The arbitrary constant in \\(\\displaystyle{\\int \\frac{\\partial V}{\\partial \\phi} d \\phi}\\) may be neglected as the equation of motion remains invariant under adding a constant to \\(V \\left( \\phi \\right)\\). Therefore, the term vanishes as \\(\\phi_{\\text{max}}\\), being negligible, ensures that \\(\\phi^2\\) vanishes too.Now, we must solve for \\(\\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi\\). There is no nice substitution which gives some known term from the equation of motion. However, we may make use of the linearity of the fields obtained from Hooke’s law. Suppose in the statement, \\(\\square \\phi = - m^2 \\phi\\), we say \\(m^2 = z^\\mu z_\\mu\\) where the first and higher derivatives of \\(k^\\mu\\) vanish. \\(z^\\mu\\) is therefore some constant vector whose magnitude in Minkowski space is \\(m\\). This is why we wrote the spring constant as \\(m^2\\); had we written it as \\(m\\), we would have to substitute \\(\\sqrt{k^\\mu k_\\mu}\\) for it in the equation of motion, spoiling linearity as seen in the derivation below. But the Klein-Gordon equation was derived from the insignificance of second-order and higher terms to begin with, so such a procedure would lead to ambiguity.Substituting \\(m^2 = z^\\mu z_\\mu\\) in the equation of motion,\\[\\eta^{\\alpha \\beta} \\: \\partial_\\alpha \\partial_\\beta \\phi = - \\eta^{\\alpha \\beta} z_\\alpha z_\\beta \\phi\\]By linear independence of different components of \\(z_\\mu\\),\\[\\partial_\\alpha \\partial_\\beta \\phi = - z_\\alpha z_\\beta \\phi\\]This severely restricts the \\(k^\\mu\\) we choose. Furthermore, \\(\\phi\\) can now be solved in terms of \\(k_\\mu\\) and the only sensible way of writing its first derivative, judging by the second derivative and its indices, is,\\[\\partial_\\alpha \\phi = i z_\\alpha \\phi\\]where \\(i^2 = -1\\). This may seem unjustified, so let us use tensors explicitly so that the restriction follows naturally. We begin by stating the first substitution a bit more rigorously,\\[\\begin{aligned}\\left( \\partial_\\alpha \\otimes \\partial_\\beta \\right) \\phi = - \\phi \\left( z_\\alpha \\otimes z_\\beta \\right)\\end{aligned}\\]The above is true throughout spacetime only if \\(\\partial_\\alpha \\phi = z_\\alpha \\: i \\left( \\phi, \\cdot \\right)\\) where \\(i\\) is a \\(\\left( 0, 0 \\right)\\) tensor obeying the identity,\\[i \\left( \\phi, \\phi \\right) = - \\phi\\]Therefore, \\(\\partial_\\alpha \\left( \\phi \\right) = i z_\\alpha \\cdot \\phi\\). I.e., all \\(\\phi\\) are eigenfunctions of all \\(\\partial_\\alpha\\).Hooke’s law, upgradedFrom the above results, we have,\\[\\begin{aligned}T &amp;amp; = - \\partial_\\alpha \\phi \\: \\partial^\\alpha \\phi \\\\ &amp;amp; = - \\left( i z_\\alpha \\: \\phi \\right) \\left( i z^\\alpha \\: \\phi \\right) \\\\ &amp;amp; = - i^2 \\: z_\\alpha z^\\alpha \\phi \\\\ &amp;amp; = m^2 \\phi\\end{aligned}\\]We can also simplify \\(\\text{tr} \\left( \\epsilon_{\\rho \\sigma} \\right) = \\epsilon\\) in the limit of small oscillations,\\[\\begin{aligned}\\epsilon &amp;amp; = \\square \\phi \\\\ &amp;amp; = - m^2 \\phi\\end{aligned}\\]Clearly, \\(T = - \\epsilon\\). This can be true only if \\(T_{\\mu \\nu}\\) and \\(\\epsilon_{\\rho \\sigma}\\) have a multilinear relationship via a \\(\\left( 2, 2 \\right)\\) tensor:\\[T_{\\mu \\nu} = - C^{\\rho \\sigma}_{\\phantom{\\rho \\sigma} \\mu \\nu} \\: \\epsilon_{\\rho \\sigma}\\]The tensor \\(C^{\\rho \\sigma}_{\\phantom{\\rho \\sigma} \\mu \\nu}\\) is known as the elasticity tensor of \\(\\phi\\) which, like \\(k\\), has vanishing derivatives.ConclusionWe have derived the evolution equation of harmonic oscillators, in tensorial form, at long last! But was it necessary at all?If our goal was to derive the existence of harmonic oscillators using simple assumptions and geometric tools, the answer is yes. The assumptions we made implicitly were: The field theory should be scalar, so that internal degrees of freedom do not become important in considerations of energy. The field theory should be Lorentz invariant with a second-order equation of motion, so that the relativity of position and velocity are accounted for. All the fancy assumptions involving \\(\\phi_{\\text{max}} \\approx 0\\). Given the above conditions, harmonic osillators are bound to arise very naturally." }, { "title": "The Discrete Antiderivative Operator", "url": "/tempus-spatium/discrete-antiderivative-operator/", "categories": "discrete mathematics", "tags": "discrete calculus", "date": "2021-10-31 00:00:00 -0400", "snippet": "Discrete derivative operatorLet a function \\(f: \\mathbb{R} \\mapsto \\mathbb{R}\\). On discretizing the domain of \\(f\\) into quanta \\(h\\) centred at \\(a_0\\), \\(f: \\mathbb{A} \\mapsto \\mathbb{A}\\) where \\(\\mathbb{A} = \\left\\{ kh+a_0 : k \\in \\mathbb{Z}, a_0 \\in \\mathbb{R} \\right\\}\\), the derivative operator is replaced by the discrete derivative operator \\(\\mathcal{D}: \\mathbb{A}^{\\mathbb{A}} \\mapsto \\mathbb{A}^{\\mathbb{A}}\\) defined as,\\[\\mathcal{D} \\left\\{ f \\right\\} \\left( a \\right) = \\frac{f \\left( a \\right) - f \\left( a-h \\right)}{h} \\tag{1}\\]Notice that, \\(\\displaystyle{\\lim_{h \\to 0} \\mathcal{D} \\left\\{ f \\right\\} \\left( a \\right) = f^\\prime \\left( a \\right)}\\), if such a limit exists at \\(x=a\\) where \\(f^\\prime \\left( x \\right)\\) denotes the first derivative of \\(f \\left( x \\right)\\).As usual, we are using the backward difference to construct the difference quotient above. That’s because in this blog, we’ll primarily use discrete calculus in applications of mathematical physics, where the backward difference can be used to encode causality in a direct sense.Discrete antiderivative operatorOperational definitionThe discrete antiderivative operator \\(\\mathcal{D}^{-1}\\) is the inverse of the discrete derivative operator \\(\\mathcal{D}\\),\\[\\mathcal{D} \\left\\{ \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right\\} \\left( a \\right) = \\mathcal{D}^{-1} \\left\\{ \\mathcal{D} \\left\\{ f \\right\\} \\right\\} \\left( a \\right) = f \\left( a \\right)\\]I.e. \\(\\mathcal{D} \\circ \\mathcal{D}^{-1} = \\mathcal{D}^{-1} \\circ \\mathcal{D} = \\mathcal{D}^0\\)From \\(\\left( 1 \\right)\\), we have,\\[\\begin{align}f \\left( a \\right) &amp;amp; = \\mathcal{D} \\left\\{ \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right\\} \\left( a \\right) \\\\ &amp;amp; = \\frac{1}{h} \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right) - \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a-h \\right) \\right)\\end{align}\\]\\[\\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right) - \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a-h \\right) = h f \\left( a \\right) \\tag{2}\\]As Riemann sumsFinding \\(\\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right)\\) from the above equation alone is difficult. Instead, we will discretize the indefinite integral of the continuous function \\(f \\left( x \\right)\\). The intuition behind this is that the integral of a function is its continuous antiderivative.To do this, we will write the indefinite integral \\(F \\left( x \\right)\\) of \\(f \\left( x \\right)\\) as a Riemann integral and remove the limit,\\[F \\left( x \\right) = \\int_{F^{-1} \\left( 0 \\right)}^x d \\xi f \\left( \\xi \\right) = \\lim_{h \\to 0} \\sum_{k=1}^{n} h f \\left( F^{-1} \\left( 0 \\right) + kh \\right)\\]where \\(n = \\frac{x - F^{-1} \\left( 0 \\right)}{h}\\)Removing the limit from the Riemann integral, we get a corresponding expression for the discrete antiderivative of \\(f \\left( a \\right)\\) by changing variables in the manner,\\[f \\left( x \\right) \\leftrightarrow f \\left( a \\right), F \\left( x \\right) \\leftrightarrow \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right), F^{-1} \\left( x \\right) \\leftrightarrow \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( a \\right), n \\leftrightarrow \\frac{a-\\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right)}{h}\\]\\[\\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right) = \\sum_{k=1}^{n} h f \\left( \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) + kh \\right)\\]In other words,\\[\\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right) = h \\left[ f \\left( \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) + h \\right) + f \\left( \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) + 2h \\right) + \\dots + f \\left( a \\right) \\right]\\]We now have to express the term \\(\\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right)\\) explicitly.Finding the unknown termFor it to be true that:\\[\\mathcal{D} \\left\\{ \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right\\} \\left( a \\right) = \\mathcal{D}^{-1} \\left\\{ \\mathcal{D} \\left\\{ f \\right\\} \\right\\} \\left( a \\right) = f \\left( a \\right)\\]we will have to do the following,\\[\\begin{align}f \\left( a \\right) &amp;amp; = \\mathcal{D} \\left\\{ \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right\\} \\left( a \\right) \\\\ &amp;amp; = \\frac{1}{h} \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a \\right) - \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\left( a-h \\right) \\right) \\\\ &amp;amp; = \\frac{1}{h} \\left( h f \\left( a \\right) - h f \\left( \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) \\right) \\right) \\\\ &amp;amp; = f \\left( a \\right) - f \\left( \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) \\right)\\end{align}\\]This means that \\(f \\left( \\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) \\right) = 0\\) in order for the antiderivative operator to satisfy its definition. Or,\\[\\left( \\mathcal{D}^{-1} \\left\\{ f \\right\\} \\right)^{-1} \\left( 0 \\right) = f^{-1} \\left( 0 \\right)\\]Explicit definitionFrom the previous result, we can explicitly write the discrete antiderivative of a function,\\[\\mathcal{D}^{-1} \\{ f \\} \\left( a \\right) = \\sum_{k=1}^{n} h f \\left( f^{-1} \\left( 0 \\right) + kh \\right)\\]Where,\\[n = \\frac{a- f^{-1} \\left( 0 \\right)}{h}\\]" }, { "title": "The Impossible Cut", "url": "/tempus-spatium/impossible-cut/", "categories": "geometry", "tags": "Euclidean space, time", "date": "2021-10-26 00:00:00 -0400", "snippet": "A sweet problemThe classic statementImagine you have a cake. How can you slice it into \\(8\\) pieces in exactly \\(3\\) steps? Well, you divide the cake into two, three times, so that the number of pieces compounds to \\(2^3 = 8\\). This can be done by cutting the cake along different planes, in the following manner:How mathematicians probably cut cakesNotice how the planes of the cuts in the above are mutually orthogonal. If they weren’t so, it wouldn’t be possible to multiply the number of pieces of the cake by two every slice.‘Orthogonal’ here doesn’t necessarily imply ‘perpendicular’, rather, ‘independence’. However, mutually perpendicular cuts are guarunteed to multiply the number of pieces by two every cut, whereas oblique cuts may or may not do the same, depending on the position and angle of the cut with respect to previous cuts.EfficiencyNow, what happens if you cut the cake again, after the previous endeavour? No matter what you try, you cannot make the number of pieces twice as the previous, this time. This has to do with the geometry of \\(3D\\) Euclidean space, the space we and the cake live in: after cutting it orthogonally \\(3\\) times, there’s simply no orthogonal plane left to cut along.A nice way to mathematically state the above idea is this: in 3-dimensional space, only the first \\(3\\) cuts can be \\(100 \\%\\) efficient. By efficiency, we mean the cut concerned multiplies the number of pieces of the cake as much as possible, i.e. by \\(2\\).How do we mathematically define efficiency? By analogy with its definition in various other contexts, we may propose,\\[\\text{Efficiency} \\left( \\eta \\right) = \\frac{\\text{Magnification factor} \\left( m \\right)}{\\text{Desired magnification factor} \\left( 2 \\right)}\\]where the ‘magnification factor’ of a cut is the number by which it multiplies the number of pieces in the cake. In \\(3D\\) space, only the first \\(3\\) cuts have \\(\\eta = 1\\).The maximum efficiency of the subsequent cut is \\(\\eta = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\\) (imagine slicing a ‘column’, i.e. a half of the cake into two — this leaves us with 3 columns, so \\(m = 3/2\\)) .The next cut can have a maximum efficiency of \\(\\eta = \\frac{\\frac{4}{3}}{2} = \\frac{2}{3}\\), by the same logic as the previous. We can see a pattern emerging: for the \\(n^{\\text{th}}\\) cut following the first \\(3\\) ones, the maximum efficiency is,\\[\\eta = \\frac{n+2}{2 \\left( n+1 \\right)} = \\frac{n+2}{2n+2}\\]The impossible cutProblem statementLet us ask a daring question: is it possible, after the first \\(3\\) cuts, to make a fourth cut with \\(\\eta=1\\), in our own physical universe? In other words, can we have a fourth ‘magical’ cut which increases the number of pieces of the cake by \\(2\\) without breaking the rules of physics?Before trying to answer the question, let us see what answering it entails. Recall that in \\(3D\\) Euclidean space, the first \\(3\\) cuts have \\(\\eta=1\\). Now, it is implicitly true that for a cut to have \\(\\eta = 1\\), it must be mutually orthogonal to all previous cuts. Therefore, a fourth cut with \\(\\eta=1\\) must be mutually orthogonal to the previous \\(3\\) cuts, thereby requiring a \\(4D\\) Euclidean space.Luckily, our physical universe is no stranger to four-dimensional Euclidean geometry. Thanks to the concept of time.Time and changeMathematically speaking, time is a smooth parameter \\(t \\in \\mathbb{R}\\) which is independent of coordinates representing position. This allows us to construct a four-dimensional Euclidean space ismorphic to \\(\\mathbb{R}^4\\), with time as the fourth dimension, parameterized by \\(t\\).But, what is time? There are different ways to define time, but the one I find the most straightforward is, “time is a measure of [periodic] change”. If we have a system with a state \\(\\sigma\\) and it updates discretely and repetitively through steps \\(n\\) as \\(\\sigma = \\sigma \\left( n \\right)\\), time is a linear function of the steps, \\(t = an+b\\). Time as a continuous parameter is a field extension of the previous discrete definition (provided it is equipped with a time update operator \\(+\\), which acts as an addition operator to turn time from a measure to a field).The two above notions of time, as a fourth dimension and as a measure of periodic change, apparently seem to be different from each other. This is because the first idea is an abstraction, and the second is a way to measure the quantity associated with the abstraction. We will use both these facets of time to perform our ‘magical’ fourth cut on the cake.The cutAs we have seen, the fourth cut must be mutually orthogonal to the previous three cuts so that \\(\\eta = 1\\). As time is mutually orthogonal to spatial dimensions, we can ‘cut the cake along time’ to split the \\(8\\) pieces from before into \\(8 \\times 2 = 16\\) pieces. But what does it mean to ‘cut the cake along time’?The answer lies in what a ‘cut’ means in the first place. In the usual sense (without involving time, i.e.), a cut is a real or imaginary cross-section which separates the object it passes through into two or more distinct partitions.Since we have moved on to \\(4D\\) Euclidean space with time as a dimension, the idea of ‘points’ is now replaced by that of ‘events’. Consequently, cross-sections are replaced by continuous sets of events in space passing through the concerned object at the same time.If the said set of events passes through our cake at some time \\(t = \\tau\\) such that the cake before the event \\(\\left( t &amp;lt; \\tau \\right)\\) and the cake after the event \\(\\left( t &amp;gt; \\tau \\right)\\) are distinct, then there exist two distinct versions of the cake throughout the time dimension (and hence, \\(4D\\) Euclidean space) as a whole.To make the above process sound a bit more physical, let the ‘events passing through the cake at \\(t = \\tau\\)’ be spray-painting the cake at \\(t = \\tau\\) (with food-grade paint, worry not ;) Each event in this set of events is nothing but the spray-painting of a particular point in the cake (spatially).Also, let us define that temporally, two cakes are different cakes altogether if their colours are different. Then, before \\(t = \\tau\\), assuming we have performed the first \\(3\\) orthogonal cuts, there are \\(8\\), say, ‘normal’ pieces of the cake. But after \\(t = \\tau\\), there are \\(8\\) new pieces of coloured cake. The \\(8\\) old pieces have not vanished, but exist in the past, if we look at the cake’s entire history at once (which is what we do in \\(3D\\) space — we look at all of Euclidean space at once to determine the cake’s partitions). Voila, we have cut the cake into \\(16\\) pieces at precisely \\(t = \\tau\\) !The image below tries to visualize the ‘magic cut’:The magic cutConclusionWe now know how to slice a cake into \\(16\\) parts in \\(4\\) steps. But can we go even further? Can we construct \\(5D\\) and even higher-dimensional Euclidean spaces from mutually independent continuous quantities available in this universe? Whilst we can, there’s something which makes time unique.Remember how we had to think of the cake and its partitions throughout space and time to count its partitions? This aspect of time, which allows us to view all states of the universe — past, present and future, simultaneously — forms the core of the philosophy of eternalism , which is employed in physics as the block universe theory.In this light, it makes no concrete sense to treat continuous quantities independent of spatial and temporal coordinates (such as temperature, pressure, etc.) as dimensions, as the corresponding block universe makes no physical sense. Yet, the block universe obtained by taking time has a strong physical meaning, thanks to the structure of spacetime, a subject of relativity. In relativity, it turns out that the most fundamental background where events can take place is spacetime, which approximates to Minkowski space locally (but may have different structure globally). Furthermore, Minkowski space approximates to \\(4D\\) Euclidean space for low relative velocities and energies.The problem of cutting a cake ‘along time’ becomes very interesting if we consider special relativity and Minkowski (or, hyperbolic) geometry instead of Euclidean geometry. For starters, the relativity of simulteinity imposes a relative nature on the manner of slicing the cake. But that’s for another post.Hope you enjoyed reading till here!" }, { "title": "All the World&#39;s Not a Stage", "url": "/tempus-spatium/gravitation-time-reversal/", "categories": "relativity", "tags": "gravitation, time symmetry, symmetries", "date": "2021-10-24 00:00:00 -0400", "snippet": "A story without a filmConsider a gravitational system comprising the earth and an apple. The apple is released from a certain height and it plummets to the ground. How would the evolution of this system proceed, if, instead, time ran backwards?Our intuition tells us that if time runs backwards, the apple should, as if by definition, rise up from the ground and shoot to the sky in eternally decelerated motion. In other words, if we shot a film of the usual falling apple, reversing time would be equivalent to playing the film backwards.But our universe, strangely enough, does not function like such a film (where bodies merely play their part; ok, nevermind the reference). How can we be sure? Let us dive into the mathematics of our universe.Newtonian gravityTime symmetryBefore getting to Einstein’s general theory of relativity, let us try to answer the question in the domain of Newtonian physics. Here, gravity is famously described by Newton’s ‘universal law of gravitation’ (which is not really universal, but will do just fine for us at the moment),\\[F = - \\frac{GMm}{r^2}\\]The minus sign in the equation is pedantic (it has to do with gravity being an attractive force), but it will make no difference in answering the question.To see what happens if we reverse time, we must perform the substitution \\(t \\mapsto -t\\) where \\(t\\) is the parameter representing time. To do this, we must incorporate time explicitly into the law of gravitation, which can be done by going back to the definition of force,\\[\\begin{align}m \\frac{d^2 r}{dt^2} &amp;amp; = - \\frac{GMm}{r^2} \\\\\\frac{d^2 r}{dt^2} &amp;amp; = - \\frac{GM}{r^2}\\end{align}\\]Now, watch what happens when we reverse time:\\[\\begin{align}\\frac{d^2 r}{d \\left( -t \\right)^2} &amp;amp; = \\frac{d}{d \\left( -t \\right)} \\frac{d}{d \\left( -t \\right)} r \\\\ &amp;amp; = \\left( - \\frac{d}{dt} \\right) \\left( - \\frac{d}{dt} \\right) r \\\\ &amp;amp; = \\frac{d}{dt} \\frac{d}{dt} r \\\\ &amp;amp; = \\frac{d^2 r}{dt^2}\\end{align}\\]Pardon the loose notation, but you get the idea. The left hand side of the law of gravitation, containing the acceleration term, remains invariant under time reversal! And the right hand side does not use time explicitly, so it too remains invariant.Thus, gravity theoretically doesn’t change a bit if we reverse time. Except, motion does. The motion of a body is determined by the force field it is placed in, here described by Newton’s law of gravitation. But motion is also determined by initial conditions, which in this case are the initial position and velocity of the apple relative to the earth.Initial conditionsIf the ball is placed at an initial distance of \\(r=r_0\\) from the centre of mass of the earth, reversing time does not change the way we measure \\(r\\) or \\(r_0\\). Therefore, the initial positions of bodies in a gravitational system are invariant under time reversal.But when we express \\(r\\) as a function of time and compute velocity, time reversal flips the sign of velocity as it is a first-order time derivative of position:\\[\\frac{dr}{d \\left( -t \\right)} = - \\frac{dr}{dt}\\]Hence, the initial velocities of bodies in a gravitational system are reversed under time reversal.ResultTo summarize the above results, the parameters required to predict the motion of an apple near the earth change in the following manner under time reversal: The force field of gravity remains invariant. The initial positions remain invariant. The initial velocities are reversed. This means that if we reverse time, an apple will fall all right to the ground. However, if the apple were thrown downward with an initial velocity, reversing time amounts to throwing the ball upward, so it first rises up and ultimately falls as gravity still acts in the same direction (downward). And vice-versa, if the ball were thrown upward in the forward direction of time instead.General relativityA slightly different time symmetryLike Newtonian gravity, general relativity (GR) is time symmetric. Formally, this means that the motion obtained by reversing time is still a valid motion in the original system, even though this new motion may not be identical to the original one, as we have seen.Time symmetry works differently for GR in general, as compared to Newtonian gravity, which is a special case of GR. In GR, gravitational force fields are replaced by the metric tensor, and they do change under time reversal.The metric tensorThe metric tensor \\(\\pmb{g}\\) provides a complete description of a spacetime (at least in Riemannian/pseudo-Riemannian geometry). Its components in a holonomic basis are defined as,\\[g_{\\mu \\nu} = \\left\\langle \\partial_\\mu, \\partial_\\nu \\right\\rangle\\]where \\(\\left\\{ \\partial_\\mu \\right\\}\\) is the coordinate basis for the tangent space at each event in spacetime. \\(\\left\\langle \\right\\rangle\\) is the inner product, a generalization of the familiar dot product.Under time reversal \\(t \\mapsto - t\\), \\(\\partial_0 \\mapsto - \\partial_0\\). As a result,\\[\\begin{align}g_{00} &amp;amp; \\mapsto \\left\\langle - \\partial_0, - \\partial_0 \\right\\rangle = \\left\\langle \\partial_0, \\partial_0 \\right\\rangle = g_{00} \\\\g_{ij} &amp;amp; \\mapsto \\left\\langle \\partial_i, \\partial_j \\right\\rangle = g_{ij} \\\\g_{i0}=g_{0i} &amp;amp; \\mapsto \\left\\langle \\partial_i, - \\partial_0 \\right\\rangle = - \\left\\langle \\partial_i, \\partial_0 \\right\\rangle = - g_{i0}\\end{align}\\]Therefore, under time reversal, only the spacetimelike components of the metric tensor, \\(g_{i0}\\) are reversed. Nevertheless, the new metric is still a valid solution of the original field equations (which do not change, by virtue of being second-order partial differential equations).Since the metric changes under time reversal, so does the motion of bodies in the gravitational system, even if we ignore the changes in initial conditions.Initial conditionsIn GR, the initial conditions required to predict the evolution of a dynamic metric are the initial values \\(g_{\\mu \\nu}\\) and its first derivatives, \\(\\partial_\\rho g_{\\mu \\nu}\\). From the previous subsection, we know that the spacetimelike part of the metric is reversed under time reversal. This is a key difference from the Newtonian case, where initial positions had remained invariant.Naturally, the first derivatives of the metric change too, along with a sign inversion for the time derivative.Newtonian limitNewtonian gravity, in the framework of GR, is a weak-field, spherically symmetric vacuum solution. A characteristic of spherical symmetry is that it is described by orthogonal coordinate systems. This sets the off-diagonal, or spacetimelike components \\(g_{i0}\\) of the metric tensor to \\(0\\).Under time reversal, the spacetimelike components of the metric tensor switch signs as we have seen. But since they are zero now, they remain invariant. This explains why a Newtonian gravitational field remains exactly the same even if time is reversed.Modified and quantum gravityWhat happens to spacetime under time reversal if we don’t restrict ourselves to the Einstein field equations? The metric tensor plays a crucial role in every potentially correct alternative formulation. Thus, one expects that conceptually, time reversal works in the same way in these theories.However, things get interesting if we consider formulations of GR in which not only the metric, but other objects characterize the geometry of spacetime too. For example, there are scalar-tensor theories, where there are one or more free scalar parameters (such as a spatially and temporally variable universal gravitational constant in Brans-Dicke theory, though the theory gives results inconsistent with observations).Then, there are vector-tensor theories and even a scalar-tensor-vector gravity.With such considerations in mind, there is no general answer to the question of how spacetime changes under time reversal. In most cases, it is arbitrary, depending on the scalar or vector field concerned.But as of now (2021), Einstein’s GR is the reasonably correct classical model of gravitation. Speaking of classical models, things may take a turn when a quantum theory of gravitation is discovered. It is still not certain whether the metric will still play a crucial role in a consistent theory of quantum gravity, or if its time-symmetric nature will be retained in whatever replaces the metric. ‘Time’ will tell." }, { "title": "Demystifying the Definition Of a Covector Basis", "url": "/tempus-spatium/covector-basis-definition/", "categories": "tensor calculus", "tags": "dual vectors, tensors", "date": "2021-10-23 00:00:00 -0400", "snippet": "A covector basis \\(\\left\\{ \\pmb{\\theta}^i \\right\\}\\) is defined to act on the corresponding vector basis \\(\\left\\{ \\pmb{e}_j \\right\\}\\) in the manner,\\[\\pmb{\\theta}^i \\left( \\pmb{e}_j \\right) = \\delta^i_{\\phantom{i} j}\\]Where \\(\\delta^i_{\\phantom{i} j}\\) represents the Kronecker delta. But where does the above definition even come from? Well, turns out it’s not so mysterious after all.Say a covector \\(\\pmb{\\phi}\\) acts on a vector \\(\\pmb{x}\\). In the component form,\\[\\pmb{\\phi} \\left( \\pmb{x} \\right) = \\left( \\phi_i \\: \\pmb{\\theta}^i \\right) \\left( x^j \\: \\pmb{e}_j \\right)\\]By multilinearity,\\[\\pmb{\\phi} \\left( \\pmb{x} \\right) = \\phi_i \\: \\pmb{\\theta}^i \\left( \\pmb{e}_j \\right) \\: x^j\\]We want the result of \\(\\pmb{\\phi} \\left( \\pmb{x} \\right)\\) to be invariant. The simplest way to do so is to define it to be a scalar of the form \\(\\phi_j \\: x^j\\). Therefore,\\[\\begin{align}\\phi_i \\: \\pmb{\\theta}^i \\left( \\pmb{e}_j \\right) \\: x^j &amp;amp; = \\phi_j \\: x^j \\\\\\phi_i \\: \\pmb{\\theta}^i \\left( \\pmb{e}_j \\right) \\: x^j &amp;amp; = \\phi_i \\: \\delta^i_{\\phantom{i} j} \\: x^j\\end{align}\\]By linear independence of the components involved, we can cancel out like terms on both sides of the above equation, and are left with the required result,\\[\\pmb{\\theta}^i \\left( \\pmb{e}_j \\right) = \\delta^i_{\\phantom{i} j}\\]" }, { "title": "Algebra Done Tensorially: Part 2 (Algebras Over Fields)", "url": "/tempus-spatium/algebras-over-fields/", "categories": "representation theory", "tags": "algebras, tensors, bilinear products", "date": "2021-10-23 00:00:00 -0400", "snippet": "Welcome to Part 2 of ‘Algebra Done Tensorially’. If you haven’t already done so, make sure to check out the previous post, Part 1 (Bilinear Products) before reading this post :) I will start right from where we stopped in Part 1. Parts Topics Part 1 (Bilinear Products) tensors, bilinear products Part 2 (Algebras Over Fields) linear maps, algebra, degrees of freedom Part 3 (Complex Numbers and Quaternions) complex numbers, quaternions, gamma matrices Part 4 (Clifford Algebras) in progress Part 5 (Lie Algebras) in progress Recap: bilinear products as linear mapsWe had learnt in the previous post that passing only one argument to a bilinear product makes it a linear map on the vector space characterized by the remaining argument. In the component form, given a tensor \\(\\Phi^I_{\\phantom{I} J}\\) and bilinear product \\(B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N}\\),\\[\\Phi^I_{\\phantom{I} J} \\: B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} = \\Lambda^M_{\\phantom{M} K} \\Lambda^{L}_{\\phantom{L} N}\\]Recall that we are using the tuple index notation, where capital letters for indices represent tuples, e.g. \\(I \\equiv i_1 i_2 \\dots i_p\\). The only exception to this rule lies in the notation for the Jacobian, where \\(\\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} = \\Lambda^{i_1^\\prime}_{\\phantom{i_1^\\prime} i_1} \\Lambda^{i_2^\\prime}_{\\phantom{i_2^\\prime} i_2} \\dots \\Lambda^{i_p^\\prime}_{\\phantom{i_p^\\prime} i_p}\\).You will have noticed that generally, a Jacobian is of the form \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\), since it maps the unprimed vector space to the primed vector space. In contrast, there are no primed indices in the component form for a linear map constructed from a bilinear product. Let us fix this little ambiguity,\\[\\Phi^K_{\\phantom{K} L} \\: B^{L \\phantom{K} J \\phantom{I} I^\\prime}_{\\phantom{L} K \\phantom{J} I \\phantom{I^\\prime} J^\\prime} = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime}\\]However, the components of the bilinear product now have both primed and unprimed indices. This can be resolved by transforming the primed indices to unprimed ones,\\[\\begin{align}\\Phi^K_{\\phantom{K} L} \\: B^{L \\phantom{K} J \\phantom{I} M}_{\\phantom{L} K \\phantom{J} I \\phantom{M} N} &amp;amp; = \\Lambda^{M}_{\\phantom{M} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} N} \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime} \\\\\\Phi^K_{\\phantom{K} L} \\: B^{L \\phantom{K} J \\phantom{I} M}_{\\phantom{L} K \\phantom{J} I \\phantom{M} N} &amp;amp; = \\Delta^M_{\\phantom{M} I} \\Delta^J_{\\phantom{J} N}\\end{align}\\]This is very similar to the original equation, but with the Kronecker delta in place of the Jacobian, which isn’t surprising as a map from unprimed to unprimed indices is strictly speaking, an identity map.However, the ‘strictly unprimed’ form is not as interesting as the one with both unprimed and primed indices, as we shall see. To take that route, we must modify our notion of a bilinear product. Since it will have both unprimed and primed indices, the components of a bilinear product no longer make sense in a single coordinate system. Just like the Jacobian, its components depend on both the sets of coordinates we are mapping between.J tensorDefinitionIn the component representation of bilinear products as linear maps, we have a product of Jacobian components on the right hand side:\\[\\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime} = \\Lambda^{i_1^\\prime}_{\\phantom{i_1^\\prime} i_1} \\dots \\Lambda^{i_p^\\prime}_{\\phantom{i_p^\\prime} i_p} \\Lambda^{j_1}_{\\phantom{j_1} j_1^\\prime} \\dots \\Lambda^{j_q}_{\\phantom{j_q} j_q^\\prime}\\]Does the above expression transform like a tensor? To answer this question, let us first define a tensor \\(\\pmb{J}\\) so that its components are the same as the above product of Jacobian components,\\[\\pmb{J} = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime} \\: \\pmb{E}_{I^\\prime J} \\pmb{\\Theta}^{I J^\\prime}\\]The components of the above tensor are as expected,\\[J^{I^\\prime \\phantom{I} J}_{\\phantom{I^\\prime} I \\phantom{J} J^\\prime} = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime}\\]As usual, capital letters for indices represent their tuples, except for the Jacobian. To verify that the above components transform like a tensor, instead of doing so manually, we can transform all primed indices to unprimed ones and verify that \\(\\pmb{J}\\) is a tensor. From the previous such endeavour, we know that doing so means replacing Jacobians by Kronecker deltas,\\[J^{I \\phantom{J} K}_{\\phantom{I} J \\phantom{K} L} = \\Delta^{I}_{\\phantom{I} J} \\Delta^{K}_{\\phantom{K} L}\\]Since an identity map is always invariant, \\(\\pmb{J}\\) is a tensor.BasisWe have seen that the basis for a J tensor is of the form \\(\\pmb{E}_{I^\\prime J} \\pmb{\\Theta}^{I J^\\prime}\\). However, we can use bilinear products to construct a different basis for J tensors:\\[\\begin{align}\\pmb{J} &amp;amp; = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime} \\: \\pmb{E}_{I^\\prime J} \\pmb{\\Theta}^{I J^\\prime} \\\\ &amp;amp; = \\Phi^K_{\\phantom{K} L} \\: B^{L \\phantom{K} J \\phantom{I} I^\\prime}_{\\phantom{L} K \\phantom{J} I \\phantom{I^\\prime} J^\\prime} \\: \\pmb{E}_{I^\\prime J} \\pmb{\\Theta}^{I J^\\prime} \\\\ &amp;amp; = \\Phi^K_{\\phantom{K} L} \\pmb{\\Theta}^{I^\\prime} \\pmb{E}_{J^\\prime} \\left[ \\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) \\right] \\pmb{E}_{I^\\prime J} \\pmb{\\Theta}^{I J^\\prime} \\\\ &amp;amp; = \\Phi^K_{\\phantom{K} L} \\: \\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) \\pmb{E}_J \\pmb{\\Theta}^I\\end{align}\\]Therefore, in the basis \\(\\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) \\pmb{E}_J \\pmb{\\Theta}^I\\), the coordinates of \\(\\pmb{J}\\) are \\(\\Phi^{K}_{\\phantom{K} L}\\).Note that the basis above is not a covariant one.As a vector spaceSince \\(\\pmb{J}\\) has a basis \\(\\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) \\pmb{E}_J \\pmb{\\Theta}^I\\), a corresponding vector space can be constructed from the span of the basis. 1 Elements belonging to the vector space, by definition, can be added with each other and multiplied by scalars:\\[\\begin{align}\\sum_n \\pmb{J}_{\\left( n \\right)} &amp;amp; = \\sum_n \\Phi^{\\phantom{\\left( n \\right)} K}_{\\left( n \\right) \\phantom{K} L} \\: \\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) \\pmb{E}_J \\pmb{\\Theta}^I \\\\\\lambda \\cdot \\pmb{J} &amp;amp; = \\left( \\lambda \\: \\Phi^K_{\\phantom{K} L} \\right) \\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) \\pmb{E}_J \\pmb{\\Theta}^I\\end{align}\\]The subscript \\(n\\) has been enclosed in brackets to remind us that it labels different J tensors, not any index.AlgebraDefinitionAn algebra \\(\\left( U, \\mathcal{B} \\right)\\) is a vector space \\(U\\) equipped with a bilinear product \\(\\mathcal{B}:U \\mapsto U\\).For our interests, \\(U\\) is the vector space corresponding to \\(J\\) and \\(\\mathcal{B}\\) is a bilinear product on the tensors associated with \\(J\\).CharacterizationAn algebra is characterized by any two of the following three objects: A vector space parameterized in a basis \\(\\Phi^K_{\\phantom{K} L}\\) by components \\(\\pmb{E}_K \\pmb{\\Theta}^L\\) A bilinear operator \\(\\pmb{\\mathcal{B}}\\) whose action on basis vectors is defined by its components, \\[\\pmb{\\mathcal{B}} \\left( \\pmb{E}_K \\pmb{\\Theta}^L, \\pmb{E}_I \\pmb{\\Theta}^J \\right) = B^{L \\phantom{K} J \\phantom{I} I^\\prime}_{\\phantom{L} K \\phantom{J} I \\phantom{I^\\prime} J^\\prime} \\: \\pmb{E}_{I^\\prime} \\pmb{\\Theta}^{J^\\prime}\\] The Jacobian \\(\\pmb{\\Lambda}\\) for the unprimed and primed coordinates, or equivalently, the corresponding J tensor \\(\\pmb{J}\\)The reason we need only two of the above objects is that the three objects have a multilinear relationship, allowing any unknown object to be determined from the remaining two when they are known:\\[\\Phi^K_{\\phantom{K} L} B^{L \\phantom{K} J \\phantom{I} I^\\prime}_{\\phantom{L} K \\phantom{J} I \\phantom{I^\\prime} J^\\prime} = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^{J}_{\\phantom{L} J^\\prime} = J^{I^\\prime \\phantom{I} J}_{\\phantom{I^\\prime} I \\phantom{J} J^\\prime}\\]Implicit relationshipsIf two of the three objects that can be used to characterize an algebra have a multilinear relationship of their own (an ‘implicit relationship’), then one necessarily needs only one object to characterize the algebra, as the other two can be obtained from the usual multilinear relationship and the implicit relationship.Degrees of freedomDefinitionThe degrees of freedom of a tensor are its independent components. The number of degrees of freedom of a tensor remains the same no matter the basis.For example, consider (without loss of generality) a rank-2 tensor whose components in some basis are \\(T^i_{\\phantom{i} j} = T^i_{\\phantom{i} f \\left( i \\right)}\\). Since the second index, \\(j = f \\left( i \\right)\\) is related to \\(i\\), there is only one independent index. If we transform the coordinates to a primed frame, this fact isn’t changed,\\[\\begin{align}T^{i^\\prime}_{\\phantom{i^\\prime} j^\\prime} &amp;amp; = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} T^i_{\\phantom{i} j} \\Lambda^{j}_{\\phantom{j} j^\\prime} \\\\ &amp;amp; = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} T^i_{\\phantom{i} f \\left( i \\right)} \\Lambda^{f \\left( i \\right)}_{\\phantom{f \\left( i \\right)} f \\left( i^\\prime \\right)}\\end{align}\\]Representation theoryRepresentation theory is a discipline in which algebra is studied by representing elements of algebraic structures as multilinear relationships.We will represent \\(\\pmb{J}\\) as a tensor in an implicit basis. Our little ‘magic trick’ will be to identify the degrees of freedom in \\(J^{I^\\prime \\phantom{I} J}_{\\phantom{I^\\prime} I \\phantom{J} J^\\prime}\\) with a vector space parameterized by \\(\\Phi^K_{\\phantom{K} L}\\). All remaining components of \\(\\pmb{J}\\) will be some function of some component(s) of \\(\\pmb{\\Phi}\\).Thus, we are establishing an implicit relationship between \\(\\pmb{J}\\) and \\(\\pmb{\\Phi}\\), thereby reducing the characterization of algebra to only one object of the three we have seen.The JacobianGeometry is an important aspect of algebra. We will formally define it in the next post, but until then, it is useful to know that the geometry of an algebra is determined by its Jacobian. In representation theory, the degrees of freedom of the Jacobian carry all the information necessary to determine the geometry of an algebra.It had been said in the previous subsection that we are considering the degrees of freedom of a J tensor. Since it is built using the Jacobian in a reversible manner, the number of degrees of freedom remain the same in both. But the Jacobian, a rank-2 tensor, can have no more than \\(D^2\\) components \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\), and hence those many degrees of freedom, given a \\(D\\) dimensional vector space.This means that to represent \\(\\pmb{\\Lambda}\\) by \\(\\pmb{\\Phi}\\) in an implicit basis, \\(\\pmb{\\Phi}\\) need not have a higher rank than that of the Jacobian, 2, as higher-rank terms become redundant! Furthermore, even rank-2 representations are redundant, as we already have the components of the Jacobian in its rank-2 tensor.This severe restriction implies that it is useful to only study algebras whose underlying Jacobian in \\(D\\) dimensions has \\(D\\) degrees of freedom. In other words, \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\) must be characterized entirely by some vector \\(\\phi^k\\) (or its dual).Vector algebraStructureAs per the restriction of algebra to vectors (or covectors) to reduce redundancy, we will require one of the following elements to characterize a vector algebra: A Jacobian \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\). Since we are dealing with a rank-1 algebra i.e. vector algebra, the Jacobian is the J tensor for the algebra. A vector \\(\\phi^k\\) which packs the degrees of freedom of \\(\\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\). A bilinear product \\(\\pmb{\\mathcal{B}}\\) which operates on basis vectors as, \\(\\pmb{\\mathcal{B}} \\left( \\pmb{e}_k, \\pmb{e}_i \\right) = B^{i^\\prime}_{\\phantom{i^\\prime} ki} \\pmb{e}_{i^\\prime}\\). As usual, the multilinear relationship between the above elements is,\\[\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} ki} = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i}\\]And the action of the linear map above on a vector \\(x^i\\) is,\\[\\phi^k B^{i^\\prime}_{\\phantom{i^\\prime} ki} x^i = \\Lambda^{i^\\prime}_{\\phantom{i^\\prime} i} x^i = x^{i^\\prime}\\]CharacterizationNow that we know one of the three usual objects will be required to characterize a vector algebra, which one do we choose?An algebra is described by its underlying Jacobian. Therefore, the form of the Jacobian is a sensible parameter for an algebraic structure. Due to the whole degrees of freedom jargon, the form of the vector space representing the Jacobian comes for free. This means that to complete our description of an algebra, we must find the bilinear product from the Jacobian.Wrapping upThis completes the basic concepts we will require to work with algebras tensorially. Examples solidifying these ideas will be seen in the next post, Algebra Done Tensorially: Part 3 (Complex Numbers and Quaternions). Then, we will generalize the results to a family of algebras, in Algebra Done Tensorially: Part 4 (Clifford Algebras). Technically, what we’re constructing here is a vector bundle \\(\\pmb{J}\\) parameterized by the vector space \\(\\pmb{\\Phi}\\). &amp;#8617; " }, { "title": "Algebra Done Tensorially: Part 1 (Bilinear Products)", "url": "/tempus-spatium/bilinear-products/", "categories": "representation theory", "tags": "bilinear products, tensors, algebras", "date": "2021-10-18 00:00:00 -0400", "snippet": "Welcome to this five-part series of posts: Parts Topics Part 1 (Bilinear Products) tensors, bilinear products Part 2 (Algebras Over Fields) linear maps, algebra, degrees of freedom Part 3 (Complex Numbers and Quaternions) complex numbers, quaternions, gamma matrices Part 4 (Clifford Algebras) in progress Part 5 (Lie Algebras) in progress In this first post, I will be exploring bilinear products using tensor analysis. Since tensors will be used extensively in this series, I thought it best to quickly recap their working. Note that this is not appropriate as an introduction to tensors. If you haven’t already studied tensors, I’d recommend watching this excellent playlist by YouTuber Eigenchris.If you are already aware of how tensors work, skip to bilinear products.Notation All tensorial objects are written in boldface. This includes basis vectors and covectors, as they remain invariant under a change of coordinates (what really changes is the labels we associate with our choice of basis). Dummy indices are summed over, according to the Einstein summation convention. Bilinear products are written in calligraphic font to distinguish them from ordinary tensors. Tuples of indices are represented by their capital letters, even if the indices belong to different tensors, in which case the tensors are being multiplied in some form, such as through the tensor product, or by multiplying the components. Jump to tuple index notation to understand this better. TensorsDefinitionGiven a vector space \\(V\\) and its dual space \\(V^\\text{*}\\), a rank \\(\\left( p, q \\right)\\) tensor is a multilinear map \\(\\pmb{T} \\in \\underset{i=1}{\\overset{p}{\\bigotimes}} V \\underset{j=1}{\\overset{q}{\\bigotimes}} V^\\text{*}\\).This means that given a covector basis \\(\\left\\{ \\pmb{\\theta}^i \\right\\}\\) and vector basis \\(\\left\\{ \\pmb{e}_j \\right\\}\\), if \\(\\chi_{i_1} \\pmb{\\theta}^{i_1}, \\psi_{i_2} \\pmb{\\theta}^{i_2}, \\dots, \\omega_{i_p} \\pmb{\\theta}^{i_p} \\in V^\\text{*}\\) and \\(u^{j_1} \\pmb{e}_{j_1}, v^{j_2} \\pmb{e}_{j_2} , \\dots, w^{j_q} \\pmb{e}_{j_q} \\in V\\),\\[\\pmb{T} \\left( \\chi_{i_1} \\pmb{\\theta}^{i_1}, \\psi_{i_2} \\pmb{\\theta}^{i_2}, \\dots, \\omega_{i_p} \\pmb{\\theta}^{i_p}, u^{j_1} \\pmb{e}_{j_1}, v^{j_2} \\pmb{e}_{j_2}, \\dots, w^{j_q} \\pmb{e}_{j_q} \\right) = \\chi_{i_1} \\psi_{i_2} \\dots \\omega_{i_p} \\: \\pmb{T} \\left( \\pmb{\\theta}^{i_1}, \\pmb{\\theta}^{i_2}, \\dots, \\pmb{\\theta}^{i_p}, \\pmb{e}_{j_1}, \\pmb{e}_{j_2}, \\dots, \\pmb{e}_{j_q} \\right) u^{j_1} v^{j_2} \\dots w^{j_q}\\]ComponentsWith respect to the previous section, for a tensor \\(\\pmb{T}\\), its components \\(T^{i_1 \\dots i_p}_{\\phantom{i_1 \\dots i_p} j_1 \\dots j_q}\\) are defined in the manner,\\[\\pmb{T} \\left( \\pmb{\\theta}^{i_1}, \\cdots, \\pmb{\\theta}^{i_p}, \\pmb{e}_{j_1}, \\dots, \\pmb{e}_{j_q} \\right) = T^{i_1 \\dots i_p}_{\\phantom{i_1 \\dots i_p} j_1 \\dots j_q} \\: \\underset{a=1}{\\overset{p}{\\bigotimes}} \\pmb{e}_{i_a} \\underset{b=1}{\\overset{q}{\\bigotimes}} \\pmb{\\theta}^{j_b}\\]The covector basis is defined to act on the vector basis and vice-versa as:\\[\\pmb{\\theta}^i \\left( \\pmb{e}_j \\right) = \\pmb{e}_j \\left( \\pmb{\\theta}^i \\right) = \\delta^i_{\\phantom{i}_j}\\](The logic behind the above definition is explained in another post.)Assuming that covectors act on vectors only and vice-versa under the tensor product, we can extend the action to multiple copies of basis vectors and covectors so that,\\[\\underset{a=1}{\\overset{p}{\\bigotimes}} \\pmb{\\theta}^{i_a} \\underset{b=1}{\\overset{q}{\\bigotimes}} \\pmb{e}_{j_b} : \\underset{a=1}{\\overset{p}{\\bigotimes}} V \\underset{b=1}{\\overset{q}{\\bigotimes}} V^* \\mapsto \\mathcal{I} \\left[ V^{p+q} \\right] = \\mathcal{I} \\left[ \\left( V^* \\right)^{p+q} \\right]\\]where \\(\\mathcal{I}\\) is the identity map. Now writing the above explicitly, we get,\\[\\underset{a=1}{\\overset{p}{\\bigotimes}} \\pmb{\\theta}^{i_a} \\underset{b=1}{\\overset{q}{\\bigotimes}} \\pmb{e}_{j_b} \\left[ \\underset{a=1}{\\overset{p}{\\bigotimes}} \\pmb{e}_{k_a} \\underset{b=1}{\\overset{q}{\\bigotimes}} \\pmb{\\theta}^{l_b} \\right] = \\prod_{a=1}^p \\delta^{i_a}_{\\phantom{i_a} k_a} \\prod_{b=1}^q \\delta^{l_b}_{\\phantom{l_b} j_b}\\]The components of a tensor can now be written as,\\[T^{i_1 \\dots i_p}_{\\phantom{i_1 \\dots i_p} j_1 \\dots j_q} = \\underset{a=1}{\\overset{p}{\\bigotimes}} \\pmb{\\theta}^{i_a} \\underset{b=1}{\\overset{q}{\\bigotimes}} \\pmb{e}_{j_b} \\left[ \\pmb{T} \\left( \\pmb{\\theta}^{i_1}, \\cdots, \\pmb{\\theta}^{i_p}, \\pmb{e}_{j_1}, \\dots, \\pmb{e}_{j_q} \\right) \\right]\\]To simplify our lives, let us represent tuples of indices with their capital letter, e.g. \\(I \\equiv i_1 \\dots i_p\\). Also, let us write any multiplication of tensor components, or tensors, with a capital letter, along with tuples for the indices. Lastly, let the tensor product symbol be omitted altogether. Then, the previous set of equations becomes more readable,\\[\\begin{align}\\pmb{T} \\left( \\pmb{\\Theta}^I, \\pmb{E}_J \\right) &amp;amp; = T^K_{\\phantom{K} L} \\: \\pmb{E}_K \\pmb{\\Theta}^L \\\\T^K_{\\phantom{K} L} &amp;amp; = \\pmb{\\Theta}^K \\pmb{E}_L \\left[ \\pmb{T} \\left( \\pmb{\\Theta}^I, \\pmb{E}_J \\right) \\right]\\end{align}\\]Under a change of coordinates represented by a Jacobian \\(\\Lambda^{j^\\prime}_{\\phantom{j^\\prime} j}\\), the coordinates of a tensor transform as,\\[\\begin{align}T^{i_1^\\prime \\dots i_p^\\prime}_{\\phantom{i_1^\\prime \\dots i_p^\\prime} j_1^\\prime \\dots j_q^\\prime} &amp;amp; = T \\left( \\pmb{\\theta}^{i_1^\\prime}, \\dots, \\pmb{\\theta}^{i_p^\\prime}, \\pmb{e}_{j_1^\\prime}, \\dots, \\pmb{e}_{j_q^\\prime} \\right) \\\\ &amp;amp; = T \\left( \\Lambda^{i_1^\\prime}_{\\phantom{i_1^\\prime} i_1} \\pmb{\\theta}^{i_1}, \\dots, \\Lambda^{i_p^\\prime}_{\\phantom{i_p^\\prime} i_p} \\pmb{\\theta}^{i_p}, \\Lambda^{j_1}_{\\phantom{j_1} j_1^\\prime} \\pmb{e}_{j_1}, \\dots, \\Lambda^{j_q}_{\\phantom{j_q} j_q^\\prime} \\pmb{e}_{j_q} \\right)\\end{align}\\]By multilinearity,\\[\\begin{align}T^{i_1^\\prime \\dots i_p^\\prime}_{\\phantom{i_1^\\prime \\dots i_p^\\prime} j_1^\\prime \\dots j_q^\\prime} &amp;amp; = \\Lambda^{i_1^\\prime}_{\\phantom{i_1^\\prime} i_1} \\dots \\Lambda^{i_p^\\prime}_{\\phantom{i_p^\\prime} i_p} \\: T \\left( \\pmb{\\theta}^{i_1}, \\dots, \\pmb{\\theta}^{i_p}, \\pmb{e}_{j_1}, \\dots, \\pmb{e}_{j_q} \\right) \\Lambda^{j_1}_{\\phantom{j_1} j_1^\\prime} \\dots \\Lambda^{j_q}_{\\phantom{j_q} j_q^\\prime} \\\\T^{i_1^\\prime \\dots i_p^\\prime}_{\\phantom{i_1^\\prime \\dots i_p^\\prime} j_1^\\prime \\dots j_q^\\prime} &amp;amp; = \\Lambda^{i_1^\\prime}_{\\phantom{i_1^\\prime} i_1} \\dots \\Lambda^{i_p^\\prime}_{\\phantom{i_p^\\prime} i_p} \\: T^{i_1 \\dots i_p}_{\\phantom{i_1 \\dots i_p} j_1 \\dots j_q} \\Lambda^{j_1}_{\\phantom{j_1} j_1^\\prime} \\dots \\Lambda^{j_q}_{\\phantom{j_q} j_q^\\prime} \\\\T^{i_1^\\prime \\dots i_p^\\prime}_{\\phantom{i_1^\\prime \\dots i_p^\\prime} j_1^\\prime \\dots j_q^\\prime} &amp;amp; = \\left( \\prod_{a=1}^p \\Lambda^{i_a^\\prime}_{\\phantom{i_a^\\prime} i_a} \\prod_{b=1}^q \\Lambda^{j_b}_{\\phantom{j_b} j_b^\\prime} \\right) T^{i_1 \\dots i_p}_{\\phantom{i_1 \\dots i_p} j_1 \\dots j_q}\\end{align}\\]Or using the tuple notation for indices,\\[T^{I^\\prime}_{\\phantom{I^\\prime} J^\\prime} = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\: T^I_{\\phantom{I} J} \\: \\Lambda^J_{\\phantom{J} J^\\prime}\\]This is known as the tensor transformation law. Note that the notation for the Jacobian is a slight deviation from the regular tuple notation, in that \\(\\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I}\\) is not some tensor \\(\\Lambda^{i_1 \\dots i_p}_{\\phantom{i_1 \\dots i_p} j_1 \\dots j_q}\\), but a product of tensor components, \\(\\Lambda^{i_1^\\prime}_{\\phantom{i_1^\\prime} i_1} \\dots \\Lambda^{i_p^\\prime}_{\\phantom{i_p^\\prime} i_p}\\). This will be the only exception in the tuple notation.InvarianceUnder a change of coordinates \\(\\Lambda^{j^\\prime}_{\\phantom{j^\\prime} j}\\), the components of \\(\\pmb{T}\\) transform but \\(\\pmb{T}\\) itself remains invariant:\\[\\begin{align}\\pmb{T}^\\prime &amp;amp; = T^{I^\\prime}_{\\phantom{I^\\prime} J^\\prime} \\: \\pmb{E}_{I^\\prime} \\pmb{\\Theta}^\\prime \\\\ &amp;amp; = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\: T^I_{\\phantom{I} J} \\: \\Lambda^J_{\\phantom{J} J^\\prime} \\: \\Lambda^K_{\\phantom{K} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} L} \\: \\pmb{E}_K \\pmb{\\Theta}^L \\\\ &amp;amp; = \\Lambda^{I^\\prime}_{\\phantom{I^\\prime} I} \\Lambda^K_{\\phantom{K} I^\\prime} \\Lambda^J_{\\phantom{J} J^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} L} \\: T^I_{\\phantom{I} J} \\: \\pmb{E}_K \\pmb{\\Theta}^L \\\\ &amp;amp; = \\Delta^K_{\\phantom{K} I} \\Delta^J_{\\phantom{J} L} T^I_{\\phantom{I} J} \\: \\pmb{E}_K \\pmb{\\Theta}^L \\\\ &amp;amp; = T^I_{\\phantom{I} J} \\: \\pmb{E}_I \\pmb{\\Theta}^J \\\\ &amp;amp; = \\pmb{T}\\end{align}\\]where \\(\\Delta^K_{\\phantom{K} I} = \\delta^{k_1}_{\\phantom{k_1} i_1} \\dots \\delta^{k_p}_{\\phantom{k_p} i_p}\\). Once again, we see a deviation from the regular tuple notation, of similar kind as in the case of the Jacobian \\(\\pmb{\\Lambda}\\). This is not another exception to the notation; the Kronecker delta \\(\\pmb{\\delta}\\) is a special kind of Jacobian, which maps a vector space to itself.Bilinear ProductsDefinitionGiven a vector space \\(V\\), its dual space \\(V^\\text{*}\\) and rank \\(\\left( p, q \\right)\\) tensors \\(\\pmb{\\Phi}\\) and \\(\\pmb{T}\\), a bilinear product \\(\\mathcal{B}\\) is a bilinear map,\\[\\mathcal{B} \\left( \\pmb{\\Phi}, \\pmb{T} \\right) \\in \\underset{i=1}{\\overset{p}{\\bigotimes}} V \\underset{j=1}{\\overset{q}{\\bigotimes}} V^\\text{*}\\]Thus, \\(\\mathcal{B}\\) linearly maps a pair of tensors belonging to the same tensor space, to another tensor in the same tensor space.As the name suggests, bilinear products are bilinear, i.e. linear in both their arguments,\\[\\begin{align}\\mathcal{B} \\left( \\sum_i \\pmb{\\Phi_i}, \\pmb{T} \\right) &amp;amp; = \\sum_i \\mathcal{B} \\left( \\pmb{\\Phi}_i, \\pmb{T} \\right) \\\\\\mathcal{B} \\left( \\pmb{\\Phi_i}, \\sum_j \\pmb{T_j} \\right) &amp;amp; = \\sum_j \\mathcal{B} \\left( \\pmb{\\Phi}, \\pmb{T}_j \\right) \\\\\\implies \\mathcal{B} \\left( \\sum_i \\pmb{\\Phi_i}, \\sum_j \\pmb{T_j} \\right) &amp;amp; = \\sum_i \\sum_j \\mathcal{B} \\left( \\pmb{\\Phi}_i, \\pmb{T}_j \\right)\\end{align}\\]ComponentsGiven the components of two tensors, by bilinearity, their bilinear product can be expressed as,\\[\\begin{align}\\pmb{\\mathcal{B}} \\left( \\pmb{\\Phi}, \\pmb{T} \\right) &amp;amp; = \\pmb{\\mathcal{B}} \\left( \\Phi^I_{\\phantom{I} J} \\: \\pmb{E}_I \\pmb{\\Theta}^J, T^K_{\\phantom{K} L} \\: \\pmb{E}_K \\pmb{\\Theta}^L \\right) \\\\ &amp;amp; = \\Phi^I_{\\phantom{I} J} \\: \\pmb{\\mathcal{B}} \\left( \\pmb{E}_I \\pmb{\\Theta}^J, \\pmb{E}_K \\pmb{\\Theta}^L \\right) \\: T^K_{\\phantom{K} L}\\end{align}\\]In the context of the above, the components \\(B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N}\\) of \\(\\mathcal{B}\\) may be defined as,\\[\\begin{align}\\pmb{\\mathcal{B}} \\left( \\pmb{E}_I \\pmb{\\Theta}^J, \\pmb{E}_K \\pmb{\\Theta}^L \\right) &amp;amp; = B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} \\: \\pmb{E}_M \\pmb{\\Theta}^N \\\\\\implies B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} &amp;amp; = \\pmb{\\Theta}^M \\pmb{E}_N \\left[ \\pmb{\\mathcal{B}} \\left( \\pmb{E}_I \\pmb{\\Theta}^J, \\pmb{E}_K \\pmb{\\Theta}^L \\right) \\right]\\end{align}\\]It is noteworthy that in the above, odd tuples i.e. \\(I, K, M\\) are indices running from \\(1\\) to \\(p\\), coupled with \\(V\\) ; while even tuples \\(J, L, N\\) run from \\(1\\) to \\(q\\) coupled with \\(V^\\text{*}\\).We know that \\(\\pmb{\\mathcal{B}} \\left( \\pmb{\\Phi}, \\pmb{T} \\right)\\) is a tensor in the same space as its arguments, but that does not immediately justify that the components of \\(\\pmb{\\mathcal{B}}\\) as we defined them transform like tensor components. Instead, we will have to verify that manually,\\[\\begin{align}B^{J^\\prime \\phantom{I^\\prime} L^\\prime \\phantom{K^\\prime} M^\\prime}_{\\phantom{J^\\prime} I^\\prime \\phantom{L^\\prime} K^\\prime \\phantom{M^\\prime} N^\\prime} &amp;amp; = \\pmb{\\Theta}^{M^\\prime} \\pmb{E}_{N^\\prime} \\left[ \\pmb{\\mathcal{B}} \\left( \\pmb{E}_{I^\\prime} \\pmb{\\Theta}^{J^\\prime}, \\pmb{E}_{K^\\prime} \\pmb{\\Theta}^{L^\\prime} \\right) \\right] \\\\ &amp;amp; = \\pmb{\\Theta}^{M^\\prime} \\pmb{E}_{N^\\prime} \\left[ \\pmb{\\mathcal{B}} \\left( \\Lambda^I_{\\phantom{I} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} J} \\: \\pmb{E}_{I} \\pmb{\\Theta}^{J}, \\Lambda^K_{\\phantom{K} K^\\prime} \\Lambda^{L^\\prime}_{\\phantom{L^\\prime} L} \\: \\pmb{E}_{K} \\pmb{\\Theta}^{L} \\right) \\right] \\\\ &amp;amp; = \\pmb{\\Theta}^{M^\\prime} \\pmb{E}_{N^\\prime} \\left[ \\Lambda^I_{\\phantom{I} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} J} \\: \\pmb{\\mathcal{B}} \\left( \\pmb{E}_{I} \\pmb{\\Theta}^{J}, \\pmb{E}_{K} \\pmb{\\Theta}^{L} \\right) \\: \\Lambda^K_{\\phantom{K} K^\\prime} \\Lambda^{L^\\prime}_{\\phantom{L^\\prime} L} \\right] \\\\ &amp;amp; = \\Lambda^I_{\\phantom{I} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} J} \\Lambda^K_{\\phantom{K} K^\\prime} \\Lambda^{L^\\prime}_{\\phantom{L^\\prime} L} \\pmb{\\Theta}^{M^\\prime} \\pmb{E}_{N^\\prime} \\left[ \\pmb{\\mathcal{B}} \\left( \\pmb{E}_{I} \\pmb{\\Theta}^{J}, \\pmb{E}_{K} \\pmb{\\Theta}^{L} \\right) \\right] \\\\ &amp;amp; = \\Lambda^I_{\\phantom{I} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} J} \\Lambda^K_{\\phantom{K} K^\\prime} \\Lambda^{L^\\prime}_{\\phantom{L^\\prime} L} \\left( \\Lambda^{M^\\prime}_{\\phantom{M^\\prime} M} \\Lambda^{N}_{\\phantom{N} N^\\prime} \\pmb{\\Theta}^M \\pmb{E}_N \\right) \\left[ \\pmb{\\mathcal{B}} \\left( \\pmb{E}_{I} \\pmb{\\Theta}^{J}, \\pmb{E}_{K} \\pmb{\\Theta}^{L} \\right) \\right] \\\\ &amp;amp; = \\Lambda^I_{\\phantom{I} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} J} \\Lambda^K_{\\phantom{K} K^\\prime} \\Lambda^{L^\\prime}_{\\phantom{L^\\prime} L} \\Lambda^{M^\\prime}_{\\phantom{M^\\prime} M} \\Lambda^{N}_{\\phantom{N} N^\\prime} \\pmb{\\Theta}^M \\pmb{E}_N \\left[ \\pmb{\\mathcal{B}} \\left( \\pmb{E}_{I} \\pmb{\\Theta}^{J}, \\pmb{E}_{K} \\pmb{\\Theta}^{L} \\right) \\right] \\\\ &amp;amp; = \\Lambda^I_{\\phantom{I} I^\\prime} \\Lambda^{J^\\prime}_{\\phantom{J^\\prime} J} \\Lambda^K_{\\phantom{K} K^\\prime} \\Lambda^{L^\\prime}_{\\phantom{L^\\prime} L} \\Lambda^M_{\\phantom{M} M^\\prime} \\Lambda^{N^\\prime}_{\\phantom{N^\\prime} N} \\: B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N}\\end{align}\\]Thus, the components of \\(\\mathcal{B}\\) indeed transform like that of a tensor! In fact, given a rank \\(\\left( p, q \\right)\\) tensor, the said components represent a rank \\(\\left( 2q+p, 2p+q \\right)\\) tensor.A NoteWe have seen above how \\(B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N}\\) transforms like a tensor. It must be noted though, that the bilinear product formed from these components alone is not a tensor. I.e., the following quantity does not transform like a tensor,\\[\\pmb{\\mathcal{B}} \\left( \\pmb{E}_I \\pmb{\\Theta}^J, \\pmb{E}_K \\pmb{\\Theta}^L \\right) = B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} \\: \\pmb{E}_M \\pmb{\\Theta}^N\\]Looking at the indices, the above map itself (with its components and basis) transforms like the components (not the entire map) of a rank \\(\\left( 2q, 2p \\right)\\) tensor. We can use this fact to construct a tensor,\\[\\begin{align}\\pmb{B} &amp;amp; = \\pmb{\\mathcal{B}} \\left( \\pmb{E}_I \\pmb{\\Theta}^J, \\pmb{E}_K \\pmb{\\Theta}^L \\right) \\pmb{E}_{J L} \\pmb{\\Theta}^{IK} \\\\ &amp;amp; = B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} \\: \\pmb{E}_M \\pmb{\\Theta}^N \\pmb{E}_{J L} \\pmb{\\Theta}^{IK}\\end{align}\\]where \\(\\pmb{E}_{JL} = \\pmb{E}_J \\otimes \\pmb{E}_L\\) and \\(\\pmb{\\Theta}^{IK} = \\pmb{\\Theta}^I \\otimes \\pmb{\\Theta}^K\\). Note that the above tensor is labelled as \\(\\pmb{B}\\), which is different from the calligraphic label \\(\\pmb{\\mathcal{B}}\\) for the tensor generated from two other tensors by a bilinear product.As linear mapsWhen only one argument is passed to a bilinear product, it acts as a linear map on the vector space parameterized by the other argument,\\[\\begin{align}\\mathcal{B} \\left( \\pmb{\\Phi}, \\cdot \\right) \\left( \\sum_j \\pmb{T_j} \\right) &amp;amp; = \\sum_j \\pmb{\\mathcal{B}} \\left( \\pmb{\\Phi}, \\cdot \\right) \\left( \\pmb{T_j} \\right) \\\\\\pmb{\\mathcal{B}} \\left( \\cdot, \\pmb{T} \\right) \\left( \\sum_i \\pmb{\\Phi_i} \\right) &amp;amp; = \\sum_i \\pmb{\\mathcal{B}} \\left( \\cdot, \\pmb{T} \\right) \\left( \\pmb{\\Phi_i} \\right)\\end{align}\\]In the component form,\\[\\begin{align}\\pmb{\\mathcal{B}} \\left( \\pmb{\\Phi}, \\cdot \\right) &amp;amp; = \\pmb{\\mathcal{B}} \\left( \\Phi^{I}_{\\phantom{I} J} \\: \\pmb{E}_I \\pmb{\\Theta}^J, \\cdot \\right) \\\\ &amp;amp; = \\Phi^I_{\\phantom{I} J} \\: B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} \\: \\pmb{E}_M \\pmb{\\Theta}^N \\\\ &amp;amp; = \\Lambda^M_{\\phantom{M} K} \\Lambda^{L}_{\\phantom{L} N} \\: \\pmb{E}_M \\pmb{\\Theta}^N\\end{align}\\]By linear independence of the components of the tensors involved,\\[\\Phi^I_{\\phantom{I} J} \\: B^{J \\phantom{I} L \\phantom{K} M}_{\\phantom{J} I \\phantom{L} K \\phantom{M} N} = \\Lambda^M_{\\phantom{M} K} \\Lambda^{L}_{\\phantom{L} N}\\]In other words, given a basis, a tensor (or more generally, tensor field) and a bilinear product, one can covariantly transform the components of an arbitrary tensor in the same space as the one given. This will be the starting point of the next post in this series, Algebra Done Tensorially: Part 2 (Algebras Over Fields)." }, { "title": "A Tale of Two Quantities", "url": "/tempus-spatium/tale-two-quantities/", "categories": "fundamentals", "tags": "measurement, narrative", "date": "2021-10-15 00:00:00 -0400", "snippet": "The war(Disclaimer: All events narrated in the ‘bar’ are fictional. Any historical inaccuracies are intentional.)Two men walk into a bar. They enter just in time, for it starts raining heavily. One man is rather small. “I’m a point Charge”, says he.The other looks athletic. Almost sprinting, he shouts, “I’m Current”.The two men then glare at each other. Charge’s voice is rather feeble, and Current talks too rapidly. Not surprisingly, much of their argument is inaudible. The situation grows awkward. But it’s raining outside and the crowd indoors must watch where this goes.“There, there, gentlemen!”, calls a cheery looking bartender. “Tell me what ye wanta drink.”“Actually, I’d prefer a smoke”, says Charge. “Photon gas would be fine .”“Hah, that’s for kids, Charge! I’d like some ferrofluid, please.”, announces Current.“Oh yes, you and your magnetic effects. At least I’m a healthy little man who has settled in a good place!”“What’s this fight about, if I may, gentlemen?”, asked the bartender sincerely.“Well, Current here thinks he’s more fundamental than me. Clearly, he’s not! I mean, look at our descriptions! Here”, Charge hands the bartender a sheet of paper.The bartender reads, “Name, Charge. Age, as old as the universe, for charge is conserved. Hm. Nature, scalar. Favourite hobby, staying in equilibrium. Description: charge is an intrinsic property of particles which interact with the electromagnetic field.”“Righto. Now, name, Current. Nature, scalar. Favourite hobby, travelling. Description: Current is the amount of charge passing through a given point in unit time.”“Now you tell us, Mr. Bartender, who sounds like a simpler man to you?”“Err, if ye ask me, it has to be this man, Charge. But Current seems to be a good man too”, adds the Bartender quickly, on seeing Current’s expression (which is ironically a frozen one).“But I’m puzzled. Is this all you two were quarrelling about?”“You see, Mr. Bartender, the contrary was agreed upon a few weeks back in the General Conference on Weights and Measures in Paris.”, said Current with a glowing face.“Yeah, whenever he says that, the heating effect of Current is visible.”, jokes Charge.But the Bartender wasn’t buying this story. “Ye’re playing games with me, aren’t ya?”At this point, a curious bystander steps in, carrying a newspaper in his hand. “They’re right, Mr. Bartender. Take a look for yourself.”Breaking news 1“Now that’s confusing! They’ve defined Current all in terms of Charge and yet Current is a fundamental quantity!”, says a struggling Mr. Bartender.Interlude: Planck unitsWhy Planck units?No matter how abstract physics gets, measurement is an indispensable aspect of physics. Even in theoretical physics, where one seldom needs to pick up actual measuring apparatus, things like units and quantities and their definitions, etc. are important.Consider: a mathematician is free to say, “let the length of a line segment be \\(x\\)”. But to a physicist, that statement isn’t enough. He or she would ask, “\\(x\\) what? Metres? Feet? Planck units?”Anyway, the point is that even in works of theory, a physicist must carefully consider what units we use and how they are defined.Now, have you ever wondered why an atom is so small? For example, the hydrogen atom is only about \\(1.2 \\times 10^{-10} \\: \\text{m}\\) wide, ignoring the intricacies of quantum mechanics. But what if I were to tell you that you’ve been asking the wrong question?The right question is, “why is \\(1 \\: \\text{m}\\) so big relative to a hydrogen atom?”, as Leonard Susskind points out in his excellent book, Quantum Mechanics: The Theoretical Minimum. The answer is that \\(1 \\: \\text{m}\\) is a human-friendly unit, i.e. it is convenient in daily human life; and, it takes billions of billions of atoms to create an organism as complex as a human. Not surprisingly, units pertaining to atomic phenomena will seem to be teeny-weeny to us giants, so to speak.But nature and physics do not seem to care whether humans are present to observe it, or not. From the perspective of the universe, our common units are arbitrary. Since the job of a physicist is to describe nature fundamentally, he or she often faces a situation where a more ‘natural’ system of units is required. That is, units that are tied to fundamental things in nature.Nature’s “… and these are a few of my favourite things”One of the most important physicists ever, German physicist Max Planck, identified that in the light of choosing natural units, one may consider the following fundamental phenomena, characterized by their own fundamental constants: Atoms. Planck had dirtied his own hands in some quantum mechanics, which describes atoms extremely well. As he had discovered (with quite some credit going to Einstein, Bohr and other pioneers of the ‘old quantum theory’), a constant now named after him, Planck’s constant, plays a very important role in nature. Namely, it encodes nature’s way of quantizing physical quantities. Planck picked the original constant divided by twice of pi, \\(\\frac{h}{2 \\pi}\\), as it appears an awful lot in quantum physics. This is called reduced Planck’s constant \\(\\hbar\\), or simply Planck’s constant in short. Light. Light is an evergreen topic of interest in physics, right from Newton’s time. Even during Planck’s rise, there had been quite a buzz about light and Einstein’s profound insight into it, culminating into the special theory of relativity. Einstein’s coup was to realize that the speed of light in vacuum is invariant, no matter the inertial frame chosen. This speed, written as \\(c\\), is a fundamental property of the universe itself. Generally speaking, it encodes the universe’s tendency to exhibit relativistic effects. Gravitation. The two giants when it comes to discovering the nature of gravitation are Newton and Einstein (admittedly, there are many more geniuses involved but Newton and Einstein were both the mightiest pioneers in the field in their times). Both used the universal gravitational constant \\(G\\), which encodes the strength of gravitational effects in this universe. Information. Information is one of the most fundamental ideas in physics and mathematics. Every system in this universe contains some information, and the more disorder the system has, the more information is contained in it. I.e., the more information is required to characterize the state of that system. Much of 18th and 19th century physics revolved around the related idea of entropy, which is central to thermodynamics. Here, the so-called Boltzmann’s constant \\(k_B\\) plays a crucial part. It encodes the relation between the entropy of a system and its number of microstates corresponding to some macrostate. The Planck unitsThe most basic quantities in physics are: length, mass and time. They are essential to, say, track the motion of objects, which forms one of the most fundamental branches of physics, mechanics.How do we express the units of these quantities if they are to be derived from the fundamental units we saw earlier? We simply use the right dimensions and get what are known as the Planck units,Planck units 2The importance of these units, in Planck’s own words, is, … it is possible to set up units for length, mass, time and temperature, which are independent of special bodies or substances, necessarily retaining their meaning for all times and for all civilizations, including extraterrestrial and non-human ones, which can be called ‘natural units of measure’.Back to the barThe bartender is still confused, standing tightly at one spot. Suddenly, a rattle at the door breaks the silence. The man with the fancy newspaper walks up to the door and opens it. A tall man wearing a hat enters and looks around. Before he can even introduce himself, a delighted Mr. Current squeals,“Oh, but it is Herr Max Planck himself!”Planck looks around him and immediately understands the situation. After all, he had heard of it in the Quanta Magazine. Confidently, he takes ‘charge’ of the situation and produces a black board.“This”, he says, “is a black body”. Before a mesmerized audience can say anything, he starts writing on it with a chalk. “Consider the well-known Coulomb’s law for the electrostatic force between two charges \\(q_1\\) and \\(q_2\\),”\\[F = k_e \\frac{q_1 q_2}{d^2}\\]If we are concerned only with the dimensions of the quantities involved, then we have,\\[q^2 = \\frac{Fd^2}{k_e}\\]Recalling Newton’s own gravitational law, and plugging in for \\(F d^2\\),\\[q^2 = G \\frac{ m^2}{k_e}\\]where \\(k_e\\) is Coulomb’s constant. After applying my units,\\[q_P^2 = \\hbar c\\]Thus, \\(q_P = \\sqrt{\\hbar c}\\). This is awkward. Surely, the “square root of Planck’s constant times the speed of light” does not amount to much. But observe what happens if I work with current instead:\\[I_P = \\frac{q_P}{t_P} = \\frac{c^3}{\\sqrt{G}}\\]Did you know that \\(\\sqrt{G}\\) does have a physical interpretation? The escape velocity \\(v_e\\) of a spherical body of mass \\(M\\) and radius \\(R\\) is given by,\\[v_e = \\sqrt{\\frac{2GM}{R}}\\]Thus, \\(\\sqrt{G}\\) represents \\(\\frac{1}{\\sqrt{2}}\\) times the escape velocity of a spherical body whose mass is numerically equal to its radius in the system of units concerned.Presto, current is more fundamental than charge when it comes to writing it down in terms of nature’s deepest facts.”Saying this, the legendary physicist walks to the counter and picks up a drink. There is a big round of applause, the bartender being the loudest. For the first time in a really long time, Charge and Current smile at each other and shake hands.Who wins?So, what is the end take from the tale of two quantities — charge and current? Well, what if I were to tell you that it was a tale indeed, for the practical, or common reason for choosing current as a fundamental quantity is very different?It simply lies in the fact that current can be measured more accurately than charge. Current is measured using its magnetic effect on a magnetometer. To measure static charge accurately, one always does so using current in some form or the other. Be it measuring the capacitance and voltage of the body storing the charge, or transferring the charge to an instrument and integrating the current over time, the measurement of charge is a dynamic process.This relative ease of measuring current is a natural fact. It has to do with the very nature of measurement: when we measure a quantity, we disturb the object associated with it. Disturbing charges, more often than not, leads to current in nature.But do not be disheartened. The tale you read is fictional, but not entirely baseless, if we remove the purely fictional elements. The main logic in it is that \\(\\sqrt{G}\\) has a more straightforward physical interpretation than \\(\\sqrt{\\hbar c}\\). Is this a natural fact? It is hard to tell. What we do know, after Einstein, is that gravitation has a deeply geometric working in nature. Gravity operates through spacetime itself, rather than through quantum fields, which makes it a unique phenomenon. Even if there is no matter or energy in a large region of space, gravitation can exist in the form of gravitational waves. The point is, both Planck charge and Planck current involve ‘awkward’ units, i.e. square roots of fundamental units, but at least \\(\\sqrt{G}\\) has a relatively simple physical interpretation, not due to the human body’s size, but the simplicity of gravitation itself.But there’s another side to the story. You see, there is no clear historical answer for the question of why current is a fundamental quantity in the SI system of units. Sure, the 11 \\(^{th}\\) General Conference on Weights and Measures, 1960, Paris, is well documented. But the minds, the thoughts, of those attending the conference aren’t. We’ll never learn the unique insights and logic these people had in a precise manner. This is why imagination is not insensible here, after all. It is a matter of history, not just of physics. Source: https://en.wikipedia.org/wiki/International_System_of_Units#Base_units &amp;#8617; Source: https://en.wikipedia.org/wiki/Planck_units#History_and_definition &amp;#8617; " }, { "title": "Deriving the Gamma Function from Scratch", "url": "/tempus-spatium/gamma-function-from-scratch/", "categories": "analysis", "tags": "gamma function, Laplace transforms", "date": "2021-10-14 00:00:00 -0400", "snippet": "What is the gamma function about?About 300 years ago, the influential mathematician Leonhard Euler solved the problem of extending the factorial function to non-integers. He originally found an infinite product representation, which he soon expressed in the integral form,\\[\\displaystyle{ z! = \\int_0^1 \\left( - \\ln u \\right)^z du }\\]By the substitution \\(t = - \\ln u\\), one obtains the more common representation of the extended factorial:\\[\\displaystyle{ z! = \\int_0^\\infty t^z e^{-t} dt }\\]Note that we have used the symbol \\(z\\) to denote complex arguments. The above function is defined for all complex numbers, except for negative reals.In complex analysis, one uses the Gamma function more often (which is actually an arbitrary tradition), which in terms of the extended factorial is defined as,\\[\\displaystyle{ \\Gamma \\left( z \\right) = \\left( z-1 \\right)! = \\int_0^\\infty t^{z-1} e^{-t} dt }\\]This celebrated function appears in a lot of places, from complex analysis to quantum mechanics. Hence, it is important to understand how to derive it intuitively. We will derive the extended factorial first, and plug in \\(z-1\\) to get the gamma function. But before that, let us look at some of the properties of the extended factorial, which make it a suitable candidate for continuing the factorial to non-integer arguments.PropertiesThe first important property of the extended factorial is,\\[z! = z \\left( z-1 \\right) ! \\tag{1}\\]This can be seen by using the explicit integral representation of the extended factorial and using integration by parts,\\[\\begin{align}z! &amp;amp; = \\int_0^\\infty t^z e^{-t} dt \\\\ &amp;amp; = t^z \\int_0^\\infty e^{-t} dt - \\int_0^\\infty \\frac{d}{dt} t^z \\left( \\int e^{-t} dt \\right) dt \\\\ &amp;amp; = \\left[ - t^z e^{-t} \\right]_0^\\infty - \\int_0^\\infty zt^{z-1} \\left( -e^{-t} \\right) dt \\\\ &amp;amp; = z \\int_0^\\infty t^{z-1} e^{-t} dt \\\\ &amp;amp; = z \\left( z-1 \\right) !\\end{align}\\]The second important property is that \\(0! = 1\\). This, again, can be derived from the integral representation,\\[\\begin{align}0! &amp;amp; = \\int_0^\\infty t^0 e^{-t} dt \\\\ &amp;amp; = \\int_0^\\infty e^{-t} dt \\\\ &amp;amp; = \\left[ -e^{-t} \\right]_0^\\infty \\\\ &amp;amp; = 1\\end{align}\\]These two properties together allow us to retrieve the regular factorial function defined over the whole numbers (denoted by \\(n\\)),\\[\\begin{align}n! &amp;amp; = n \\left( n-1 \\right) ! \\\\ &amp;amp; = n \\left( n-1 \\right) \\left( n-2 \\right) ! \\\\ &amp;amp; \\vdots \\\\ &amp;amp; = n \\left( n-1 \\right) \\left( n-2 \\right) \\dots 2 \\times 1 \\times 0! \\\\ &amp;amp; = n \\left( n-1 \\right) \\left( n-2 \\right) \\dots 2 \\times 1\\end{align}\\]DerivationStrategyAfter reading till here, you may think that “I get it, the extended factorial has nice properties, but where does it come from?”, and if you do, you’re on the right track.The extended factorial first came from Euler’s mind, so one could investigate how he conceived of it. However, that route isn’t straightforward. It requires a deep understanding of analysis. After all, Euler got his neat equations thanks to spending a lot of time with such problems.Instead, we will begin with the properties of the factorial, which are obvious even from whole number arguments.Consider equation \\(\\left( 1 \\right)\\), which is basically a functional equation for the factorial function. If we can solve for the factorial function so that the functional equation holds even for non-integer arguments, and use the boundary condition \\(0! = 1\\), we have found the extended factorial.Interlude: the Laplace transformThe Laplace transform is an essential tool in the mathematician’s and physicist’s toolbox. It is used to convert differential equations to algebraic equations. This allows us to solve the algebraic equation, ‘inverse Laplace transform’ it and find exact solutions for the original differential equation.The Laplace transform \\(F \\left( s \\right)\\) of a function \\(f \\left( t \\right)\\) is defined as,\\[\\displaystyle{ F \\left( s \\right) = \\mathcal{L} \\left\\{ f \\left( t \\right) \\right\\} \\left( s \\right) = \\int_0^\\infty f \\left( t \\right) e^{-st} dt }\\]Thus, the Laplace transform operator \\(\\mathcal{L}\\) takes in a function \\(f \\left( t \\right)\\) in the time, or \\(t\\) domain and gives a new function \\(F \\left( s \\right)\\) in the \\(s\\) domain.Laplace transforms have some interesting properties, which will deem to be useful for this derivation. Though I will derive the properties used in this post, it could be helpful to run through them yourself in the previous link.Alert readers may have realized by this point that the extended factorial is itself a Laplace transform,\\[z! = \\mathcal{L} \\left\\{ t^z \\right\\} \\left( 1 \\right)\\]Here, \\(t^z\\) is not just a function of \\(t\\), but also of the variable \\(z\\). Similarly, \\(z!\\) is a function of \\(z\\), but not an explicit function of \\(s\\). Hence, for our purposes, we will generalize the operation of the Laplace transform as,\\[\\displaystyle{ U_s \\left( z, t \\right) = \\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right) = \\int_0^\\infty u \\left( z, t \\right) e^{-st} dt }\\]We have used the symbol \\(u\\) for the new argument function of the Laplace transform operator, to remind us that it is a function of both \\(z\\) and \\(t\\).Getting startedA useful property of the Laplace transform is the following,\\[\\displaystyle{ \\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right) = \\frac{1}{s} \\left( \\mathcal{L} \\left\\{ \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\right\\} \\left( s \\right) + \\lim_{t \\to 0^-} u \\left( z, t \\right) \\right) \\tag{2} }\\]Where \\(\\frac{\\partial}{\\partial t}\\) denotes differentiation with respect to the variable \\(t\\), while keeping other variables constant. As usual, the above can be derived by plugging in the explicit expression for the left hand side and transforming it to the right hand side,\\[\\begin{align}\\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right) &amp;amp; = \\int_0^\\infty u \\left( z, t \\right) e^{-st} dt \\\\ &amp;amp; = u \\left( z, t \\right) \\int_0^\\infty e^{-st} dt - \\int_0^\\infty \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\left( \\int e^{-st} dt \\right) dt \\\\ &amp;amp; = \\left[- \\frac{1}{s} u \\left( z, t \\right) e^{-st} \\right]_0^\\infty - \\int_0^\\infty \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\left( - \\frac{1}{s} e^{-st} \\right) dt \\\\ &amp;amp; = \\frac{1}{s} u \\left( z, 0^- \\right) + \\frac{1}{s} \\int_0^\\infty \\frac{\\partial u \\left( z, t \\right)}{\\partial t} e^{-st} dt \\\\ &amp;amp; = \\frac{1}{s} \\left( \\mathcal{L} \\left\\{ \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\right\\} \\left( s \\right) + u \\left( z, 0^- \\right) \\right)\\end{align}\\]Some brute forceLet us compare the equations \\(\\left( 1 \\right)\\) and \\(\\left( 2 \\right)\\),\\[z! = z \\left( z-1 \\right) ! + 0 \\tag{1}\\]\\[\\displaystyle{ \\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right) = \\frac{1}{s} \\mathcal{L} \\left\\{ \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\right\\} \\left( s \\right) + \\frac{1}{s} u \\left( z, 0^- \\right) \\tag{2} }\\]Apparently, the two above equations look very different from each other. But is it possible to establish an exact correspondance between them? As it turns out, yes. Let us make the assumption that \\(z!\\) has an inverse Laplace transform. Then, it can be expressed as a Laplace transform of some function, in the following manner,\\[z! = \\frac{1}{s} \\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right)\\]Those are the first terms in the two equations we are comparing. Further, let,\\[z \\left( z-1 \\right) ! = \\frac{1}{s} \\mathcal{L} \\left\\{ \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\right\\} \\left( s \\right)\\]\\[\\frac{1}{s} u \\left( z, 0^- \\right) = 0\\]From the first two relations,\\[z \\left( z-1 \\right)! = z \\mathcal{L} \\left\\{ u \\left( z-1, t \\right) \\right\\} \\left( s \\right) = \\frac{1}{s} \\mathcal{L} \\left\\{ \\frac{\\partial u \\left( z, t \\right)}{\\partial t} \\right\\} \\left( s \\right)\\]Or explicitly,\\[\\displaystyle{ \\frac{1}{s} \\int_0^\\infty \\frac{\\partial u \\left( z, t \\right)}{\\partial t} e^{-st} dt = z \\int_0^\\infty u \\left( z-1, t \\right) e^{-st} dt }\\]We need to solve for both \\(u \\left( z, t \\right)\\) and \\(s\\). Firstly, we can pull in various free variables in both sides of the equation, into their repsective integrals, since they are independent of the variable over which we are integrating,\\[\\displaystyle{ \\int_0^\\infty \\frac{1}{s} \\frac{\\partial u \\left( z, t \\right)}{\\partial t} e^{-st} dt = \\int_0^\\infty z u \\left( z-1, t \\right) e^{-st} dt }\\]From the above, it is obvious that,\\[\\frac{1}{s} \\frac{\\partial u \\left( z, t \\right)}{\\partial t} = z u \\left( z-1, t \\right)\\]There is no general way to solve the above functional equation. However, if you have kept some high school differentiation formulas up your sleeve, you will remember that,\\[\\frac{\\partial}{\\partial t} t^z = z t^{z-1}\\]We can put this in exactly the form we want, by defining \\(u \\left( z, t \\right) = t^z\\),\\[\\frac{1}{1} \\frac{\\partial u \\left( z, t \\right)}{\\partial t} = z u \\left( z-1, t \\right)\\]Thus, \\(s=1\\). Plugging all of this into the original statement \\(z! = \\frac{1}{s} \\mathcal{L} \\left\\{ u \\left( z, t \\right) \\right\\} \\left( s \\right)\\),\\[z! = \\mathcal{L} \\left\\{ t^z \\right\\} \\left( 1 \\right) = \\int_0^\\infty t^z e^{-t} dt\\]Furthermore, \\(\\Gamma \\left( z \\right)\\) can be obtained by plugging in \\(z-1\\). Voila! Or, not so voila, as we have derived the extended integral from simple assumptions!May the force be with youBut we are not yet done. Remember the second important property of the extended factorial? It must obey the boundary condition \\(0! =1\\) in order to reduce to the regular factorial for integer arguments. As shown earlier,\\[0! = \\mathcal{L} \\left\\{ t^0 \\right\\} \\left( 1 \\right) = \\mathcal{L} \\left\\{ 1 \\right\\} \\left( 1 \\right) = 1\\]Lastly, since we had assumed that \\(u \\left( z, 0^- \\right) = 0\\), we must verify it now,\\[\\displaystyle{ u \\left( z, 0^- \\right) = \\lim_{t \\to 0^-} t^z = 0 }\\]At long last, we can confidently say that we have found the extended factorial function defined over complex numbers.ConclusionThis brings us to the end of our little endeavour. Clearly, it was a not an easy one, but in Euler’s own words, Logic is the foundation of the certainty of all the knowledge we acquire." } ]
